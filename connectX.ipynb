{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - Connect X\n",
    "https://www.kaggle.com/c/connectx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version history:\n",
    "- v1.0 Initial version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this competition we will work with an agent that will receive these elements:\n",
    "- configuration\n",
    "  + columns - The number of columns on the board\n",
    "  + rows - The number of rows on the board\n",
    "  + inarow - The number of checkers in a row required to win\n",
    "- observation\n",
    "  + board - array [rows x columns] beginning from top left and ending at bottom right. Each element has three possible values:\n",
    "    * 0 = Empty\n",
    "    * 1 = Player1\n",
    "    * 2 = Player2\n",
    "  + mark - Which player is the agent. 1 or 2\n",
    "\n",
    "The agent should return which column to place a checker in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Additional documentation (especially interfaces) can be found on all public functions:\n",
    "from kaggle_environments import make\n",
    "help(make)\n",
    "env = make(\"connectx\")\n",
    "dir(env)\n",
    "help(env.specification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"<!--\n",
       "  Copyright 2020 Kaggle Inc\n",
       "\n",
       "  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "  you may not use this file except in compliance with the License.\n",
       "  You may obtain a copy of the License at\n",
       "\n",
       "      http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "  Unless required by applicable law or agreed to in writing, software\n",
       "  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "  See the License for the specific language governing permissions and\n",
       "  limitations under the License.\n",
       "-->\n",
       "<!DOCTYPE html>\n",
       "<html lang=&quot;en&quot;>\n",
       "  <head>\n",
       "    <title>Kaggle Simulation Player</title>\n",
       "    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n",
       "    <link\n",
       "      rel=&quot;stylesheet&quot;\n",
       "      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n",
       "      crossorigin=&quot;anonymous&quot;\n",
       "    />\n",
       "    <style type=&quot;text/css&quot;>\n",
       "      html,\n",
       "      body {\n",
       "        height: 100%;\n",
       "        font-family: sans-serif;\n",
       "        margin: 0px;\n",
       "      }\n",
       "      canvas {\n",
       "        /* image-rendering: -moz-crisp-edges;\n",
       "        image-rendering: -webkit-crisp-edges;\n",
       "        image-rendering: pixelated;\n",
       "        image-rendering: crisp-edges; */\n",
       "      }\n",
       "    </style>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n",
       "    <script>\n",
       "      // Polyfill for Styled Components\n",
       "      window.React = {\n",
       "        ...preact,\n",
       "        createElement: preact.h,\n",
       "        PropTypes: { func: {} },\n",
       "      };\n",
       "    </script>\n",
       "    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n",
       "  </head>\n",
       "  <body>\n",
       "    <script>\n",
       "      \n",
       "window.kaggle = {\n",
       "  &quot;debug&quot;: false,\n",
       "  &quot;playing&quot;: true,\n",
       "  &quot;step&quot;: 0,\n",
       "  &quot;controls&quot;: true,\n",
       "  &quot;environment&quot;: {\n",
       "    &quot;id&quot;: &quot;04402192-a24b-11ec-ba79-38142801973a&quot;,\n",
       "    &quot;name&quot;: &quot;connectx&quot;,\n",
       "    &quot;title&quot;: &quot;ConnectX&quot;,\n",
       "    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n",
       "    &quot;version&quot;: &quot;1.0.1&quot;,\n",
       "    &quot;configuration&quot;: {\n",
       "      &quot;rows&quot;: 5,\n",
       "      &quot;columns&quot;: 5,\n",
       "      &quot;inarow&quot;: 3,\n",
       "      &quot;episodeSteps&quot;: 1000,\n",
       "      &quot;actTimeout&quot;: 2,\n",
       "      &quot;runTimeout&quot;: 1200,\n",
       "      &quot;agentTimeout&quot;: 60,\n",
       "      &quot;timeout&quot;: 2\n",
       "    },\n",
       "    &quot;specification&quot;: {\n",
       "      &quot;action&quot;: {\n",
       "        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n",
       "        &quot;type&quot;: &quot;integer&quot;,\n",
       "        &quot;minimum&quot;: 0,\n",
       "        &quot;default&quot;: 0\n",
       "      },\n",
       "      &quot;agents&quot;: [\n",
       "        2\n",
       "      ],\n",
       "      &quot;configuration&quot;: {\n",
       "        &quot;episodeSteps&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;minimum&quot;: 1,\n",
       "          &quot;default&quot;: 1000\n",
       "        },\n",
       "        &quot;actTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 2\n",
       "        },\n",
       "        &quot;runTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 1200\n",
       "        },\n",
       "        &quot;columns&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 7,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;rows&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 6,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;inarow&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 4,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;agentTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;timeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 2,\n",
       "          &quot;minimum&quot;: 0\n",
       "        }\n",
       "      },\n",
       "      &quot;info&quot;: {},\n",
       "      &quot;observation&quot;: {\n",
       "        &quot;remainingOverageTime&quot;: {\n",
       "          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n",
       "          &quot;shared&quot;: false,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;step&quot;: {\n",
       "          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 0\n",
       "        },\n",
       "        &quot;board&quot;: {\n",
       "          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n",
       "          &quot;type&quot;: &quot;array&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;default&quot;: []\n",
       "        },\n",
       "        &quot;mark&quot;: {\n",
       "          &quot;defaults&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ],\n",
       "          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n",
       "          &quot;enum&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ]\n",
       "        }\n",
       "      },\n",
       "      &quot;reward&quot;: {\n",
       "        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n",
       "        &quot;enum&quot;: [\n",
       "          -1,\n",
       "          0,\n",
       "          1\n",
       "        ],\n",
       "        &quot;default&quot;: 0,\n",
       "        &quot;type&quot;: [\n",
       "          &quot;number&quot;,\n",
       "          &quot;null&quot;\n",
       "        ]\n",
       "      }\n",
       "    },\n",
       "    &quot;steps&quot;: [\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 0,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 1,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 2,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 3,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 4,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 5,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: -1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        }\n",
       "      ]\n",
       "    ],\n",
       "    &quot;rewards&quot;: [\n",
       "      1,\n",
       "      -1\n",
       "    ],\n",
       "    &quot;statuses&quot;: [\n",
       "      &quot;DONE&quot;,\n",
       "      &quot;DONE&quot;\n",
       "    ],\n",
       "    &quot;schema_version&quot;: 1,\n",
       "    &quot;info&quot;: {}\n",
       "  },\n",
       "  &quot;logs&quot;: [\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.031988,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.401911,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'mark': 2, 'step': 1, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;Traceback (most recent call last):\\n  File \\&quot;C:\\\\Users\\\\alberto.hernandez\\\\Anaconda3\\\\envs\\\\connectx\\\\lib\\\\site-packages\\\\kaggle_environments\\\\agent.py\\&quot;, line 157, in act\\n    action = self.agent(*args)\\n  File \\&quot;<ipython-input-3-29a03b9c96ee>\\&quot;, line 54, in agent_DeepQL\\n    model = tf.keras.models.load_model('model_DQN.h5')\\n  File \\&quot;C:\\\\Users\\\\alberto.hernandez\\\\Anaconda3\\\\envs\\\\connectx\\\\lib\\\\site-packages\\\\tensorflow_core\\\\python\\\\keras\\\\saving\\\\save.py\\&quot;, line 146, in load_model\\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\\n  File \\&quot;C:\\\\Users\\\\alberto.hernandez\\\\Anaconda3\\\\envs\\\\connectx\\\\lib\\\\site-packages\\\\tensorflow_core\\\\python\\\\keras\\\\saving\\\\hdf5_format.py\\&quot;, line 168, in load_model_from_hdf5\\n    custom_objects=custom_objects)\\n  File \\&quot;C:\\\\Users\\\\alberto.hernandez\\\\Anaconda3\\\\envs\\\\connectx\\\\lib\\\\site-packages\\\\tensorflow_core\\\\python\\\\keras\\\\saving\\\\model_config.py\\&quot;, line 55, in model_from_config\\n    return deserialize(config, custom_objects=custom_objects)\\n  File \\&quot;C:\\\\Users\\\\alberto.hernandez\\\\Anaconda3\\\\envs\\\\connectx\\\\lib\\\\s&quot;\n",
       "      }\n",
       "    ],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 3.6e-05,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 0, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.017205,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.008429,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0], 'mark': 1}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;Traceback (most recent call last):\\n  File \\&quot;C:\\\\Users\\\\alberto.hernandez\\\\Anaconda3\\\\envs\\\\connectx\\\\lib\\\\site-packages\\\\kaggle_environments\\\\agent.py\\&quot;, line 157, in act\\n    action = self.agent(*args)\\n  File \\&quot;<ipython-input-4-9d69ce52ddb8>\\&quot;, line 54, in agent_DeepQL\\n    model = tf.keras.models.load_model('model_DQN.h5')\\n  File \\&quot;C:\\\\Users\\\\alberto.hernandez\\\\Anaconda3\\\\envs\\\\connectx\\\\lib\\\\site-packages\\\\tensorflow_core\\\\python\\\\keras\\\\saving\\\\save.py\\&quot;, line 146, in load_model\\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\\n  File \\&quot;C:\\\\Users\\\\alberto.hernandez\\\\Anaconda3\\\\envs\\\\connectx\\\\lib\\\\site-packages\\\\tensorflow_core\\\\python\\\\keras\\\\saving\\\\hdf5_format.py\\&quot;, line 168, in load_model_from_hdf5\\n    custom_objects=custom_objects)\\n  File \\&quot;C:\\\\Users\\\\alberto.hernandez\\\\Anaconda3\\\\envs\\\\connectx\\\\lib\\\\site-packages\\\\tensorflow_core\\\\python\\\\keras\\\\saving\\\\model_config.py\\&quot;, line 55, in model_from_config\\n    return deserialize(config, custom_objects=custom_objects)\\n  File \\&quot;C:\\\\Users\\\\alberto.hernandez\\\\Anaconda3\\\\envs\\\\connectx\\\\lib\\\\s&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 2e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 0, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\\n{'rows': 10, 'columns': 8, 'inarow': 5, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1.4e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 1.8e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0], 'mark': 1}\\n{'rows': 10, 'columns': 8, 'inarow': 5, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1.5e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 3.2e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 0], 'mark': 1}\\n{'rows': 10, 'columns': 8, 'inarow': 5, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1.2e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 2e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 6, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 2], 'mark': 1}\\n{'rows': 10, 'columns': 8, 'inarow': 5, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1.2e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 1.8e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 8, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 2], 'mark': 1}\\n{'rows': 10, 'columns': 8, 'inarow': 5, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1.1e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 1.7e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 10, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 2, 2, 2], 'mark': 1}\\n{'rows': 10, 'columns': 8, 'inarow': 5, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 1.7e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 12, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 2, 0, 2, 2, 2], 'mark': 1}\\n{'rows': 10, 'columns': 8, 'inarow': 5, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 9e-06,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 2.6e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 0, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\\n{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 2.1e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 2.5e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0], 'mark': 1}\\n{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1.3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 2.5e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 2, 0], 'mark': 1}\\n{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 1.5e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 0, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\\n{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1.6e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 1.4e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0], 'mark': 1}\\n{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1.1e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 1.6e-05,\n",
       "        &quot;stdout&quot;: &quot;{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 2, 0], 'mark': 1}\\n{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ]\n",
       "  ],\n",
       "  &quot;mode&quot;: &quot;ipython&quot;,\n",
       "  &quot;width&quot;: 500,\n",
       "  &quot;height&quot;: 450\n",
       "};\n",
       "\n",
       "\n",
       "window.kaggle.renderer = // Copyright 2020 Kaggle Inc\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "function renderer({\n",
       "  act,\n",
       "  agents,\n",
       "  environment,\n",
       "  frame,\n",
       "  height = 400,\n",
       "  interactive,\n",
       "  isInteractive,\n",
       "  parent,\n",
       "  step,\n",
       "  update,\n",
       "  width = 400,\n",
       "}) {\n",
       "  // Configuration.\n",
       "  const { rows, columns, inarow } = environment.configuration;\n",
       "\n",
       "  // Common Dimensions.\n",
       "  const unit = 8;\n",
       "  const minCanvasSize = Math.min(height, width);\n",
       "  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n",
       "  const cellSize = Math.min(\n",
       "    (width - minOffset * 2) / columns,\n",
       "    (height - minOffset * 2) / rows\n",
       "  );\n",
       "  const cellInset = 0.8;\n",
       "  const pieceScale = cellSize / 100;\n",
       "  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n",
       "  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n",
       "\n",
       "  // Canvas Setup.\n",
       "  let canvas = parent.querySelector(&quot;canvas&quot;);\n",
       "  if (!canvas) {\n",
       "    canvas = document.createElement(&quot;canvas&quot;);\n",
       "    parent.appendChild(canvas);\n",
       "\n",
       "    if (interactive) {\n",
       "      canvas.addEventListener(&quot;click&quot;, evt => {\n",
       "        if (!isInteractive()) return;\n",
       "        const rect = evt.target.getBoundingClientRect();\n",
       "        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n",
       "        if (col >= 0 && col < columns) act(col);\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n",
       "\n",
       "  // Character Paths (based on 100x100 tiles).\n",
       "  const kPath = new Path2D(\n",
       "    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n",
       "  );\n",
       "  const goose1Path = new Path2D(\n",
       "    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n",
       "  );\n",
       "  const goose2Path = new Path2D(\n",
       "    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n",
       "  );\n",
       "  const goose3Path = new Path2D(\n",
       "    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n",
       "  );\n",
       "\n",
       "  // Canvas setup and reset.\n",
       "  let c = canvas.getContext(&quot;2d&quot;);\n",
       "  canvas.width = width;\n",
       "  canvas.height = height;\n",
       "  c.fillStyle = &quot;#000B2A&quot;;\n",
       "  c.fillRect(0, 0, canvas.width, canvas.height);\n",
       "\n",
       "  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n",
       "\n",
       "  const getColor = (mark, opacity = 1) => {\n",
       "    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n",
       "    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n",
       "    return &quot;#fff&quot;;\n",
       "  };\n",
       "\n",
       "  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n",
       "    const [row, col] = getRowCol(cell);\n",
       "    c.arc(\n",
       "      xOffset + xFrame * (col * cellSize + cellSize / 2),\n",
       "      yOffset + yFrame * (row * cellSize + cellSize / 2),\n",
       "      (cellInset * cellSize) / 2 - radiusOffset,\n",
       "      2 * Math.PI,\n",
       "      false\n",
       "    );\n",
       "  };\n",
       "\n",
       "  // Render the pieces.\n",
       "  const board = environment.steps[step][0].observation.board;\n",
       "\n",
       "  const drawPiece = mark => {\n",
       "    // Base Styles.\n",
       "    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n",
       "    c.fillStyle = getColor(mark, opacity);\n",
       "    c.strokeStyle = getColor(mark);\n",
       "    c.shadowColor = getColor(mark);\n",
       "    c.shadowBlur = 8 / cellInset;\n",
       "    c.lineWidth = 1 / cellInset;\n",
       "\n",
       "    // Outer circle.\n",
       "    c.save();\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 50, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.lineWidth *= 4;\n",
       "    c.stroke();\n",
       "    c.fill();\n",
       "    c.restore();\n",
       "\n",
       "    // Inner circle.\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 40, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.stroke();\n",
       "\n",
       "    // Kaggle &quot;K&quot;.\n",
       "    if (mark === 1) {\n",
       "      const scale = 0.54;\n",
       "      c.save();\n",
       "      c.translate(23, 23);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(kPath);\n",
       "      c.restore();\n",
       "    }\n",
       "\n",
       "    // Kaggle &quot;Goose&quot;.\n",
       "    if (mark === 2) {\n",
       "      const scale = 0.6;\n",
       "      c.save();\n",
       "      c.translate(24, 28);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(goose1Path);\n",
       "      c.stroke(goose2Path);\n",
       "      c.stroke(goose3Path);\n",
       "      c.beginPath();\n",
       "      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n",
       "      c.closePath();\n",
       "      c.fill();\n",
       "      c.restore();\n",
       "    }\n",
       "  };\n",
       "\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    const [row, col] = getRowCol(i);\n",
       "    if (board[i] === 0) continue;\n",
       "    // Easing In.\n",
       "    let yFrame = Math.min(\n",
       "      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n",
       "      1\n",
       "    );\n",
       "\n",
       "    if (\n",
       "      step > 1 &&\n",
       "      environment.steps[step - 1][0].observation.board[i] === board[i]\n",
       "    ) {\n",
       "      yFrame = 1;\n",
       "    }\n",
       "\n",
       "    c.save();\n",
       "    c.translate(\n",
       "      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n",
       "      yOffset +\n",
       "        yFrame * (cellSize * row) +\n",
       "        (cellSize - cellSize * cellInset) / 2\n",
       "    );\n",
       "    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n",
       "    drawPiece(board[i]);\n",
       "    c.restore();\n",
       "  }\n",
       "\n",
       "  // Background Gradient.\n",
       "  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n",
       "  const bgStyle = c.createRadialGradient(\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    0,\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    bgRadius\n",
       "  );\n",
       "  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n",
       "  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n",
       "\n",
       "  // Render the board overlay.\n",
       "  c.beginPath();\n",
       "  c.rect(0, 0, canvas.width, canvas.height);\n",
       "  c.closePath();\n",
       "  c.shadowBlur = 0;\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    drawCellCircle(i);\n",
       "    c.closePath();\n",
       "  }\n",
       "  c.fillStyle = bgStyle;\n",
       "  c.fill(&quot;evenodd&quot;);\n",
       "\n",
       "  // Render the board overlay cell outlines.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    c.beginPath();\n",
       "    drawCellCircle(i);\n",
       "    c.strokeStyle = &quot;#0361B2&quot;;\n",
       "    c.lineWidth = 1;\n",
       "    c.stroke();\n",
       "    c.closePath();\n",
       "  }\n",
       "\n",
       "  const drawLine = (fromCell, toCell) => {\n",
       "    if (frame < 0.5) return;\n",
       "    const lineFrame = (frame - 0.5) / 0.5;\n",
       "    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n",
       "    const x2 =\n",
       "      x1 +\n",
       "      lineFrame *\n",
       "        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n",
       "    const y1 =\n",
       "      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n",
       "    const y2 =\n",
       "      y1 +\n",
       "      lineFrame *\n",
       "        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n",
       "    c.beginPath();\n",
       "    c.lineCap = &quot;round&quot;;\n",
       "    c.lineWidth = 4;\n",
       "    c.strokeStyle = getColor(board[fromCell]);\n",
       "    c.shadowBlur = 8;\n",
       "    c.shadowColor = getColor(board[fromCell]);\n",
       "    c.moveTo(x1, y1);\n",
       "    c.lineTo(x2, y2);\n",
       "    c.stroke();\n",
       "  };\n",
       "\n",
       "  // Generate a graph of the board.\n",
       "  const getCell = (cell, rowOffset, columnOffset) => {\n",
       "    const row = Math.floor(cell / columns) + rowOffset;\n",
       "    const col = (cell % columns) + columnOffset;\n",
       "    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n",
       "    return col + row * columns;\n",
       "  };\n",
       "  const makeNode = cell => {\n",
       "    const node = { cell, directions: [], value: board[cell] };\n",
       "    for (let r = -1; r <= 1; r++) {\n",
       "      for (let c = -1; c <= 1; c++) {\n",
       "        if (r === 0 && c === 0) continue;\n",
       "        node.directions.push(getCell(cell, r, c));\n",
       "      }\n",
       "    }\n",
       "    return node;\n",
       "  };\n",
       "  const graph = board.map((_, i) => makeNode(i));\n",
       "\n",
       "  // Check for any wins!\n",
       "  const getSequence = (node, direction) => {\n",
       "    const sequence = [node.cell];\n",
       "    while (sequence.length < inarow) {\n",
       "      const next = graph[node.directions[direction]];\n",
       "      if (!next || node.value !== next.value || next.value === 0) return;\n",
       "      node = next;\n",
       "      sequence.push(node.cell);\n",
       "    }\n",
       "    return sequence;\n",
       "  };\n",
       "\n",
       "  // Check all nodes.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    // Check all directions (not the most efficient).\n",
       "    for (let d = 0; d < 8; d++) {\n",
       "      const seq = getSequence(graph[i], d);\n",
       "      if (seq) {\n",
       "        drawLine(seq[0], seq[inarow - 1]);\n",
       "        i = board.length;\n",
       "        break;\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Upgrade the legend.\n",
       "  if (agents.length && (!agents[0].color || !agents[0].image)) {\n",
       "    const getPieceImage = mark => {\n",
       "      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n",
       "      parent.appendChild(pieceCanvas);\n",
       "      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n",
       "      pieceCanvas.width = 100;\n",
       "      pieceCanvas.height = 100;\n",
       "      c = pieceCanvas.getContext(&quot;2d&quot;);\n",
       "      c.translate(10, 10);\n",
       "      c.scale(0.8, 0.8);\n",
       "      drawPiece(mark);\n",
       "      const dataUrl = pieceCanvas.toDataURL();\n",
       "      parent.removeChild(pieceCanvas);\n",
       "      return dataUrl;\n",
       "    };\n",
       "\n",
       "    agents.forEach(agent => {\n",
       "      agent.color = getColor(agent.index + 1);\n",
       "      agent.image = getPieceImage(agent.index + 1);\n",
       "    });\n",
       "    update({ agents });\n",
       "  }\n",
       "};\n",
       "\n",
       "\n",
       "    \n",
       "    </script>\n",
       "    <script>\n",
       "      const h = htm.bind(preact.h);\n",
       "      const { useContext, useEffect, useRef, useState } = preactHooks;\n",
       "      const styled = window.styled.default;\n",
       "\n",
       "      const Context = preact.createContext({});\n",
       "\n",
       "      const Loading = styled.div`\n",
       "        animation: rotate360 1.1s infinite linear;\n",
       "        border: 8px solid rgba(255, 255, 255, 0.2);\n",
       "        border-left-color: #0cb1ed;\n",
       "        border-radius: 50%;\n",
       "        height: 40px;\n",
       "        position: relative;\n",
       "        transform: translateZ(0);\n",
       "        width: 40px;\n",
       "\n",
       "        @keyframes rotate360 {\n",
       "          0% {\n",
       "            transform: rotate(0deg);\n",
       "          }\n",
       "          100% {\n",
       "            transform: rotate(360deg);\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Logo = styled(\n",
       "        (props) => h`\n",
       "        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n",
       "          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n",
       "            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n",
       "              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n",
       "              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n",
       "              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n",
       "              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n",
       "              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n",
       "              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n",
       "            </g>\n",
       "          </svg>\n",
       "        </a>\n",
       "      `\n",
       "      )`\n",
       "        display: inline-flex;\n",
       "      `;\n",
       "\n",
       "      const Header = styled((props) => {\n",
       "        const { environment } = useContext(Context);\n",
       "\n",
       "        return h`<div className=${props.className} >\n",
       "          <${Logo} />\n",
       "          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n",
       "          ${environment.title}\n",
       "        </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-bottom: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        color: #fff;\n",
       "        display: flex;\n",
       "        flex: 0 0 36px;\n",
       "        font-size: 14px;\n",
       "        justify-content: space-between;\n",
       "        padding: 0 8px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Renderer = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { animate, debug, playing, renderer, speed } = context;\n",
       "        const ref = preact.createRef();\n",
       "\n",
       "        useEffect(async () => {\n",
       "          if (!ref.current) return;\n",
       "\n",
       "          const renderFrame = async (start, step, lastFrame) => {\n",
       "            if (step !== context.step) return;\n",
       "            if (lastFrame === 1) {\n",
       "              if (!animate) return;\n",
       "              start = Date.now();\n",
       "            }\n",
       "            const frame =\n",
       "              playing || animate\n",
       "                ? Math.min((Date.now() - start) / speed, 1)\n",
       "                : 1;\n",
       "            try {\n",
       "              if (debug) console.time(&quot;render&quot;);\n",
       "              await renderer({\n",
       "                ...context,\n",
       "                frame,\n",
       "                height: ref.current.clientHeight,\n",
       "                hooks: preactHooks,\n",
       "                parent: ref.current,\n",
       "                preact,\n",
       "                styled,\n",
       "                width: ref.current.clientWidth,\n",
       "              });\n",
       "            } catch (error) {\n",
       "              if (debug) console.error(error);\n",
       "              console.log({ ...context, frame, error });\n",
       "            } finally {\n",
       "              if (debug) console.timeEnd(&quot;render&quot;);\n",
       "            }\n",
       "            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n",
       "          };\n",
       "\n",
       "          await renderFrame(Date.now(), context.step);\n",
       "        }, [ref.current, context.step, context.renderer]);\n",
       "\n",
       "        return h`<div className=${props.className} ref=${ref} />`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        height: 100%;\n",
       "        left: 0;\n",
       "        justify-content: center;\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Processing = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        const text = processing === true ? &quot;Processing...&quot; : processing;\n",
       "        return h`<div className=${props.className}>${text}</div>`;\n",
       "      })`\n",
       "        bottom: 0;\n",
       "        color: #fff;\n",
       "        font-size: 12px;\n",
       "        left: 0;\n",
       "        line-height: 24px;\n",
       "        position: absolute;\n",
       "        text-align: center;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Viewer = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        return h`<div className=${props.className}>\n",
       "          <${Renderer} />\n",
       "          ${processing && h`<${Processing} />`}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        background-image: radial-gradient(\n",
       "          circle closest-side,\n",
       "          #000b49,\n",
       "          #000b2a\n",
       "        );\n",
       "        display: flex;\n",
       "        flex: 1;\n",
       "        overflow: hidden;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      // Partitions the elements of arr into subarrays of max length num.\n",
       "      const groupIntoSets = (arr, num) => {\n",
       "        const sets = [];\n",
       "        arr.forEach(a => {\n",
       "          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n",
       "            sets.push([]);\n",
       "          }\n",
       "          sets[sets.length - 1].push(a);\n",
       "        });\n",
       "        return sets;\n",
       "      }\n",
       "\n",
       "      // Expects `width` input prop to set proper max-width for agent name span.\n",
       "      const Legend = styled((props) => {\n",
       "        const { agents, legend } = useContext(Context);\n",
       "\n",
       "        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n",
       "\n",
       "        return h`<div className=${props.className}>\n",
       "          ${agentPairs.map(agentList =>\n",
       "            h`<ul>\n",
       "                ${agentList.map(a =>\n",
       "                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n",
       "                      ${a.image && h`<img src=${a.image} />`}\n",
       "                      <span>${a.name}</span>\n",
       "                    </li>`\n",
       "                )}\n",
       "              </ul>`)}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        font-family: sans-serif;\n",
       "        font-size: 14px;\n",
       "        height: 48px;\n",
       "        width: 100%;\n",
       "\n",
       "        ul {\n",
       "          align-items: center;\n",
       "          display: flex;\n",
       "          flex-direction: row;\n",
       "          justify-content: center;\n",
       "        }\n",
       "\n",
       "        li {\n",
       "          align-items: center;\n",
       "          display: inline-flex;\n",
       "          transition: color 1s;\n",
       "        }\n",
       "\n",
       "        span {\n",
       "          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n",
       "          overflow: hidden;\n",
       "          text-overflow: ellipsis;\n",
       "          white-space: nowrap;\n",
       "        }\n",
       "\n",
       "        img {\n",
       "          height: 24px;\n",
       "          margin-left: 4px;\n",
       "          margin-right: 4px;\n",
       "          width: 24px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepInput = styled.input.attrs({\n",
       "        type: &quot;range&quot;,\n",
       "      })`\n",
       "        appearance: none;\n",
       "        background: rgba(255, 255, 255, 0.15);\n",
       "        border-radius: 2px;\n",
       "        display: block;\n",
       "        flex: 1;\n",
       "        height: 4px;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "        width: 100%;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "\n",
       "        &::-webkit-slider-thumb {\n",
       "          appearance: none;\n",
       "          background: #1ebeff;\n",
       "          border-radius: 100%;\n",
       "          cursor: pointer;\n",
       "          height: 12px;\n",
       "          margin: 0;\n",
       "          position: relative;\n",
       "          width: 12px;\n",
       "\n",
       "          &::after {\n",
       "            content: &quot;&quot;;\n",
       "            position: absolute;\n",
       "            top: 0px;\n",
       "            left: 0px;\n",
       "            width: 200px;\n",
       "            height: 8px;\n",
       "            background: green;\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const PlayButton = styled.button`\n",
       "        align-items: center;\n",
       "        background: none;\n",
       "        border: none;\n",
       "        color: white;\n",
       "        cursor: pointer;\n",
       "        display: flex;\n",
       "        flex: 0 0 56px;\n",
       "        font-size: 20px;\n",
       "        height: 40px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepCount = styled.span`\n",
       "        align-items: center;\n",
       "        color: white;\n",
       "        display: flex;\n",
       "        font-size: 14px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        padding: 0 16px;\n",
       "        pointer-events: none;\n",
       "      `;\n",
       "\n",
       "      const Controls = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "        const value = step + 1;\n",
       "        const onClick = () => (playing ? pause() : play());\n",
       "        const onInput = (e) => {\n",
       "          pause();\n",
       "          setStep(parseInt(e.target.value) - 1);\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n",
       "          playing\n",
       "            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "        }</svg><//>\n",
       "            <${StepInput} min=&quot;1&quot; max=${\n",
       "          environment.steps.length\n",
       "        } value=&quot;${value}&quot; onInput=${onInput} />\n",
       "            <${StepCount}>${value} / ${environment.steps.length}<//>\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-top: 4px solid #212121;\n",
       "        display: flex;\n",
       "        flex: 0 0 44px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Info = styled((props) => {\n",
       "        const {\n",
       "          environment,\n",
       "          playing,\n",
       "          step,\n",
       "          speed,\n",
       "          animate,\n",
       "          header,\n",
       "          controls,\n",
       "          settings,\n",
       "        } = useContext(Context);\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            info:\n",
       "            step(${step}),\n",
       "            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n",
       "            speed(${speed}),\n",
       "            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n",
       "          </div>`;\n",
       "      })`\n",
       "        color: #888;\n",
       "        font-family: monospace;\n",
       "        font-size: 12px;\n",
       "      `;\n",
       "\n",
       "      const Settings = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${Info} />\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        background: #fff;\n",
       "        border-top: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        padding: 20px;\n",
       "        width: 100%;\n",
       "\n",
       "        h1 {\n",
       "          font-size: 20px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Player = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { agents, controls, header, legend, loading, settings, width } = context;\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            ${loading && h`<${Loading} />`}\n",
       "            ${!loading && header && h`<${Header} />`}\n",
       "            ${!loading && h`<${Viewer} />`}\n",
       "            ${!loading && legend && h`<${Legend} width=${width}/>`}\n",
       "            ${!loading && controls && h`<${Controls} />`}\n",
       "            ${!loading && settings && h`<${Settings} />`}\n",
       "          </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        background: #212121;\n",
       "        border: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "        height: 100%;\n",
       "        justify-content: center;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const App = () => {\n",
       "        const renderCountRef = useRef(0);\n",
       "        const [_, setRenderCount] = useState(0);\n",
       "\n",
       "        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n",
       "        const speeds = [\n",
       "          0,\n",
       "          3000,\n",
       "          1000,\n",
       "          500,\n",
       "          333, // Default\n",
       "          200,\n",
       "          100,\n",
       "          50,\n",
       "          25,\n",
       "          10,\n",
       "        ];\n",
       "\n",
       "        const contextRef = useRef({\n",
       "          animate: false,\n",
       "          agents: [],\n",
       "          controls: false,\n",
       "          debug: false,\n",
       "          environment: { steps: [], info: {} },\n",
       "          header: window.innerHeight >= 600,\n",
       "          height: window.innerHeight,\n",
       "          interactive: false,\n",
       "          legend: true,\n",
       "          loading: false,\n",
       "          playing: false,\n",
       "          processing: false,\n",
       "          renderer: () => &quot;DNE&quot;,\n",
       "          settings: false,\n",
       "          speed: speeds[4],\n",
       "          step: 0,\n",
       "          width: window.innerWidth,\n",
       "        });\n",
       "\n",
       "        // Context helpers.\n",
       "        const rerender = (contextRef.current.rerender = () =>\n",
       "          setRenderCount((renderCountRef.current += 1)));\n",
       "        const setStep = (contextRef.current.setStep = (newStep) => {\n",
       "          contextRef.current.step = newStep;\n",
       "          rerender();\n",
       "        });\n",
       "        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n",
       "          contextRef.current.playing = playing;\n",
       "          rerender();\n",
       "        });\n",
       "        const pause = (contextRef.current.pause = () => setPlaying(false));\n",
       "\n",
       "        const playNext = () => {\n",
       "          const context = contextRef.current;\n",
       "\n",
       "          if (\n",
       "            context.playing &&\n",
       "            context.step < context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(context.step + 1);\n",
       "            play(true);\n",
       "          } else {\n",
       "            pause();\n",
       "          }\n",
       "        };\n",
       "\n",
       "        const play = (contextRef.current.play = (continuing) => {\n",
       "          const context = contextRef.current;\n",
       "          if (context.playing && !continuing) return;\n",
       "          if (!context.playing) setPlaying(true);\n",
       "          if (\n",
       "            !continuing &&\n",
       "            context.step === context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(0);\n",
       "          }\n",
       "          setTimeout(playNext, context.speed);\n",
       "        });\n",
       "\n",
       "        const updateContext = (o) => {\n",
       "          const context = contextRef.current;\n",
       "          Object.assign(context, o, {\n",
       "            environment: { ...context.environment, ...(o.environment || {}) },\n",
       "          });\n",
       "          rerender();\n",
       "        };\n",
       "\n",
       "        // First time setup.\n",
       "        useEffect(() => {\n",
       "          // Timeout is used to ensure useEffect renders once.\n",
       "          setTimeout(() => {\n",
       "            // Initialize context with window.kaggle.\n",
       "            updateContext(window.kaggle || {});\n",
       "\n",
       "            if (window.kaggle.playing) {\n",
       "                play(true);\n",
       "            }\n",
       "\n",
       "            // Listen for messages received to update the context.\n",
       "            window.addEventListener(\n",
       "              &quot;message&quot;,\n",
       "              (event) => {\n",
       "                // Ensure the environment names match before updating.\n",
       "                try {\n",
       "                  if (\n",
       "                    event.data.environment.name ==\n",
       "                    contextRef.current.environment.name\n",
       "                  ) {\n",
       "                    updateContext(event.data);\n",
       "                  }\n",
       "                } catch {}\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "            // Listen for keyboard commands.\n",
       "            window.addEventListener(\n",
       "              &quot;keydown&quot;,\n",
       "              (event) => {\n",
       "                const {\n",
       "                  interactive,\n",
       "                  isInteractive,\n",
       "                  playing,\n",
       "                  step,\n",
       "                  environment,\n",
       "                } = contextRef.current;\n",
       "                const key = event.keyCode;\n",
       "                const zero_key = 48\n",
       "                const nine_key = 57\n",
       "                if (\n",
       "                  interactive ||\n",
       "                  isInteractive() ||\n",
       "                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n",
       "                )\n",
       "                  return;\n",
       "\n",
       "                if (key === 32) {\n",
       "                  playing ? pause() : play();\n",
       "                } else if (key === 39) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step < environment.steps.length - 1) setStep(step + 1);\n",
       "                  rerender();\n",
       "                } else if (key === 37) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step > 0) setStep(step - 1);\n",
       "                  rerender();\n",
       "                } else if (key >= zero_key && key <= nine_key) {\n",
       "                  contextRef.current.speed = speeds[key - zero_key];\n",
       "                }\n",
       "                event.preventDefault();\n",
       "                return false;\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "          }, 1);\n",
       "        }, []);\n",
       "\n",
       "        if (contextRef.current.debug) {\n",
       "          console.log(&quot;context&quot;, contextRef.current);\n",
       "        }\n",
       "\n",
       "        // Ability to update context.\n",
       "        contextRef.current.update = updateContext;\n",
       "\n",
       "        // Ability to communicate with ipython.\n",
       "        const execute = (contextRef.current.execute = (source) =>\n",
       "          new Promise((resolve, reject) => {\n",
       "            try {\n",
       "              window.parent.IPython.notebook.kernel.execute(source, {\n",
       "                iopub: {\n",
       "                  output: (resp) => {\n",
       "                    const type = resp.msg_type;\n",
       "                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n",
       "                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n",
       "                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n",
       "                  },\n",
       "                },\n",
       "              });\n",
       "            } catch (e) {\n",
       "              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n",
       "            }\n",
       "          }));\n",
       "\n",
       "        // Ability to return an action from an interactive session.\n",
       "        contextRef.current.act = (action) => {\n",
       "          const id = contextRef.current.environment.id;\n",
       "          updateContext({ processing: true });\n",
       "          execute(`\n",
       "            import json\n",
       "            from kaggle_environments import interactives\n",
       "            if &quot;${id}&quot; in interactives:\n",
       "                action = json.loads('${JSON.stringify(action)}')\n",
       "                env, trainer = interactives[&quot;${id}&quot;]\n",
       "                trainer.step(action)\n",
       "                print(json.dumps(env.steps))`)\n",
       "            .then((resp) => {\n",
       "              try {\n",
       "                updateContext({\n",
       "                  processing: false,\n",
       "                  environment: { steps: JSON.parse(resp) },\n",
       "                });\n",
       "                play();\n",
       "              } catch (e) {\n",
       "                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n",
       "                console.error(resp, e);\n",
       "              }\n",
       "            })\n",
       "            .catch((e) => console.error(e));\n",
       "        };\n",
       "\n",
       "        // Check if currently interactive.\n",
       "        contextRef.current.isInteractive = () => {\n",
       "          const context = contextRef.current;\n",
       "          const steps = context.environment.steps;\n",
       "          return (\n",
       "            context.interactive &&\n",
       "            !context.processing &&\n",
       "            context.step === steps.length - 1 &&\n",
       "            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n",
       "          );\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <${Context.Provider} value=${contextRef.current}>\n",
       "            <${Player} />\n",
       "          <//>`;\n",
       "      };\n",
       "\n",
       "      preact.render(h`<${App} />`, document.body);\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n",
       "\" width=\"500\" height=\"450\" frameborder=\"0\"></iframe> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\n",
      "{'remainingOverageTime': {'description': 'Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.', 'shared': False, 'type': 'number', 'minimum': 0, 'default': 60}, 'step': {'description': 'Current step within the episode.', 'type': 'integer', 'shared': True, 'minimum': 0, 'default': 0}, 'board': {'description': 'Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2', 'type': 'array', 'shared': True, 'default': []}, 'mark': {'defaults': [1, 2], 'description': 'Which checkers are the agents.', 'enum': [1, 2]}}\n",
      "configuration:\n",
      "{'episodeSteps': {'description': 'Maximum number of steps in the episode.', 'type': 'integer', 'minimum': 1, 'default': 1000}, 'actTimeout': {'description': 'Maximum runtime (seconds) to obtain an action from an agent.', 'type': 'number', 'minimum': 0, 'default': 2}, 'runTimeout': {'description': 'Maximum runtime (seconds) of an episode (not necessarily DONE).', 'type': 'number', 'minimum': 0, 'default': 1200}, 'columns': {'description': 'The number of columns on the board', 'type': 'integer', 'default': 7, 'minimum': 1}, 'rows': {'description': 'The number of rows on the board', 'type': 'integer', 'default': 6, 'minimum': 1}, 'inarow': {'description': 'The number of checkers in a row required to win.', 'type': 'integer', 'default': 4, 'minimum': 1}, 'agentTimeout': {'description': 'Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.', 'type': 'number', 'minimum': 0, 'default': 60}, 'timeout': {'description': 'Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.', 'type': 'integer', 'default': 2, 'minimum': 0}}\n",
      "action:\n",
      "{'description': 'Column to drop a checker onto the board.', 'type': 'integer', 'minimum': 0, 'default': 0}\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3})\n",
    "# observation.board es un array que va de arriba a abajo!!! board[0] --> celda de arriba a la izquierda\n",
    "def agent(observation, configuration):\n",
    "    print(observation) # {board: [...], mark: 1}\n",
    "    print(configuration) # {rows: 10, columns: 8, inarow: 5}\n",
    "    if (observation.board[0] == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 3\n",
    "#    return 3 # Action: always place a mark in the 3rd column.\n",
    "\n",
    "# Run an episode using the agent above vs the default random agent.\n",
    "env.run([agent, \"random\"])\n",
    "env.render(mode=\"ipython\", width=500, height=450)\n",
    "\n",
    "# Print schemas from the specification.\n",
    "print(\"observation:\")\n",
    "print(env.specification.observation)\n",
    "print(\"configuration:\")\n",
    "print(env.specification.configuration)\n",
    "print(\"action:\")\n",
    "print(env.specification.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.toJSON()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of element observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.specification.observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of element configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.specification.configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of element action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.specification.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.specification.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(env.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "env = make(\"connectx\", {\"rows\": 10, \"columns\": 8, \"inarow\": 5}, debug=True)\n",
    "\n",
    "num_cols = env.configuration.columns\n",
    "#board = env.observation.board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### MonteCarlo Control  \n",
    "An approach to the optimal policy by generating episodes  \n",
    "Off Policy. Learn policy $ \\pi $ using policy $ \\mu $ (random selection of actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyMapMC = {}\n",
    "policyUpdatesMC = {} # Number of times the value of this status has been updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load policyMap from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from: https://stackoverflow.com/questions/56403013/how-to-save-the-dictionary-that-have-tuple-keys\n",
    "\n",
    "dic = ''\n",
    "with open(r'policyMapMC.dict','r') as f:\n",
    "         for i in f.readlines():\n",
    "            dic=i #string\n",
    "policyMapMC = eval(dic)\n",
    "\n",
    "dic = ''\n",
    "with open(r'policyUpdatesMC.dict','r') as f:\n",
    "         for i in f.readlines():\n",
    "            dic=i #string\n",
    "policyUpdatesMC = eval(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(policyMapMC.keys()), len(policyUpdatesMC.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(policyMapMC.values()), max(policyMapMC.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(policyMapMC.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from kaggle_environments import make\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "num_cols = env.configuration.columns\n",
    "\n",
    "# Training agent in first position (player 1) against the negamax agent.\n",
    "trainer = env.train([None, \"negamax\"])\n",
    "\n",
    "discount = 0.9\n",
    "\n",
    "t = datetime.datetime.now()\n",
    "print(\"Inicio\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "obs = trainer.reset()\n",
    "for i in range(20000):\n",
    "    done = False\n",
    "    #env.render()\n",
    "    while not done:\n",
    "        available_cols = [col for col in range(num_cols) if obs.board[col] == 0]\n",
    "        action = int(np.random.choice(available_cols)) # Action for the agent being trained.\n",
    "        obs, reward, done, info = trainer.step(action)\n",
    "        if done:\n",
    "            # Update policy\n",
    "            last_step_reward = env.steps[-1][0][\"reward\"]\n",
    "            # print(\"Reward:\", last_step_reward)\n",
    "            num_steps = env.steps[-1][0][\"observation\"][\"step\"]\n",
    "            # print(\"Steps:\", num_steps)\n",
    "\n",
    "            for step in env.steps:\n",
    "                board = step[0][\"observation\"][\"board\"]\n",
    "                # Convert board in a tuple to use it as the key in a dict structure\n",
    "                board_key = tuple(board)\n",
    "                policyUpdatesMC[board_key] = policyUpdatesMC.get(board_key, 0) + 1\n",
    "                # Value decreased depending on discount and how far is the reward\n",
    "                newValue = (discount ** (num_steps - step[0][\"observation\"][\"step\"])) * last_step_reward\n",
    "                # Each time the policy is updated, the update is smaller\n",
    "                policyMapMC[board_key] = policyMapMC.get(board_key, 0) + ((newValue - policyMapMC.get(board_key, 0)) / policyUpdatesMC[board_key])\n",
    "                \n",
    "            obs = trainer.reset()\n",
    "            \n",
    "    if (i % 10000) == 0:\n",
    "        print(i, datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        \n",
    "t = datetime.datetime.now()\n",
    "print(\"Fin\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# 50000\n",
    "# 16:13:37\n",
    "# 18:00:35\n",
    "# 30000\n",
    "# 20:20:39\n",
    "# 21:54:36\n",
    "# 50000\n",
    "# 15:01:30\n",
    "# 15:47:47\n",
    "# 50000\n",
    "# 16:29:17\n",
    "# 17:12:46\n",
    "# 20000\n",
    "# 15:33:48\n",
    "# 16:17:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save policyMap to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'policyMapMC.dict','w+') as f:\n",
    "     f.write(str(policyMapMC))\n",
    "\n",
    "with open(r'policyUpdatesMC.dict','w+') as f:\n",
    "     f.write(str(policyUpdatesMC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.steps # lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "def agent_MC(observation, configuration):\n",
    "    \n",
    "    ################################## START NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    # Given a board (as a list) and a column, returns the index of the first row available\n",
    "    def first_row_avail(board, col, num_rows, num_cols):\n",
    "        \"\"\"\n",
    "        Given a board (as a list) and a column, returns the index of the first row available\n",
    "        \"\"\"\n",
    "        index = -1\n",
    "        for i in range(num_rows): # from top to bottom\n",
    "            board_index = col + (i * num_cols)\n",
    "            if board[board_index] == 0:\n",
    "                index = board_index\n",
    "            else:\n",
    "                return index\n",
    "        return index\n",
    "            \n",
    "    ################################## END NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    print(configuration) # {rows: 5, columns: 5, inarow: 3}\n",
    "    # Number of rows on the board\n",
    "    num_rows = configuration.rows\n",
    "    # Number of columns on the board\n",
    "    num_cols = configuration.columns\n",
    "    # Number of checkers \"in a row\" needed to win\n",
    "    in_a_row = configuration.inarow\n",
    "    print(observation) # {board: [...], mark: 1}\n",
    "    # The current serialized board (rows x columns) - array [rows x columns] with top row first\n",
    "    board = observation.board\n",
    "    # Which player the agent is playing as (1 or 2).\n",
    "    mark = observation.mark\n",
    "    # If the board is empty, put the first checker in the middle column\n",
    "    if max(board)==0:\n",
    "        return round(num_cols/2)\n",
    "    # Load the policiMap from file\n",
    "    policyMapMC = {}\n",
    "    dic = ''\n",
    "    with open(r'policyMapMC.dict','r') as f:\n",
    "        for i in f.readlines():\n",
    "            dic=i #string\n",
    "    policyMapMC = eval(dic)\n",
    "    # List of available columns\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # Convert the list board into a two-dimensional array\n",
    "    #board_array=np.array(board)\n",
    "    #board_array=board_array.reshape((num_rows,num_cols))\n",
    "    # Evaluate the target board with a checker in each available column, and get the best one\n",
    "    reward_list=[] # List with rewards of each movement\n",
    "    for target_col in available_cols:\n",
    "        target_board = board.copy()\n",
    "        target_index = first_row_avail(board, target_col, num_rows, num_cols)\n",
    "        target_board[target_index] = mark\n",
    "        # Convert target_board into a tuple so that we can use it as a key for the policyMap\n",
    "        board_key = tuple(target_board)\n",
    "        reward = policyMapMC.get(board_key, 0.0001)\n",
    "        reward_list.append(reward)\n",
    "        print(\"Column:\"+str(target_col)+\" Reward:\"+str(reward))\n",
    "    # Get the highest reward if the agent is player 1 and the lowest if is player 2. The policy has been learned with player 1\n",
    "    if mark == 1:\n",
    "        max_reward = max (reward_list)\n",
    "    else:\n",
    "        max_reward = min (reward_list)\n",
    "    # Return the column with the highest reward\n",
    "    return available_cols[reward_list.index(max_reward)]\n",
    "\n",
    "# Run an episode using the agent above vs the negamax agent.\n",
    "#env.run([agent_MC, \"negamax\"])\n",
    "#env.run([\"negamax\", agent_MC])\n",
    "#env.render(mode=\"ipython\", width=500, height=450)\n",
    "#env.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_aux = [0, 0, 2, 0, 0,\n",
    "             0, 0, 2, 0, 0,\n",
    "             0, 0, 2, 1, 0,\n",
    "             0, 0, 1, 1, 0,\n",
    "             0, 0, 1, 2, 0]\n",
    "\n",
    "board_aux = [0, 0, 0, 0, 0,\n",
    "             0, 0, 2, 0, 0,\n",
    "             0, 0, 2, 1, 0,\n",
    "             0, 0, 1, 1, 0,\n",
    "             0, 0, 1, 2, 0]\n",
    "\n",
    "\n",
    "tuple_aux = tuple(board_aux)\n",
    "policyMapMC.get(tuple_aux, 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'evaluate' returns a list of lists (one list per episode).  \n",
    "These lists per episode contains two elements (one per player), and these elements have these possible values:\n",
    "- 1: This player wins\n",
    "- -1: This player loses\n",
    "- 0: Draw (I guess there will be a value for draw)  \n",
    "Example:  \n",
    "[[1, -1], # First episode: Player 1 won  \n",
    " [1, -1], # Second episode: Player 1 won  \n",
    " [1, -1], # Third episode: Player 1 won  \n",
    " [1, -1], # ...  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1],  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "help(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "#evaluate(\"connectx\", [agent_MC, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "evaluate(\"connectx\", [\"random\", agent_MC], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "#evaluate(\"connectx\", [agent, \"negamax\"], num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our agent against random and negamax agent.  \n",
    "- The closer the value is to 1 the better is player 1 agent (wins more times than loses)\n",
    "- The closer the value is to -1 the worse is player 1 agent (loses more times than wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "\n",
    "# Run multiple episodes to estimate its performance.\n",
    "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [agent_MC, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [agent_MC, \"negamax\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/itertools.html\n",
    "from itertools import permutations\n",
    "from kaggle_environments import evaluate\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "\n",
    "def name(obj):\n",
    "    \"\"\"\n",
    "    If obj is a function, gets the name instead of something like <function agent_MC at 0x0000023B01C3C950>\n",
    "    \"\"\"\n",
    "    if callable(obj):\n",
    "        return getattr(obj, \"__name__\")\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "conf={\"rows\": 5, \"columns\": 5, \"inarow\": 3}\n",
    "\n",
    "list_agents = [\"random\", \"negamax\", agent_MC]\n",
    "list_matches = list(permutations(list_agents, 2))\n",
    "results = {}\n",
    "\n",
    "for players in list_matches:\n",
    "    print(\"Playing\", name(players[0]), \"vs\", name(players[1]))\n",
    "    list_players = list(players)\n",
    "    result = mean_reward(evaluate(\"connectx\", agents=list_players, configuration=conf, num_episodes=10))\n",
    "    print(result)\n",
    "    results[name(players[0])] = results.get(name(players[0]), 0) + result\n",
    "    results[name(players[1])] = results.get(name(players[1]), 0) - result\n",
    "\n",
    "# Display results ordered by the points obtained\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"{:<10} {:<10} {:<6}\".format('Position','Team','Points')) # https://docs.python.org/3/library/string.html#formatspec\n",
    "for i, result in enumerate(sorted_results):\n",
    "    print(\"{:<10} {:<10} {:>6.2f}\".format(i+1, result[0], result[1]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Temporal Differece Q-learning  \n",
    "Unlike MonteCarlo, in this algorithm we try to get Q-values (value for pair state, action) and we don't need the full episode to learn (bootstrapping)  \n",
    "Off Policy. Learn policy $ \\pi $ (greedy) using policy $ \\mu $ (epsilon-greedy: greedy or random depending on a random value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Nested dictionary. We use defaultdict because it avoids issues with unexisting keys.\n",
    "policyMapQL = defaultdict(lambda: defaultdict(lambda: 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load policyMap from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "dic = ''\n",
    "with open(r'policyMapQL.dict','r') as f:\n",
    "         for i in f.readlines():\n",
    "            dic=i #string\n",
    "policyMapQL_dict = eval(dic)\n",
    "policyMapQL = defaultdict(lambda: defaultdict(lambda: 0), policyMapQL_dict)\n",
    "for k, v in policyMapQL.items():\n",
    "    policyMapQL[k] = defaultdict(lambda: 0, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(policyMapQL.keys())\n",
    "#42435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min([min(i.values()) for i in policyMapQL.values()]), max([max(i.values()) for i in policyMapQL.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values=[]\n",
    "for i in policyMapQL.values():\n",
    "    values.extend(list(i.values()))\n",
    "plt.hist(values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "positives = 0\n",
    "negatives = 0\n",
    "zeros = 0\n",
    "for i in policyMapQL.values():\n",
    "    values.extend(list(i.values()))\n",
    "for i in values:\n",
    "    if i > 0:\n",
    "        positives += 1\n",
    "    else:\n",
    "        if i < 0:\n",
    "            negatives += 1\n",
    "        else:\n",
    "            zeros += 1\n",
    "            \n",
    "print(\"Positives:\", positives, \", Negatives:\", negatives, \"Zeros:\", zeros)\n",
    "# Q-learning --> Positives: 7769 , Negatives: 10323 Zeros: 60107\n",
    "# Minimax --> Positives: 3529 , Negatives: 3979 Zeros: 25910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "num_cols = env.configuration.columns\n",
    "\n",
    "# Training agent in first position (player 1) against the negamax agent.\n",
    "trainer = env.train([None, \"negamax\"])\n",
    "\n",
    "discount = 0.9\n",
    "epsilon = 0.99 # parameter for epsilon-greedy policy\n",
    "alpha = 0.5 # learning rate\n",
    "t_max = 1000 # used for environments without end\n",
    "\n",
    "def get_best_action(board, num_cols, policy):\n",
    "    # greedy policy\n",
    "    q_value_max = float(\"-inf\")\n",
    "    best_action = None\n",
    "    # Convert board in a tuple to use it as the key in a dict structure\n",
    "    board_key = tuple(board)\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    for col in available_cols:\n",
    "        q_value = policy.get(board_key,dict()).get(col,0)\n",
    "        if q_value > q_value_max:\n",
    "            best_action = col\n",
    "            q_value_max = q_value\n",
    "    return best_action\n",
    "\n",
    "def get_action(board, num_cols, epsilon, policy):\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # epison-greedy policy\n",
    "    if (np.random.random() > epsilon):\n",
    "        # greedy\n",
    "        return get_best_action(board, num_cols, policy)\n",
    "    else:\n",
    "        # random\n",
    "        return int(np.random.choice(available_cols))\n",
    "\n",
    "t = datetime.datetime.now()\n",
    "print(\"Inicio\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "obs = trainer.reset()\n",
    "for i in range(50000):\n",
    "    done = False\n",
    "    t = 0\n",
    "    #env.render()\n",
    "    while not done and t < t_max:\n",
    "        # Convert board in a tuple to use it as the key in a dict structure\n",
    "        board_key = tuple(obs.board)\n",
    "        action = get_action(obs.board, num_cols, epsilon, policyMapQL)\n",
    "        obs, reward, done, info = trainer.step(action)\n",
    "        # Each step usually includes two steps (my movement and opponent's movement), so I need to update the policy for both of them (I can play as player 1 or 2)\n",
    "        # If my movement is a winning one, there is only one step\n",
    "        # Update policy. Q(s,a) = (1 - alpha) * Q(s,a) + alpha * (r + gamma * max(Q(s',a')))\n",
    "        \n",
    "        # If my movement IS NOT a wining movement, we will have two steps\n",
    "        if not(done and reward==1):\n",
    "            # Update policy with my movement: My board - board_key, My action - action, My board after action - env.steps[-2][0][\"observation\"][\"board\"]\n",
    "            board_step1_key = board_key\n",
    "            action_step1 = action\n",
    "            reward_step1 = env.steps[-2][0][\"reward\"]\n",
    "            new_board_step1 = env.steps[-2][0][\"observation\"][\"board\"]\n",
    "            new_board_step1_key = tuple(new_board_step1)\n",
    "            new_action_step1 = get_best_action(new_board_step1, num_cols, policyMapQL)\n",
    "            policyMapQL[board_step1_key][action_step1] = ((1 - alpha) * policyMapQL[board_step1_key][action_step1]\n",
    "                                                          + alpha * (reward_step1 + discount * policyMapQL[new_board_step1_key][new_action_step1]))\n",
    "            \n",
    "            # Update policy with opponent's movement: board - env.steps[-2][0][\"observation\"][\"board\"], action - env.steps[-1][1][\"action\"], board after action - obs.board\n",
    "            board_step2_key = new_board_step1_key\n",
    "            action_step2 = env.steps[-1][1][\"action\"]\n",
    "            reward_step2 = reward\n",
    "            new_board_step2 = obs.board\n",
    "            new_board_step2_key = tuple(new_board_step2)\n",
    "            new_action_step2 = get_best_action(new_board_step2, num_cols, policyMapQL)\n",
    "            policyMapQL[board_step2_key][action_step2] = ((1 - alpha) * policyMapQL[board_step2_key][action_step2]\n",
    "                                                          + alpha * (reward + discount * policyMapQL[new_board_step2_key][new_action_step2]))\n",
    "        # If my movement IS a wining movement, we will have only one step\n",
    "        else:\n",
    "            new_board = obs.board\n",
    "            new_board_key = tuple(new_board)\n",
    "            new_action = get_best_action(new_board, num_cols, policyMapQL)\n",
    "            policyMapQL[board_key][action] = ((1 - alpha) * policyMapQL[board_key][action]\n",
    "                                              + alpha * (reward + discount * policyMapQL[new_board_key][new_action]))\n",
    "            \n",
    "        t += 1\n",
    "    # Decrease epsilon in every step so that exploration (random) is getting smaller against exploitation (greedy)\n",
    "    epsilon *= 0.99999\n",
    "    obs = trainer.reset()\n",
    "            \n",
    "    if (i % 10000) == 0:\n",
    "        print(i, datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        \n",
    "t = datetime.datetime.now()\n",
    "print(\"Fin\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# 50000\n",
    "# 22:11:28\n",
    "# 23:57:00\n",
    "# 50000\n",
    "# 11:39:03\n",
    "# 12:24:00\n",
    "# 50000\n",
    "# 12:40:40\n",
    "# 13:24:49\n",
    "# 50000\n",
    "# 13:28:24\n",
    "# 14:03:10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save policyMap to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyMapQL_dict = dict(policyMapQL)\n",
    "policyMapQL_dict = {k: dict(v) for k, v in policyMapQL_dict.items()}\n",
    "\n",
    "with open(r'policyMapQL.dict','w+') as f:\n",
    "     f.write(str(policyMapQL_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the agent as a function, so that we can use it with Kaggle environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "def agent_QL(observation, configuration):\n",
    "    \n",
    "    ################################## START NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    # Given a board (as a list) and a column, returns the index of the first row available\n",
    "    def first_row_avail(board, col, num_rows, num_cols):\n",
    "        \"\"\"\n",
    "        Given a board (as a list) and a column, returns the index of the first row available\n",
    "        \"\"\"\n",
    "        index = -1\n",
    "        for i in range(num_rows): # from top to bottom\n",
    "            board_index = col + (i * num_cols)\n",
    "            if board[board_index] == 0:\n",
    "                index = board_index\n",
    "            else:\n",
    "                return index\n",
    "        return index\n",
    "            \n",
    "    ################################## END NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    print(configuration) # {rows: 5, columns: 5, inarow: 3}\n",
    "    # Number of rows on the board\n",
    "    num_rows = configuration.rows\n",
    "    # Number of columns on the board\n",
    "    num_cols = configuration.columns\n",
    "    # Number of checkers \"in a row\" needed to win\n",
    "    in_a_row = configuration.inarow\n",
    "    print(observation) # {board: [...], mark: 1}\n",
    "    # The current serialized board (rows x columns) - array [rows x columns] with top row first\n",
    "    board = observation.board\n",
    "    # Which player the agent is playing as (1 or 2).\n",
    "    mark = observation.mark\n",
    "    # If the board is empty, put the first checker in the middle column\n",
    "    if max(board)==0:\n",
    "        return round(num_cols/2)\n",
    "    \n",
    "    # Load the policiMap from file\n",
    "    dic = ''\n",
    "    with open(r'policyMapQL.dict','r') as f:\n",
    "        for i in f.readlines():\n",
    "            dic=i #string\n",
    "    policyMapQL_dict = eval(dic)\n",
    "    policyMapQL = defaultdict(lambda: defaultdict(lambda: 0), policyMapQL_dict)\n",
    "    for k, v in policyMapQL.items():\n",
    "        policyMapQL[k] = defaultdict(lambda: 0, v)\n",
    "    \n",
    "    # List of available columns\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # Convert the list board into a two-dimensional array\n",
    "    #board_array=np.array(board)\n",
    "    #board_array=board_array.reshape((num_rows,num_cols))\n",
    "    # Evaluate the target board with a checker in each available column, and get the best one\n",
    "    reward_list=[] # List with rewards of each movement\n",
    "    for target_col in available_cols:\n",
    "        target_board = board.copy()\n",
    "        target_index = first_row_avail(board, target_col, num_rows, num_cols)\n",
    "        #target_board[target_index] = mark\n",
    "        # Convert target_board into a tuple so that we can use it as a key for the policyMap\n",
    "        board_key = tuple(target_board)\n",
    "        reward = policyMapQL[board_key][target_col]\n",
    "        reward_list.append(reward)\n",
    "        print(\"Column:\"+str(target_col)+\" Reward:\"+str(reward))\n",
    "    # Get the highest reward if the agent is player 1 and the lowest if is player 2. The policy has been learned with player 1\n",
    "    if mark == 1:\n",
    "        max_reward = max(reward_list)\n",
    "    else:\n",
    "        #max_reward = min(reward_list)\n",
    "        # Choose only columns with non-zero reward, because zero reward means unknown states, and the agent plays randomly in these cases\n",
    "        # If every non-zero value is close to 1 (means losing the game), it's better to take a chance with zero values\n",
    "        max_reward = min([value for value in reward_list if value!=0 and value<0.98], default=0)\n",
    "    # Return the column with the highest reward. If several actions have the same reward, select randomly one of them\n",
    "    max_rewards_index = [i for i, x in enumerate(reward_list) if x == max_reward]\n",
    "    # Check if max_rewards_index is empty (every value is zero or almost 1). In this case take a column randomly\n",
    "    if max_rewards_index == []:\n",
    "        max_rewards_index = available_cols\n",
    "    return available_cols[int(np.random.choice(max_rewards_index))]\n",
    "\n",
    "# Run an episode using the agent above vs the negamax agent.\n",
    "#env.run([agent_QL, \"negamax\"])\n",
    "#env.run([\"negamax\", agent_QL])\n",
    "#env.render(mode=\"ipython\", width=500, height=450)\n",
    "#env.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par=0\n",
    "impar=0\n",
    "for k in policyMapQL.keys():\n",
    "    if (sum([i != 0 for i in k]) % 2) == 0:\n",
    "        par += 1\n",
    "    else:\n",
    "        impar += 1\n",
    "par, impar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'evaluate' returns a list of lists (one list per episode).  \n",
    "These lists per episode contains two elements (one per player), and these elements have these possible values:\n",
    "- 1: This player wins\n",
    "- -1: This player loses\n",
    "- 0: Draw (I guess there will be a value for draw)  \n",
    "Example:  \n",
    "[[1, -1], # First episode: Player 1 won  \n",
    " [1, -1], # Second episode: Player 1 won  \n",
    " [1, -1], # Third episode: Player 1 won  \n",
    " [1, -1], # ...  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1],  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "help(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "#evaluate(\"connectx\", [agent_MC, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "evaluate(\"connectx\", [\"random\", agent_QL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "#evaluate(\"connectx\", [agent, \"negamax\"], num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our agent against random and negamax agent.  \n",
    "- The closer the value is to 1 the better is player 1 agent (wins more times than loses)\n",
    "- The closer the value is to -1 the worse is player 1 agent (loses more times than wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "\n",
    "# Run multiple episodes to estimate its performance.\n",
    "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [agent_QL, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [agent_QL, \"negamax\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Random Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", agent_QL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Negamax Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", agent_QL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/itertools.html\n",
    "from itertools import permutations\n",
    "from kaggle_environments import evaluate\n",
    "\n",
    "\n",
    "def name(obj):\n",
    "    \"\"\"\n",
    "    If obj is a function, gets the name instead of something like <function agent_MC at 0x0000023B01C3C950>\n",
    "    \"\"\"\n",
    "    if callable(obj):\n",
    "        return getattr(obj, \"__name__\")\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "    \n",
    "conf={\"rows\": 5, \"columns\": 5, \"inarow\": 3}\n",
    "\n",
    "list_agents = [\"random\", \"negamax\", agent_MC, agent_QL]\n",
    "list_matches = list(permutations(list_agents, 2))\n",
    "results = {}\n",
    "\n",
    "for players in list_matches:\n",
    "    print(\"Playing\", name(players[0]), \"vs\", name(players[1]))\n",
    "    list_players = list(players)\n",
    "    result = mean_reward(evaluate(\"connectx\", agents=list_players, configuration=conf, num_episodes=10))\n",
    "    print(result)\n",
    "    results[name(players[0])] = results.get(name(players[0]), 0) + result\n",
    "    results[name(players[1])] = results.get(name(players[1]), 0) - result\n",
    "\n",
    "print(results)\n",
    "\n",
    "# Display results ordered by the points obtained\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"{:<10} {:<10} {:<6}\".format('Position','Team','Points')) # https://docs.python.org/3/library/string.html#formatspec\n",
    "for i, result in enumerate(sorted_results):\n",
    "    print(\"{:<10} {:<10} {:>6.2f}\".format(i+1, result[0], result[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Temporal Differece Sarsa  \n",
    "Unlike MonteCarlo, in this algorithm we try to get Q-values (value for pair state, action) and we don't need the full episode to learn (bootstrapping).  \n",
    "Unlike Q-learning, this is an on-policy method.  \n",
    "On Policy. Learn policy $ \\pi $ (epsilon-greedy: greedy or random depending on a random value) using the same policy $ \\pi $ (epsilon-greedy).  \n",
    "This is useful when you want a conservative policy (penalty of failing is very high). As the policy acts randomly sometimes, it learns to be \"far\" from failing, so that these random actions don't cause the agent to be unsuccessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Nested dictionary. We use defaultdict because it avoids issues with unexisting keys.\n",
    "policyMapSA = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "rewards_list_sarsa = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load policyMap from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "dic = ''\n",
    "with open(r'policyMapSA.dict','r') as f:\n",
    "         for i in f.readlines():\n",
    "            dic=i #string\n",
    "policyMapSA_dict = eval(dic)\n",
    "policyMapSA = defaultdict(lambda: defaultdict(lambda: 0), policyMapSA_dict)\n",
    "for k, v in policyMapSA.items():\n",
    "    policyMapSA[k] = defaultdict(lambda: 0, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(policyMapSA.keys())\n",
    "# 11896\n",
    "# 20048\n",
    "# 25679\n",
    "# 30856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min([min(i.values()) for i in policyMapSA.values()]), max([max(i.values()) for i in policyMapSA.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values=[]\n",
    "for i in policyMapSA.values():\n",
    "    values.extend(list(i.values()))\n",
    "plt.hist(values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/reading-and-writing-lists-to-a-file-in-python/\n",
    "rewards_list_sarsa = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('rewards_list_sarsa.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        rewards_list_sarsa.append(int(currentPlace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rewards_list_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "num_cols = env.configuration.columns\n",
    "\n",
    "# Training agent in first position (player 1) against the negamax agent.\n",
    "trainer = env.train([None, \"negamax\"])\n",
    "\n",
    "discount = 0.9\n",
    "epsilon = 0.20 # parameter for epsilon-greedy policy\n",
    "alpha = 0.5 # learning rate\n",
    "t_max = 1000 # used for environments without end\n",
    "total_reward = 0 # used for displaying the evolution of the rewards as the model learns\n",
    "\n",
    "def get_best_action(board, num_cols, policy):\n",
    "    # greedy policy\n",
    "    q_value_max = float(\"-inf\")\n",
    "    best_action = None\n",
    "    # Convert board in a tuple to use it as the key in a dict structure\n",
    "    board_key = tuple(board)\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    for col in available_cols:\n",
    "        q_value = policy.get(board_key,dict()).get(col,0)\n",
    "        if q_value > q_value_max:\n",
    "            best_action = col\n",
    "            q_value_max = q_value\n",
    "    return best_action\n",
    "\n",
    "def get_action(board, num_cols, epsilon, policy):\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # epison-greedy policy\n",
    "    if (np.random.random() > epsilon):\n",
    "        # greedy\n",
    "        return get_best_action(board, num_cols, policy)\n",
    "    else:\n",
    "        # random\n",
    "        return int(np.random.choice(available_cols))\n",
    "\n",
    "t = datetime.datetime.now()\n",
    "print(\"Inicio\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "obs = trainer.reset()\n",
    "for i in range(50000):\n",
    "    done = False\n",
    "    t = 0\n",
    "    #env.render()\n",
    "    while not done and t < t_max:\n",
    "        # Convert board in a tuple to use it as the key in a dict structure\n",
    "        board_key = tuple(obs.board)\n",
    "        action = get_action(obs.board, num_cols, epsilon, policyMapSA)\n",
    "        obs, reward, done, info = trainer.step(action)\n",
    "        # Each step usually includes two steps (my movement and opponent's movement), so I need to update the policy for both of them (I can play as player 1 or 2)\n",
    "        # If my movement is a winning one, there is only one step\n",
    "        # Update policy. Q(s,a) = (1 - alpha) * Q(s,a) + alpha * (r + gamma * max(Q(s',a')))\n",
    "        \n",
    "        # If my movement IS NOT a wining movement, we will have two steps\n",
    "        if not(done and reward==1):\n",
    "            # Update policy with my movement: My board - board_key, My action - action, My board after action - env.steps[-2][0][\"observation\"][\"board\"]\n",
    "            board_step1_key = board_key\n",
    "            action_step1 = action\n",
    "            reward_step1 = env.steps[-2][0][\"reward\"]\n",
    "            new_board_step1 = env.steps[-2][0][\"observation\"][\"board\"]\n",
    "            new_board_step1_key = tuple(new_board_step1)\n",
    "            new_action_step1 = get_action(new_board_step1, num_cols, epsilon, policyMapSA)\n",
    "            policyMapSA[board_step1_key][action_step1] = ((1 - alpha) * policyMapSA[board_step1_key][action_step1]\n",
    "                                                          + alpha * (reward_step1 + discount * policyMapSA[new_board_step1_key][new_action_step1]))\n",
    "            \n",
    "            # Update policy with opponent's movement: board - env.steps[-2][0][\"observation\"][\"board\"], action - env.steps[-1][1][\"action\"], board after action - obs.board\n",
    "            board_step2_key = new_board_step1_key\n",
    "            action_step2 = env.steps[-1][1][\"action\"]\n",
    "            reward_step2 = reward\n",
    "            new_board_step2 = obs.board\n",
    "            new_board_step2_key = tuple(new_board_step2)\n",
    "            new_action_step2 = get_action(new_board_step2, num_cols, epsilon, policyMapSA)\n",
    "            policyMapSA[board_step2_key][action_step2] = ((1 - alpha) * policyMapSA[board_step2_key][action_step2]\n",
    "                                                          + alpha * (reward + discount * policyMapSA[new_board_step2_key][new_action_step2]))\n",
    "        # If my movement IS a wining movement, we will have only one step\n",
    "        else:\n",
    "            new_board = obs.board\n",
    "            new_board_key = tuple(new_board)\n",
    "            new_action = get_action(new_board, num_cols, epsilon, policyMapSA)\n",
    "            policyMapSA[board_key][action] = ((1 - alpha) * policyMapSA[board_key][action]\n",
    "                                              + alpha * (reward + discount * policyMapSA[new_board_key][new_action]))\n",
    "            \n",
    "        total_reward += reward\n",
    "            \n",
    "        t += 1\n",
    "        \n",
    "    rewards_list_sarsa.append(total_reward)\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Decrease epsilon in every step so that exploration (random) is getting smaller against exploitation (greedy)\n",
    "    #epsilon *= 0.99999\n",
    "    obs = trainer.reset()\n",
    "            \n",
    "    if (i % 10000) == 0:\n",
    "        print(i, datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        \n",
    "t = datetime.datetime.now()\n",
    "print(\"Fin\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# 50000\n",
    "# 22:32:37\n",
    "# 00:19:30\n",
    "# 50000\n",
    "# 22:01:43\n",
    "# 22:44:54\n",
    "# 50000\n",
    "# 23:14:03\n",
    "# 23:55:06\n",
    "# 50000\n",
    "# 19:41:03\n",
    "# 20:23:28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.unique(rewards_list_sarsa, return_counts=True)\n",
    "# (array([-1,  1]), array([21694, 28306], dtype=int64))\n",
    "# (array([-1,  1]), array([41956, 58044], dtype=int64))\n",
    "# (array([-1,  1]), array([61763, 88237], dtype=int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html\n",
    "# ewm --> Weighted average of the values from x0 to xt. The weight of each element is decreasing from xt (weight=1) to xt-n (weight=(1-alpha)^n). alpha=2/(span+1)=2/101\n",
    "# This function gives us the trend of the rewards, instead of a sharp graph with a lot of -1's and 1's\n",
    "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "# Displaying the average of n elemtens instead of each element of the list \n",
    "n = 1000\n",
    "moving_average2 = [sum(rewards_list_sarsa[i:i+n])/n for i in range(0,len(rewards_list_sarsa)-n+1)]\n",
    "\n",
    "clear_output(True)\n",
    "print('SARSA mean reward =', np.mean(rewards_list_sarsa[-1000:]))\n",
    "plt.title(\"epsilon = %s\" % epsilon)\n",
    "#plt.plot(rewards_list_sarsa, label='ev_sarsa')\n",
    "plt.plot(moving_average(rewards_list_sarsa), label='ev_sarsa')\n",
    "#plt.plot(moving_average2, label='ev_sarsa2')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(-2, 2)\n",
    "plt.show()\n",
    "\n",
    "# SARSA mean reward = 0.24\n",
    "# SARSA mean reward = 0.204\n",
    "# SARSA mean reward = 0.208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rewards_list_sarsa[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save policyMap to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyMapSA_dict = dict(policyMapSA)\n",
    "policyMapSA_dict = {k: dict(v) for k, v in policyMapSA_dict.items()}\n",
    "\n",
    "with open(r'policyMapSA.dict','w+') as f:\n",
    "     f.write(str(policyMapSA_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'rewards_list_sarsa.txt','w+') as f:\n",
    "    for element in rewards_list_sarsa:\n",
    "        f.write(str(element) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the agent as a function, so that we can use it with Kaggle environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "def agent_SA(observation, configuration):\n",
    "    \n",
    "    ################################## START NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    # Given a board (as a list) and a column, returns the index of the first row available\n",
    "    def first_row_avail(board, col, num_rows, num_cols):\n",
    "        \"\"\"\n",
    "        Given a board (as a list) and a column, returns the index of the first row available\n",
    "        \"\"\"\n",
    "        index = -1\n",
    "        for i in range(num_rows): # from top to bottom\n",
    "            board_index = col + (i * num_cols)\n",
    "            if board[board_index] == 0:\n",
    "                index = board_index\n",
    "            else:\n",
    "                return index\n",
    "        return index\n",
    "            \n",
    "    ################################## END NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    print(configuration) # {rows: 5, columns: 5, inarow: 3}\n",
    "    # Number of rows on the board\n",
    "    num_rows = configuration.rows\n",
    "    # Number of columns on the board\n",
    "    num_cols = configuration.columns\n",
    "    # Number of checkers \"in a row\" needed to win\n",
    "    in_a_row = configuration.inarow\n",
    "    print(observation) # {board: [...], mark: 1}\n",
    "    # The current serialized board (rows x columns) - array [rows x columns] with top row first\n",
    "    board = observation.board\n",
    "    # Which player the agent is playing as (1 or 2).\n",
    "    mark = observation.mark\n",
    "    # If the board is empty, put the first checker in the middle column\n",
    "    if max(board)==0:\n",
    "        return round(num_cols/2)\n",
    "    \n",
    "    # Load the policiMap from file\n",
    "    dic = ''\n",
    "    with open(r'policyMapSA.dict','r') as f:\n",
    "        for i in f.readlines():\n",
    "            dic=i #string\n",
    "    policyMapSA_dict = eval(dic)\n",
    "    policyMapSA = defaultdict(lambda: defaultdict(lambda: 0), policyMapSA_dict)\n",
    "    for k, v in policyMapSA.items():\n",
    "        policyMapSA[k] = defaultdict(lambda: 0, v)\n",
    "    \n",
    "    # List of available columns\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # Convert the list board into a two-dimensional array\n",
    "    #board_array=np.array(board)\n",
    "    #board_array=board_array.reshape((num_rows,num_cols))\n",
    "    # Evaluate the target board with a checker in each available column, and get the best one\n",
    "    reward_list=[] # List with rewards of each movement\n",
    "    for target_col in available_cols:\n",
    "        target_board = board.copy()\n",
    "        target_index = first_row_avail(board, target_col, num_rows, num_cols)\n",
    "        #target_board[target_index] = mark\n",
    "        # Convert target_board into a tuple so that we can use it as a key for the policyMap\n",
    "        board_key = tuple(target_board)\n",
    "        reward = policyMapSA[board_key][target_col]\n",
    "        reward_list.append(reward)\n",
    "        print(\"Column:\"+str(target_col)+\" Reward:\"+str(reward))\n",
    "    # Get the highest reward if the agent is player 1 and the lowest if is player 2. The policy has been learned with player 1\n",
    "    if mark == 1:\n",
    "        max_reward = max(reward_list)\n",
    "    else:\n",
    "        # max_reward = min(reward_list)\n",
    "        # Choose only columns with non-zero reward, because zero reward means unknown states, and the agent plays randomly in these cases\n",
    "        # If every non-zero value is close to 1 (means losing the game), it's better to take a chance with zero values\n",
    "        max_reward = min([value for value in reward_list if value!=0 and value<0.98], default=0)\n",
    "    # Return the column with the highest reward. If several actions have the same reward, select randomly one of them\n",
    "    max_rewards_index = [i for i, x in enumerate(reward_list) if x == max_reward]\n",
    "    # Check if max_rewards_index is empty (every value is zero or almost 1). In this case take a column randomly\n",
    "    if max_rewards_index == []:\n",
    "        max_rewards_index = available_cols\n",
    "    # epison-greedy policy\n",
    "    epsilon = 0.2\n",
    "    if (np.random.random() > epsilon):\n",
    "        # greedy\n",
    "        print(\"Best action selected\")\n",
    "        return available_cols[int(np.random.choice(max_rewards_index))]\n",
    "    else:\n",
    "        # random\n",
    "        print(\"Random action selected\")\n",
    "        return int(np.random.choice(available_cols))\n",
    "#    return available_cols[int(np.random.choice(max_rewards_index))]\n",
    "\n",
    "# Run an episode using the agent above vs the negamax agent.\n",
    "#env.run([agent_SA, \"negamax\"])\n",
    "#env.run([\"negamax\", agent_SA])\n",
    "#env.run([\"random\", agent_SA])\n",
    "#env.render(mode=\"ipython\", width=500, height=450)\n",
    "#env.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par=0\n",
    "impar=0\n",
    "for k in policyMapSA.keys():\n",
    "    if (sum([i != 0 for i in k]) % 2) == 0:\n",
    "        par += 1\n",
    "    else:\n",
    "        impar += 1\n",
    "par, impar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'evaluate' returns a list of lists (one list per episode).  \n",
    "These lists per episode contains two elements (one per player), and these elements have these possible values:\n",
    "- 1: This player wins\n",
    "- -1: This player loses\n",
    "- 0: Draw (I guess there will be a value for draw)  \n",
    "Example:  \n",
    "[[1, -1], # First episode: Player 1 won  \n",
    " [1, -1], # Second episode: Player 1 won  \n",
    " [1, -1], # Third episode: Player 1 won  \n",
    " [1, -1], # ...  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1],  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "help(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "#evaluate(\"connectx\", [agent_MC, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "evaluate(\"connectx\", [\"random\", agent_SA], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10, debug=True)\n",
    "#evaluate(\"connectx\", [agent, \"negamax\"], num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our agent against random and negamax agent.  \n",
    "- The closer the value is to 1 the better is player 1 agent (wins more times than loses)\n",
    "- The closer the value is to -1 the worse is player 1 agent (loses more times than wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "\n",
    "# Run multiple episodes to estimate its performance.\n",
    "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [agent_SA, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [agent_SA, \"negamax\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Random Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", agent_SA], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Negamax Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", agent_SA], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/itertools.html\n",
    "from itertools import permutations\n",
    "from kaggle_environments import evaluate\n",
    "\n",
    "\n",
    "def name(obj):\n",
    "    \"\"\"\n",
    "    If obj is a function, gets the name instead of something like <function agent_MC at 0x0000023B01C3C950>\n",
    "    \"\"\"\n",
    "    if callable(obj):\n",
    "        return getattr(obj, \"__name__\")\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "    \n",
    "conf={\"rows\": 5, \"columns\": 5, \"inarow\": 3}\n",
    "\n",
    "list_agents = [\"random\", \"negamax\", agent_MC, agent_QL, agent_SA]\n",
    "list_matches = list(permutations(list_agents, 2))\n",
    "results = {}\n",
    "\n",
    "for players in list_matches:\n",
    "    print(\"Playing\", name(players[0]), \"vs\", name(players[1]))\n",
    "    list_players = list(players)\n",
    "    result = mean_reward(evaluate(\"connectx\", agents=list_players, configuration=conf, num_episodes=10))\n",
    "    print(result)\n",
    "    results[name(players[0])] = results.get(name(players[0]), 0) + result\n",
    "    results[name(players[1])] = results.get(name(players[1]), 0) - result\n",
    "\n",
    "#print(results)\n",
    "\n",
    "# Display results ordered by the points obtained\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"{:<10} {:<10} {:<6}\".format('Position','Team','Points')) # https://docs.python.org/3/library/string.html#formatspec\n",
    "for i, result in enumerate(sorted_results):\n",
    "    print(\"{:<10} {:<10} {:>6.2f}\".format(i+1, result[0], result[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Temporal Differece Expected Sarsa  \n",
    "Unlike MonteCarlo, in this algorithm we try to get Q-values (value for pair state, action) and we don't need the full episode to learn (bootstrapping).  \n",
    "Unlike Q-learning, this is an on-policy method.  \n",
    "On Policy. Learn policy $ \\pi $ (epsilon-greedy: greedy or random depending on a random value) using the same policy $ \\pi $ (epsilon-greedy).  \n",
    "This is useful when you want a conservative policy (penalty of failing is very high). As the policy acts randomly sometimes, it learns to be \"far\" from failing, so that these random actions don't cause the agent to be unsuccessful.  \n",
    "Unlike Sarsa, it uses the weighted average of the next possible Q-values to update the current Q-value. In this case every action has the same probability, so we use the average instead of the weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Nested dictionary. We use defaultdict because it avoids issues with unexisting keys.\n",
    "policyMapESA = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "rewards_list_exp_sarsa = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load policyMap from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "dic = ''\n",
    "with open(r'policyMapESA.dict','r') as f:\n",
    "         for i in f.readlines():\n",
    "            dic=i #string\n",
    "policyMapESA_dict = eval(dic)\n",
    "policyMapESA = defaultdict(lambda: defaultdict(lambda: 0), policyMapESA_dict)\n",
    "for k, v in policyMapESA.items():\n",
    "    policyMapESA[k] = defaultdict(lambda: 0, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(policyMapESA.keys())\n",
    "# 12011\n",
    "# 14351\n",
    "# 16018\n",
    "# 17388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min([min(i.values()) for i in policyMapESA.values()]), max([max(i.values()) for i in policyMapESA.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values=[]\n",
    "for i in policyMapESA.values():\n",
    "    values.extend(list(i.values()))\n",
    "plt.hist(values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/reading-and-writing-lists-to-a-file-in-python/\n",
    "rewards_list_exp_sarsa = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('rewards_list_exp_sarsa.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        rewards_list_exp_sarsa.append(int(currentPlace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rewards_list_exp_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "num_cols = env.configuration.columns\n",
    "\n",
    "# Training agent in first position (player 1) against the negamax agent.\n",
    "trainer = env.train([None, \"negamax\"])\n",
    "\n",
    "discount = 0.9\n",
    "epsilon = 0.20 # parameter for epsilon-greedy policy\n",
    "alpha = 0.5 # learning rate\n",
    "t_max = 1000 # used for environments without end\n",
    "total_reward = 0 # used for displaying the evolution of the rewards as the model learns\n",
    "\n",
    "def get_best_action(board, num_cols, policy):\n",
    "    # greedy policy\n",
    "    q_value_max = float(\"-inf\")\n",
    "    best_action = None\n",
    "    # Convert board in a tuple to use it as the key in a dict structure\n",
    "    board_key = tuple(board)\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    for col in available_cols:\n",
    "        q_value = policy.get(board_key,dict()).get(col,0)\n",
    "        if q_value > q_value_max:\n",
    "            best_action = col\n",
    "            q_value_max = q_value\n",
    "    return best_action\n",
    "\n",
    "def get_action(board, num_cols, epsilon, policy):\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # epison-greedy policy\n",
    "    if (np.random.random() > epsilon):\n",
    "        # greedy\n",
    "        return get_best_action(board, num_cols, policy)\n",
    "    else:\n",
    "        # random\n",
    "        return int(np.random.choice(available_cols))\n",
    "\n",
    "def get_avg_next_qvalue(board, num_cols, policy):\n",
    "    \"\"\"\n",
    "    Returns the average of all the possible next Q-values (board, action)\n",
    "    \"\"\"\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # Convert board in a tuple to use it as the key in a dict structure\n",
    "    board_key = tuple(board)\n",
    "    return np.mean([policy.get(board_key,dict()).get(action,0) for action in available_cols])\n",
    "\n",
    "t = datetime.datetime.now()\n",
    "print(\"Inicio\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "obs = trainer.reset()\n",
    "for i in range(50000):\n",
    "    done = False\n",
    "    t = 0\n",
    "    #env.render()\n",
    "    while not done and t < t_max:\n",
    "        # Convert board in a tuple to use it as the key in a dict structure\n",
    "        board_key = tuple(obs.board)\n",
    "        action = get_action(obs.board, num_cols, epsilon, policyMapESA)\n",
    "        obs, reward, done, info = trainer.step(action)\n",
    "        # Each step usually includes two steps (my movement and opponent's movement), so I need to update the policy for both of them (I can play as player 1 or 2)\n",
    "        # If my movement is a winning one, there is only one step\n",
    "        # Update policy. Q(s,a) = (1 - alpha) * Q(s,a) + alpha * (r + gamma * max(Q(s',a')))\n",
    "        \n",
    "        # If my movement IS NOT a wining movement, we will have two steps\n",
    "        if not(done and reward==1):\n",
    "            # Update policy with my movement: My board - board_key, My action - action, My board after action - env.steps[-2][0][\"observation\"][\"board\"]\n",
    "            board_step1_key = board_key\n",
    "            action_step1 = action\n",
    "            reward_step1 = env.steps[-2][0][\"reward\"]\n",
    "            new_board_step1 = env.steps[-2][0][\"observation\"][\"board\"]\n",
    "            new_board_step1_key = tuple(new_board_step1)\n",
    "            policyMapESA[board_step1_key][action_step1] = ((1 - alpha) * policyMapESA[board_step1_key][action_step1]\n",
    "                                                          + alpha * (reward_step1 + discount * get_avg_next_qvalue(new_board_step1, num_cols, policyMapESA)))\n",
    "            \n",
    "            # Update policy with opponent's movement: board - env.steps[-2][0][\"observation\"][\"board\"], action - env.steps[-1][1][\"action\"], board after action - obs.board\n",
    "            board_step2_key = new_board_step1_key\n",
    "            action_step2 = env.steps[-1][1][\"action\"]\n",
    "            reward_step2 = reward\n",
    "            new_board_step2 = obs.board\n",
    "            policyMapESA[board_step2_key][action_step2] = ((1 - alpha) * policyMapESA[board_step2_key][action_step2]\n",
    "                                                          + alpha * (reward + discount * get_avg_next_qvalue(new_board_step2, num_cols, policyMapESA)))\n",
    "        # If my movement IS a wining movement, we will have only one step\n",
    "        else:\n",
    "            # Q-value of (winning_board, state) makes no sense because the game is over, so we simplify the expression\n",
    "            policyMapESA[board_key][action] = ((1 - alpha) * policyMapESA[board_key][action]) + alpha * reward\n",
    "            \n",
    "        total_reward += reward\n",
    "            \n",
    "        t += 1\n",
    "        \n",
    "    rewards_list_exp_sarsa.append(total_reward)\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Decrease epsilon in every step so that exploration (random) is getting smaller against exploitation (greedy)\n",
    "    #epsilon *= 0.99999\n",
    "    obs = trainer.reset()\n",
    "            \n",
    "    if (i % 10000) == 0:\n",
    "        print(i, datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        \n",
    "t = datetime.datetime.now()\n",
    "print(\"Fin\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# 50000\n",
    "# 22:16:03\n",
    "# 23:56:54\n",
    "# 50000\n",
    "# 14:25:04\n",
    "# 15:14:38\n",
    "# 50000\n",
    "# 15:26:16\n",
    "# 16:02:39\n",
    "# 50000\n",
    "# 21:55:01\n",
    "# 20:23:28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.unique(rewards_list_exp_sarsa, return_counts=True)\n",
    "# (array([-1,  1]), array([16915, 33085], dtype=int64))\n",
    "# (array([-1,  1]), array([30447, 69553], dtype=int64))\n",
    "# (array([-1,  1]), array([ 43716, 106284], dtype=int64))\n",
    "# (array([-1,  1]), array([ 56703, 143297], dtype=int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html\n",
    "# ewm --> Weighted average of the values from x0 to xt. The weight of each element is decreasing from xt (weight=1) to xt-n (weight=(1-alpha)^n). alpha=2/(span+1)=2/101\n",
    "# This function gives us the trend of the rewards, instead of a sharp graph with a lot of -1's and 1's\n",
    "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "# Displaying the average of n elemtens instead of each element of the list \n",
    "n = 1000\n",
    "moving_average2 = [sum(rewards_list_exp_sarsa[i:i+n])/n for i in range(0,len(rewards_list_exp_sarsa)-n+1)]\n",
    "\n",
    "clear_output(True)\n",
    "print('Expected SARSA mean reward =', np.mean(rewards_list_exp_sarsa[-1000:]))\n",
    "plt.title(\"epsilon = %s\" % epsilon)\n",
    "#plt.plot(rewards_list_exp_sarsa, label='ev_sarsa')\n",
    "plt.plot(moving_average(rewards_list_exp_sarsa), label='ev_sarsa')\n",
    "#plt.plot(moving_average2, label='ev_sarsa2')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(-2, 2)\n",
    "plt.show()\n",
    "\n",
    "# Expected SARSA mean reward = 0.438\n",
    "# Expected SARSA mean reward = 0.424\n",
    "# Expected SARSA mean reward = 0.492\n",
    "# Expected SARSA mean reward = 0.508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rewards_list_exp_sarsa[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save policyMap to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyMapESA_dict = dict(policyMapESA)\n",
    "policyMapESA_dict = {k: dict(v) for k, v in policyMapESA_dict.items()}\n",
    "\n",
    "with open(r'policyMapESA.dict','w+') as f:\n",
    "     f.write(str(policyMapESA_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'rewards_list_exp_sarsa.txt','w+') as f:\n",
    "    for element in rewards_list_exp_sarsa:\n",
    "        f.write(str(element) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the agent as a function, so that we can use it with Kaggle environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "def agent_ESA(observation, configuration):\n",
    "    \n",
    "    ################################## START NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    # Given a board (as a list) and a column, returns the index of the first row available\n",
    "    def first_row_avail(board, col, num_rows, num_cols):\n",
    "        \"\"\"\n",
    "        Given a board (as a list) and a column, returns the index of the first row available\n",
    "        \"\"\"\n",
    "        index = -1\n",
    "        for i in range(num_rows): # from top to bottom\n",
    "            board_index = col + (i * num_cols)\n",
    "            if board[board_index] == 0:\n",
    "                index = board_index\n",
    "            else:\n",
    "                return index\n",
    "        return index\n",
    "            \n",
    "    ################################## END NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    print(configuration) # {rows: 5, columns: 5, inarow: 3}\n",
    "    # Number of rows on the board\n",
    "    num_rows = configuration.rows\n",
    "    # Number of columns on the board\n",
    "    num_cols = configuration.columns\n",
    "    # Number of checkers \"in a row\" needed to win\n",
    "    in_a_row = configuration.inarow\n",
    "    print(observation) # {board: [...], mark: 1}\n",
    "    # The current serialized board (rows x columns) - array [rows x columns] with top row first\n",
    "    board = observation.board\n",
    "    # Which player the agent is playing as (1 or 2).\n",
    "    mark = observation.mark\n",
    "    # If the board is empty, put the first checker in the middle column\n",
    "    if max(board)==0:\n",
    "        return round(num_cols/2)\n",
    "    \n",
    "    # Load the policiMap from file\n",
    "    dic = ''\n",
    "    with open(r'policyMapESA.dict','r') as f:\n",
    "        for i in f.readlines():\n",
    "            dic=i #string\n",
    "    policyMapESA_dict = eval(dic)\n",
    "    policyMapESA = defaultdict(lambda: defaultdict(lambda: 0), policyMapESA_dict)\n",
    "    for k, v in policyMapESA.items():\n",
    "        policyMapESA[k] = defaultdict(lambda: 0, v)\n",
    "    \n",
    "    # List of available columns\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # Convert the list board into a two-dimensional array\n",
    "    #board_array=np.array(board)\n",
    "    #board_array=board_array.reshape((num_rows,num_cols))\n",
    "    # Evaluate the target board with a checker in each available column, and get the best one\n",
    "    reward_list=[] # List with rewards of each movement\n",
    "    for target_col in available_cols:\n",
    "        target_board = board.copy()\n",
    "        target_index = first_row_avail(board, target_col, num_rows, num_cols)\n",
    "        #target_board[target_index] = mark\n",
    "        # Convert target_board into a tuple so that we can use it as a key for the policyMap\n",
    "        board_key = tuple(target_board)\n",
    "        reward = policyMapESA[board_key][target_col]\n",
    "        reward_list.append(reward)\n",
    "        print(\"Column:\"+str(target_col)+\" Reward:\"+str(reward))\n",
    "    # Get the highest reward if the agent is player 1 and the lowest if is player 2. The policy has been learned with player 1\n",
    "    if mark == 1:\n",
    "        max_reward = max(reward_list)\n",
    "    else:\n",
    "        # max_reward = min(reward_list)\n",
    "        # Choose only columns with non-zero reward, because zero reward means unknown states, and the agent plays randomly in these cases\n",
    "        # If every non-zero value is close to 1 (means losing the game), it's better to take a chance with zero values\n",
    "        max_reward = min([value for value in reward_list if value!=0 and value<0.98], default=0)\n",
    "    # Return the column with the highest reward. If several actions have the same reward, select randomly one of them\n",
    "    max_rewards_index = [i for i, x in enumerate(reward_list) if x == max_reward]\n",
    "    # Check if max_rewards_index is empty (every value is zero or almost 1). In this case take a column randomly\n",
    "    if max_rewards_index == []:\n",
    "        max_rewards_index = available_cols\n",
    "    # epison-greedy policy\n",
    "    epsilon = 0.2\n",
    "    if (np.random.random() > epsilon):\n",
    "        # greedy\n",
    "        print(\"Best action selected\")\n",
    "        return available_cols[int(np.random.choice(max_rewards_index))]\n",
    "    else:\n",
    "        # random\n",
    "        print(\"Random action selected\")\n",
    "        return int(np.random.choice(available_cols))\n",
    "#    return available_cols[int(np.random.choice(max_rewards_index))]\n",
    "\n",
    "# Run an episode using the agent above vs the negamax agent.\n",
    "#env.run([agent_ESA, \"negamax\"])\n",
    "#env.run([\"negamax\", agent_ESA])\n",
    "#env.run([\"random\", agent_ESA])\n",
    "#env.render(mode=\"ipython\", width=500, height=450)\n",
    "#env.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par=0\n",
    "impar=0\n",
    "for k in policyMapESA.keys():\n",
    "    if (sum([i != 0 for i in k]) % 2) == 0:\n",
    "        par += 1\n",
    "    else:\n",
    "        impar += 1\n",
    "par, impar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'evaluate' returns a list of lists (one list per episode).  \n",
    "These lists per episode contains two elements (one per player), and these elements have these possible values:\n",
    "- 1: This player wins\n",
    "- -1: This player loses\n",
    "- 0: Draw (I guess there will be a value for draw)  \n",
    "Example:  \n",
    "[[1, -1], # First episode: Player 1 won  \n",
    " [1, -1], # Second episode: Player 1 won  \n",
    " [1, -1], # Third episode: Player 1 won  \n",
    " [1, -1], # ...  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1],  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "help(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "#evaluate(\"connectx\", [agent_MC, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "evaluate(\"connectx\", [\"random\", agent_ESA], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10, debug=True)\n",
    "#evaluate(\"connectx\", [agent, \"negamax\"], num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our agent against random and negamax agent.  \n",
    "- The closer the value is to 1 the better is player 1 agent (wins more times than loses)\n",
    "- The closer the value is to -1 the worse is player 1 agent (loses more times than wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "\n",
    "# Run multiple episodes to estimate its performance.\n",
    "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [agent_ESA, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [agent_ESA, \"negamax\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Random Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", agent_ESA], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Negamax Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", agent_ESA], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/itertools.html\n",
    "from itertools import permutations\n",
    "from kaggle_environments import evaluate\n",
    "\n",
    "\n",
    "def name(obj):\n",
    "    \"\"\"\n",
    "    If obj is a function, gets the name instead of something like <function agent_MC at 0x0000023B01C3C950>\n",
    "    \"\"\"\n",
    "    if callable(obj):\n",
    "        return getattr(obj, \"__name__\")\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "    \n",
    "conf={\"rows\": 5, \"columns\": 5, \"inarow\": 3}\n",
    "\n",
    "list_agents = [\"random\", \"negamax\", agent_MC, agent_QL, agent_SA, agent_ESA]\n",
    "list_matches = list(permutations(list_agents, 2))\n",
    "results = {}\n",
    "\n",
    "for players in list_matches:\n",
    "    print(\"Playing\", name(players[0]), \"vs\", name(players[1]))\n",
    "    list_players = list(players)\n",
    "    result = mean_reward(evaluate(\"connectx\", agents=list_players, configuration=conf, num_episodes=10))\n",
    "    print(result)\n",
    "    results[name(players[0])] = results.get(name(players[0]), 0) + result\n",
    "    results[name(players[1])] = results.get(name(players[1]), 0) - result\n",
    "\n",
    "#print(results)\n",
    "\n",
    "# Display results ordered by the points obtained\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"{:<10} {:<10} {:<6}\".format('Position','Team','Points')) # https://docs.python.org/3/library/string.html#formatspec\n",
    "for i, result in enumerate(sorted_results):\n",
    "    print(\"{:<10} {:<10} {:>6.2f}\".format(i+1, result[0], result[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Temporal Differece Minimax Q-learning  \n",
    "Unlike regular Q-learning, in this algorithm we get the best next action by using the maximum (for player1) or minimum (for player2) Q-value. In regular Q-learning we always use the maximum.  \n",
    "Off Policy. Learn policy $ \\pi $ (greedy) using policy $ \\mu $ (epsilon-greedy: greedy or random depending on a random value)  \n",
    "Based on Littman's paper: https://courses.cs.duke.edu//spring07/cps296.3/littman94markov.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Nested dictionary. We use defaultdict because it avoids issues with unexisting keys.\n",
    "policyMapMQL = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "rewards_list_minimax_ql = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load policyMap from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "dic = ''\n",
    "with open(r'policyMapMQL.dict','r') as f:\n",
    "         for i in f.readlines():\n",
    "            dic=i #string\n",
    "policyMapMQL_dict = eval(dic)\n",
    "policyMapMQL = defaultdict(lambda: defaultdict(lambda: 0), policyMapMQL_dict)\n",
    "for k, v in policyMapMQL.items():\n",
    "    policyMapMQL[k] = defaultdict(lambda: 0, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41549"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(policyMapMQL.keys())\n",
    "# 41549\n",
    "# 21422\n",
    "# 25804\n",
    "# 15809"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([min(i.values()) for i in policyMapMQL.values()]), max([max(i.values()) for i in policyMapMQL.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values=[]\n",
    "for i in policyMapMQL.values():\n",
    "    values.extend(list(i.values()))\n",
    "plt.hist(values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: 8391 , Negatives: 22705 Zeros: 70406\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "positives = 0\n",
    "negatives = 0\n",
    "zeros = 0\n",
    "for i in policyMapMQL.values():\n",
    "    values.extend(list(i.values()))\n",
    "for i in values:\n",
    "    if i > 0:\n",
    "        positives += 1\n",
    "    else:\n",
    "        if i < 0:\n",
    "            negatives += 1\n",
    "        else:\n",
    "            zeros += 1\n",
    "            \n",
    "print(\"Positives:\", positives, \", Negatives:\", negatives, \"Zeros:\", zeros)\n",
    "# Positives: 8391 , Negatives: 22705 Zeros: 70406\n",
    "# Positives: 2453 , Negatives: 8164 Zeros: 26689\n",
    "# Positives: 3228 , Negatives: 9583 Zeros: 31323\n",
    "# Positives: 4254 , Negatives: 11108 Zeros: 37044"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/reading-and-writing-lists-to-a-file-in-python/\n",
    "rewards_list_minimax_ql = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('rewards_list_minimax_ql.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        rewards_list_minimax_ql.append(int(currentPlace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewards_list_minimax_ql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n",
      "Inicio 20:10:54\n",
      "0 20:10:54\n",
      "10000 20:24:32\n",
      "20000 20:38:44\n",
      "30000 20:52:17\n",
      "40000 21:05:54\n",
      "Fin 21:19:42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "num_cols = env.configuration.columns\n",
    "\n",
    "# Training agent in first position (player 1) against the negamax agent.\n",
    "trainer = env.train([None, \"negamax\"])\n",
    "\n",
    "discount = 0.9\n",
    "epsilon = 0.99 # parameter for epsilon-greedy policy\n",
    "# epsilon = 0.2 # parameter for epsilon-greedy policy\n",
    "alpha = 0.5 # learning rate\n",
    "t_max = 1000 # used for environments without end\n",
    "total_reward = 0 # used for displaying the evolution of the rewards as the model learns\n",
    "\n",
    "def get_best_action(board, num_cols, policy, function):\n",
    "    \"\"\"\n",
    "    Returns the action with maximum q-value or minimum q-value depending on the needs\n",
    "    function parameter should be \"max\" or \"min\"\n",
    "    \"\"\"\n",
    "    # function parameter validation\n",
    "    if (function != \"max\" and function != \"min\"):\n",
    "        print(\"Parameter function should be \"\"max\"\" or \"\"min\"\"\")\n",
    "        return -1\n",
    "    \n",
    "    # greedy policy\n",
    "    if (function == \"max\"):\n",
    "        best_q_value = float(\"-inf\")\n",
    "    else:\n",
    "        best_q_value = float(\"inf\")\n",
    "    best_action = None\n",
    "    # Convert board in a tuple to use it as the key in a dict structure\n",
    "    board_key = tuple(board)\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    for col in available_cols:\n",
    "        q_value = policy.get(board_key,dict()).get(col,0)\n",
    "        if (function == \"max\" and q_value > best_q_value and q_value != 0):\n",
    "        # Condition q_value != 0 allows rewards to propagate to the initial states. Without this condition we usually take unexplored actions (with q_value==0),\n",
    "        # instead of explored ones (with q_value<0), as we are taking the maximum. Then, negative rewards didn't \"reach\" the initial states\n",
    "            best_action = col\n",
    "            best_q_value = q_value\n",
    "        else:\n",
    "            if (function == \"min\" and q_value < best_q_value and q_value != 0):\n",
    "            # Condition q_value != 0 allows rewards to propagate to the initial states. Without this condition we usually take unexplored actions (with q_value==0),\n",
    "            # instead of explored ones (with q_value>0), as we are taking the minimum. Then, positive rewards didn't \"reach\" the initial states\n",
    "                best_action = col\n",
    "                best_q_value = q_value\n",
    "                \n",
    "        # If every q_value is zero, select randomly one action\n",
    "        if (best_action == None):\n",
    "            best_action = int(np.random.choice(available_cols))\n",
    "            best_q_value = 0\n",
    "            \n",
    "    return best_action\n",
    "\n",
    "def get_action(board, num_cols, epsilon, policy):\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # epison-greedy policy\n",
    "    if (np.random.random() > epsilon):\n",
    "        # greedy\n",
    "        return get_best_action(board, num_cols, policy, \"max\")\n",
    "    else:\n",
    "        # random\n",
    "        return int(np.random.choice(available_cols))\n",
    "\n",
    "t = datetime.datetime.now()\n",
    "print(\"Inicio\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "obs = trainer.reset()\n",
    "for i in range(50000):\n",
    "    done = False\n",
    "    t = 0\n",
    "    #env.render()\n",
    "    while not done and t < t_max:\n",
    "        # Convert board in a tuple to use it as the key in a dict structure\n",
    "        board_key = tuple(obs.board)\n",
    "        action = get_action(obs.board, num_cols, epsilon, policyMapMQL)\n",
    "        obs, reward, done, info = trainer.step(action)\n",
    "        # Each step usually includes two steps (my movement and opponent's movement), so I need to update the policy for both of them (I can play as player 1 or 2)\n",
    "        # If my movement is a winning one, there is only one step\n",
    "        # Update policy (player1). Q(s,a) = (1 - alpha) * Q(s,a) + alpha * (r + gamma * max(Q(s',a')))\n",
    "        # Update policy (player2). Q(s,a) = (1 - alpha) * Q(s,a) + alpha * (r + gamma * min(Q(s',a')))\n",
    "        \n",
    "        # If my movement IS NOT a wining movement, we will have two steps\n",
    "        if not(done and reward==1):\n",
    "            # Update policy with my movement: My board - board_key, My action - action, My board after action - env.steps[-2][0][\"observation\"][\"board\"]\n",
    "            board_step1_key = board_key\n",
    "            action_step1 = action\n",
    "            reward_step1 = env.steps[-2][0][\"reward\"]\n",
    "            new_board_step1 = env.steps[-2][0][\"observation\"][\"board\"]\n",
    "            new_board_step1_key = tuple(new_board_step1)\n",
    "            # Next action is for player 2, and its best action is the one with the minimum q_value\n",
    "            new_action_step1 = get_best_action(new_board_step1, num_cols, policyMapMQL, \"min\")\n",
    "            policyMapMQL[board_step1_key][action_step1] = ((1 - alpha) * policyMapMQL[board_step1_key][action_step1]\n",
    "                                                          + alpha * (reward_step1 + discount * policyMapMQL[new_board_step1_key][new_action_step1]))\n",
    "            \n",
    "            # Update policy with opponent's movement: board - env.steps[-2][0][\"observation\"][\"board\"], action - env.steps[-1][1][\"action\"], board after action - obs.board\n",
    "            board_step2_key = new_board_step1_key\n",
    "            action_step2 = env.steps[-1][1][\"action\"]\n",
    "            reward_step2 = reward\n",
    "            new_board_step2 = obs.board\n",
    "            new_board_step2_key = tuple(new_board_step2)\n",
    "            # Next action is for player 1, and its best action is the one with the maximum q_value\n",
    "            new_action_step2 = get_best_action(new_board_step2, num_cols, policyMapMQL, \"max\")\n",
    "            policyMapMQL[board_step2_key][action_step2] = ((1 - alpha) * policyMapMQL[board_step2_key][action_step2]\n",
    "                                                          + alpha * (reward + discount * policyMapMQL[new_board_step2_key][new_action_step2]))\n",
    "        # If my movement IS a wining movement, we will have only one step\n",
    "        else:\n",
    "            # Q-value of (winning_board, state) makes no sense because the game is over, so we simplify the expression\n",
    "            policyMapMQL[board_key][action] = ((1 - alpha) * policyMapMQL[board_key][action]) + alpha * reward\n",
    "            \n",
    "        total_reward += reward\n",
    "            \n",
    "        t += 1\n",
    "\n",
    "    rewards_list_minimax_ql.append(total_reward)\n",
    "    total_reward = 0\n",
    "        \n",
    "    # Decrease epsilon in every step so that exploration (random) is getting smaller against exploitation (greedy)\n",
    "    epsilon *= 0.99999\n",
    "    obs = trainer.reset()\n",
    "            \n",
    "    if (i % 10000) == 0:\n",
    "        print(i, datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        \n",
    "t = datetime.datetime.now()\n",
    "print(\"Fin\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# 50000\n",
    "# 23:20:41\n",
    "# 23:58:51\n",
    "# 50000\n",
    "# 23:10:25\n",
    "# 00:15:59\n",
    "# 50000\n",
    "# 00:23:06\n",
    "# 01:30:21\n",
    "# 50000\n",
    "# 20:10:54\n",
    "# 21:19:42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([175480,  24520], dtype=int64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.unique(rewards_list_minimax_ql, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax Q-Learning mean reward = -0.58\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html\n",
    "# ewm --> Weighted average of the values from x0 to xt. The weight of each element is decreasing from xt (weight=1) to xt-n (weight=(1-alpha)^n). alpha=2/(span+1)=2/101\n",
    "# This function gives us the trend of the rewards, instead of a sharp graph with a lot of -1's and 1's\n",
    "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "# Displaying the average of n elemtens instead of each element of the list \n",
    "n = 1000\n",
    "moving_average2 = [sum(rewards_list_minimax_ql[i:i+n])/n for i in range(0,len(rewards_list_minimax_ql)-n+1)]\n",
    "\n",
    "clear_output(True)\n",
    "print('Minimax Q-Learning mean reward =', np.mean(rewards_list_minimax_ql[-1000:]))\n",
    "plt.title(\"epsilon = %s\" % epsilon)\n",
    "#plt.plot(rewards_list_minimax_ql, label='ev_sarsa')\n",
    "plt.plot(moving_average(rewards_list_minimax_ql), label='ev_minimax_ql')\n",
    "#plt.plot(moving_average2, label='ev_sarsa2')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.show()\n",
    "\n",
    "# Minimax Q-Learning mean reward = -0.6 (eps=0.60046)\n",
    "# Minimax Q-Learning mean reward = -0.538 (eps=0.60046)\n",
    "# Minimax Q-Learning mean reward = -0.558 (eps=0.60046)\n",
    "# Minimax Q-Learning mean reward = -0.58 (eps=0.60046)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rewards_list_minimax_ql[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save policyMap to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyMapMQL_dict = dict(policyMapMQL)\n",
    "policyMapMQL_dict = {k: dict(v) for k, v in policyMapMQL_dict.items()}\n",
    "\n",
    "with open(r'policyMapMQL.dict','w+') as f:\n",
    "     f.write(str(policyMapMQL_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'rewards_list_minimax_ql.txt','w+') as f:\n",
    "    for element in rewards_list_minimax_ql:\n",
    "        f.write(str(element) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the agent as a function, so that we can use it with Kaggle environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "def agent_MQL(observation, configuration):\n",
    "    \n",
    "    ################################## START NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    # Given a board (as a list) and a column, returns the index of the first row available\n",
    "    def first_row_avail(board, col, num_rows, num_cols):\n",
    "        \"\"\"\n",
    "        Given a board (as a list) and a column, returns the index of the first row available\n",
    "        \"\"\"\n",
    "        index = -1\n",
    "        for i in range(num_rows): # from top to bottom\n",
    "            board_index = col + (i * num_cols)\n",
    "            if board[board_index] == 0:\n",
    "                index = board_index\n",
    "            else:\n",
    "                return index\n",
    "        return index\n",
    "            \n",
    "    ################################## END NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    print(configuration) # {rows: 5, columns: 5, inarow: 3}\n",
    "    # Number of rows on the board\n",
    "    num_rows = configuration.rows\n",
    "    # Number of columns on the board\n",
    "    num_cols = configuration.columns\n",
    "    # Number of checkers \"in a row\" needed to win\n",
    "    in_a_row = configuration.inarow\n",
    "    print(observation) # {board: [...], mark: 1}\n",
    "    # The current serialized board (rows x columns) - array [rows x columns] with top row first\n",
    "    board = observation.board\n",
    "    # Which player the agent is playing as (1 or 2).\n",
    "    mark = observation.mark\n",
    "    # If the board is empty, put the first checker in the middle column\n",
    "    if max(board)==0:\n",
    "        return round(num_cols/2)\n",
    "    \n",
    "    # Load the policiMap from file\n",
    "    dic = ''\n",
    "    with open(r'policyMapMQL.dict','r') as f:\n",
    "        for i in f.readlines():\n",
    "            dic=i #string\n",
    "    policyMapMQL_dict = eval(dic)\n",
    "    policyMapMQL = defaultdict(lambda: defaultdict(lambda: 0), policyMapMQL_dict)\n",
    "    for k, v in policyMapMQL.items():\n",
    "        policyMapMQL[k] = defaultdict(lambda: 0, v)\n",
    "    \n",
    "    # List of available columns\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # Convert the list board into a two-dimensional array\n",
    "    #board_array=np.array(board)\n",
    "    #board_array=board_array.reshape((num_rows,num_cols))\n",
    "    # Evaluate the target board with a checker in each available column, and get the best one\n",
    "    reward_list=[] # List with rewards of each movement\n",
    "    for target_col in available_cols:\n",
    "        target_board = board.copy()\n",
    "        target_index = first_row_avail(board, target_col, num_rows, num_cols)\n",
    "        #target_board[target_index] = mark\n",
    "        # Convert target_board into a tuple so that we can use it as a key for the policyMap\n",
    "        board_key = tuple(target_board)\n",
    "        reward = policyMapMQL[board_key][target_col]\n",
    "        reward_list.append(reward)\n",
    "        print(\"Column:\"+str(target_col)+\" Reward:\"+str(reward))\n",
    "    # Get the highest reward if the agent is player 1 and the lowest if is player 2. The policy has been learned with player 1\n",
    "    if mark == 1:\n",
    "        max_reward = max(reward_list)\n",
    "    else:\n",
    "        #max_reward = min(reward_list)\n",
    "        # Choose only columns with non-zero reward, because zero reward means unknown states, and the agent plays randomly in these cases\n",
    "        # If every non-zero value is close to 1 (means losing the game), it's better to take a chance with zero values\n",
    "        max_reward = min([value for value in reward_list if value!=0 and value<0.98], default=0)\n",
    "    # Return the column with the highest reward. If several actions have the same reward, select randomly one of them\n",
    "    max_rewards_index = [i for i, x in enumerate(reward_list) if x == max_reward]\n",
    "    # Check if max_rewards_index is empty (every value is zero or almost 1). In this case take a column randomly\n",
    "    if max_rewards_index == []:\n",
    "        max_rewards_index = available_cols\n",
    "    return available_cols[int(np.random.choice(max_rewards_index))]\n",
    "\n",
    "# Run an episode using the agent above vs the negamax agent.\n",
    "#env.run([agent_MQL, \"negamax\"])\n",
    "#env.run([\"negamax\", agent_MQL])\n",
    "#env.run([\"random\", agent_MQL])\n",
    "#env.render(mode=\"ipython\", width=500, height=450)\n",
    "#env.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fichas = [1, 2, 3, 5, 7]\n",
    "\n",
    "for i in num_fichas:\n",
    "    positives = 0\n",
    "    negatives = 0\n",
    "    zeros = 0\n",
    "    num_tableros = 0\n",
    "    for k, v in policyMapMQL.items():\n",
    "        if (sum([i != 0 for i in k])) == i:\n",
    "            num_tableros += 1\n",
    "            if (i <= 2):\n",
    "                print(\"Num. fichas:\", i, \" Tablero:\", k)\n",
    "            for v1 in v.values():\n",
    "                if (v1 > 0):\n",
    "                    positives += 1\n",
    "                else:\n",
    "                    if (v1 < 0):\n",
    "                        negatives += 1\n",
    "                    else:\n",
    "                        zeros += 1\n",
    "\n",
    "    print(\"Num. fichas:\", i, \" Num. tableros:\", num_tableros, \" Positives:\", positives, \", Negatives:\", negatives, \"Zeros:\", zeros)\n",
    "    \n",
    "# Con eps fijo (0,2)\n",
    "# Num. fichas: 1  Num. tableros: 5  Positives: 8 , Negatives: 0 Zeros: 17\n",
    "# Num. fichas: 2  Num. tableros: 8  Positives: 18 , Negatives: 22 Zeros: 0\n",
    "# Num. fichas: 3  Num. tableros: 37  Positives: 33 , Negatives: 34 Zeros: 114\n",
    "# Num. fichas: 5  Num. tableros: 258  Positives: 66 , Negatives: 289 Zeros: 597\n",
    "# Num. fichas: 7  Num. tableros: 1120  Positives: 343 , Negatives: 867 Zeros: 1990\n",
    "\n",
    "# Con eps variable (0,99999 --> 0,6)\n",
    "# Num. fichas: 1  Num. tableros: 5  Positives: 8 , Negatives: 0 Zeros: 17\n",
    "# Num. fichas: 2  Num. tableros: 8  Positives: 22 , Negatives: 18 Zeros: 0\n",
    "# Num. fichas: 3  Num. tableros: 37  Positives: 37 , Negatives: 30 Zeros: 110\n",
    "# Num. fichas: 5  Num. tableros: 258  Positives: 117 , Negatives: 278 Zeros: 592\n",
    "# Num. fichas: 7  Num. tableros: 1216  Positives: 502 , Negatives: 1186 Zeros: 2155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22207, 19342)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par=0\n",
    "impar=0\n",
    "for k in policyMapMQL.keys():\n",
    "    if (sum([i != 0 for i in k]) % 2) == 0:\n",
    "        par += 1\n",
    "    else:\n",
    "        impar += 1\n",
    "par, impar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'evaluate' returns a list of lists (one list per episode).  \n",
    "These lists per episode contains two elements (one per player), and these elements have these possible values:\n",
    "- 1: This player wins\n",
    "- -1: This player loses\n",
    "- 0: Draw (I guess there will be a value for draw)  \n",
    "Example:  \n",
    "[[1, -1], # First episode: Player 1 won  \n",
    " [1, -1], # Second episode: Player 1 won  \n",
    " [1, -1], # Third episode: Player 1 won  \n",
    " [1, -1], # ...  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1],  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "help(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [1, -1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [1, -1]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "#evaluate(\"connectx\", [agent_MC, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "evaluate(\"connectx\", [\"random\", agent_MQL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "#evaluate(\"connectx\", [agent, \"negamax\"], num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our agent against random and negamax agent.  \n",
    "- The closer the value is to 1 the better is player 1 agent (wins more times than loses)\n",
    "- The closer the value is to -1 the worse is player 1 agent (loses more times than wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Agent vs Random Agent: 0.2\n",
      "My Agent vs Negamax Agent: 1.0\n",
      "Random Agent vs My Agent: -0.8\n",
      "Negamax Agent vs My Agent: 0.4\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "\n",
    "# Run multiple episodes to estimate its performance.\n",
    "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [agent_MQL, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [agent_MQL, \"negamax\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Random Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", agent_MQL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Negamax Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", agent_MQL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing random vs negamax\n",
      "-0.4\n",
      "Playing random vs agent_MC\n",
      "-1.0\n",
      "Playing random vs agent_QL\n",
      "-0.2\n",
      "Playing random vs agent_SA\n",
      "0.0\n",
      "Playing random vs agent_ESA\n",
      "-0.2\n",
      "Playing random vs agent_MQL\n",
      "-1.0\n",
      "Playing negamax vs random\n",
      "1.0\n",
      "Playing negamax vs agent_MC\n",
      "1.0\n",
      "Playing negamax vs agent_QL\n",
      "1.0\n",
      "Playing negamax vs agent_SA\n",
      "0.8\n",
      "Playing negamax vs agent_ESA\n",
      "1.0\n",
      "Playing negamax vs agent_MQL\n",
      "0.6\n",
      "Playing agent_MC vs random\n",
      "0.6\n",
      "Playing agent_MC vs negamax\n",
      "1.0\n",
      "Playing agent_MC vs agent_QL\n",
      "1.0\n",
      "Playing agent_MC vs agent_SA\n",
      "0.4\n",
      "Playing agent_MC vs agent_ESA\n",
      "1.0\n",
      "Playing agent_MC vs agent_MQL\n",
      "1.0\n",
      "Playing agent_QL vs random\n",
      "0.4\n",
      "Playing agent_QL vs negamax\n",
      "1.0\n",
      "Playing agent_QL vs agent_MC\n",
      "1.0\n",
      "Playing agent_QL vs agent_SA\n",
      "1.0\n",
      "Playing agent_QL vs agent_ESA\n",
      "1.0\n",
      "Playing agent_QL vs agent_MQL\n",
      "1.0\n",
      "Playing agent_SA vs random\n",
      "0.8\n",
      "Playing agent_SA vs negamax\n",
      "0.4\n",
      "Playing agent_SA vs agent_MC\n",
      "0.6\n",
      "Playing agent_SA vs agent_QL\n",
      "0.8\n",
      "Playing agent_SA vs agent_ESA\n",
      "0.8\n",
      "Playing agent_SA vs agent_MQL\n",
      "0.2\n",
      "Playing agent_ESA vs random\n",
      "0.6\n",
      "Playing agent_ESA vs negamax\n",
      "0.6\n",
      "Playing agent_ESA vs agent_MC\n",
      "-0.4\n",
      "Playing agent_ESA vs agent_QL\n",
      "0.8\n",
      "Playing agent_ESA vs agent_SA\n",
      "0.2\n",
      "Playing agent_ESA vs agent_MQL\n",
      "0.6\n",
      "Playing agent_MQL vs random\n",
      "0.6\n",
      "Playing agent_MQL vs negamax\n",
      "1.0\n",
      "Playing agent_MQL vs agent_MC\n",
      "1.0\n",
      "Playing agent_MQL vs agent_QL\n",
      "1.0\n",
      "Playing agent_MQL vs agent_SA\n",
      "0.6\n",
      "Playing agent_MQL vs agent_ESA\n",
      "1.0\n",
      "{'random': -6.799999999999999, 'negamax': 1.7999999999999998, 'agent_MC': 2.8, 'agent_QL': 0.9999999999999998, 'agent_SA': 0.6000000000000002, 'agent_ESA': -2.1999999999999993, 'agent_MQL': 2.8000000000000003}\n",
      "Position   Agent      Points\n",
      "1          agent_MQL    2.80\n",
      "2          agent_MC     2.80\n",
      "3          negamax      1.80\n",
      "4          agent_QL     1.00\n",
      "5          agent_SA     0.60\n",
      "6          agent_ESA   -2.20\n",
      "7          random      -6.80\n"
     ]
    }
   ],
   "source": [
    "# https://docs.python.org/3/library/itertools.html\n",
    "from itertools import permutations\n",
    "from kaggle_environments import evaluate\n",
    "\n",
    "\n",
    "def name(obj):\n",
    "    \"\"\"\n",
    "    If obj is a function, gets the name instead of something like <function agent_MC at 0x0000023B01C3C950>\n",
    "    \"\"\"\n",
    "    if callable(obj):\n",
    "        return getattr(obj, \"__name__\")\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "    \n",
    "conf={\"rows\": 5, \"columns\": 5, \"inarow\": 3}\n",
    "\n",
    "list_agents = [\"random\", \"negamax\", agent_MC, agent_QL, agent_SA, agent_ESA, agent_MQL]\n",
    "list_matches = list(permutations(list_agents, 2))\n",
    "results = {}\n",
    "\n",
    "for players in list_matches:\n",
    "    print(\"Playing\", name(players[0]), \"vs\", name(players[1]))\n",
    "    list_players = list(players)\n",
    "    result = mean_reward(evaluate(\"connectx\", agents=list_players, configuration=conf, num_episodes=10))\n",
    "    print(result)\n",
    "    results[name(players[0])] = results.get(name(players[0]), 0) + result\n",
    "    results[name(players[1])] = results.get(name(players[1]), 0) - result\n",
    "\n",
    "print(results)\n",
    "\n",
    "# Display results ordered by the points obtained\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"{:<10} {:<10} {:<6}\".format('Position','Agent','Points')) # https://docs.python.org/3/library/string.html#formatspec\n",
    "for i, result in enumerate(sorted_results):\n",
    "    print(\"{:<10} {:<10} {:>6.2f}\".format(i+1, result[0], result[1]))\n",
    "\n",
    "# Con eps fijo (0,2)\n",
    "# Position   Team       Points\n",
    "# 1          agent_MC     3.60\n",
    "# 2          negamax      2.20\n",
    "# 3          agent_SA     1.20\n",
    "# 4          agent_QL     0.80\n",
    "# 5          agent_MQL   -0.40\n",
    "# 6          agent_ESA   -1.00\n",
    "# 7          random      -6.40\n",
    "\n",
    "# Con eps variable (0,99999 --> 0,60)\n",
    "# Position   Agent      Points\n",
    "# 1          negamax      2.20\n",
    "# 2          agent_MC     1.40\n",
    "# 3          agent_MQL    1.20\n",
    "# 4          agent_QL     0.80\n",
    "# 5          agent_ESA    0.60\n",
    "# 6          agent_SA     0.20\n",
    "# 7          random      -6.40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Temporal Differece Double Q-learning  \n",
    "Unlike MonteCarlo, in this algorithm we try to get Q-values (value for pair state, action) and we don't need the full episode to learn (bootstrapping).  \n",
    "As Q-learning, this is an off-policy method.  \n",
    "Off Policy. Learn policy $ \\pi $ (greedy) using policy $ \\mu $ (epsilon-greedy: greedy or random depending on a random value)  \n",
    "Unlike Q-learning, it uses 2 Q-value tables, one for selecting the next best action, and the other one for evaluating it to update q-values. With 2 Q-value tables we try to avoid overestimation and underestimation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Nested dictionary. We use defaultdict because it avoids issues with unexisting keys.\n",
    "policyMapDQL_1 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "policyMapDQL_2 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "rewards_list_double_ql = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.peterbe.com/plog/be-careful-with-using-dict-to-create-a-copy\n",
    "# https://stackoverflow.com/questions/22389989/python-copying-or-cloning-a-defaultdict-variable\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "policyMapDQL_AAA = deepcopy(policyMapDQL_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(policyMapDQL_1), id(policyMapDQL_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(policyMapDQL_1[(0,0)]), id(policyMapDQL_AA[(0,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load policyMap from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "dic = ''\n",
    "with open(r'policyMapDQL_1.dict','r') as f:\n",
    "         for i in f.readlines():\n",
    "            dic=i #string\n",
    "policyMapDQL_dict = eval(dic)\n",
    "policyMapDQL_1 = defaultdict(lambda: defaultdict(lambda: 0), policyMapDQL_dict)\n",
    "for k, v in policyMapDQL_1.items():\n",
    "    policyMapDQL_1[k] = defaultdict(lambda: 0, v)\n",
    "\n",
    "dic = ''\n",
    "with open(r'policyMapDQL_2.dict','r') as f:\n",
    "         for i in f.readlines():\n",
    "            dic=i #string\n",
    "policyMapDQL_dict = eval(dic)\n",
    "policyMapDQL_2 = defaultdict(lambda: defaultdict(lambda: 0), policyMapDQL_dict)\n",
    "for k, v in policyMapDQL_2.items():\n",
    "    policyMapDQL_2[k] = defaultdict(lambda: 0, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8939, 8956)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(policyMapDQL_1.keys()), len(policyMapDQL_2.keys())\n",
    "# (5306, 5264)\n",
    "# (7035, 7008)\n",
    "# (7958, 7949)\n",
    "# (8939, 8956)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0, -1.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([min(i.values()) for i in policyMapDQL_1.values()]), max([max(i.values()) for i in policyMapDQL_1.values()]), min([min(i.values()) for i in policyMapDQL_2.values()]), max([max(i.values()) for i in policyMapDQL_2.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MIRAR SI SE PUEDE HACER SOLO EN 1!!!\n",
    "\n",
    "values_1=[]\n",
    "for i in policyMapDQL_1.values():\n",
    "    values_1.extend(list(i.values()))\n",
    "\n",
    "values_2=[]\n",
    "for i in policyMapDQL_2.values():\n",
    "    values_2.extend(list(i.values()))\n",
    "\n",
    "plt.hist([values_1,values_2], label=['policy1','policy2'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/reading-and-writing-lists-to-a-file-in-python/\n",
    "rewards_list_double_ql = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('rewards_list_double_ql.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        rewards_list_double_ql.append(int(currentPlace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rewards_list_double_ql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/double-q-learning-the-easy-way-a924c4085ec3\n",
    "# https://rubikscode.net/2021/07/20/introduction-to-double-q-learning/\n",
    "# Based in the Hado van Hasselt's paper: https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "num_cols = env.configuration.columns\n",
    "\n",
    "# Training agent in first position (player 1) against the negamax agent.\n",
    "trainer = env.train([None, \"negamax\"])\n",
    "\n",
    "discount = 0.9\n",
    "epsilon = 0.20 # parameter for epsilon-greedy policy\n",
    "alpha = 0.5 # learning rate\n",
    "t_max = 1000 # used for environments without end\n",
    "total_reward = 0 # used for displaying the evolution of the rewards as the model learns\n",
    "\n",
    "def get_best_action(board, num_cols, policy):\n",
    "    # greedy policy\n",
    "    q_value_max = float(\"-inf\")\n",
    "    best_action = None\n",
    "    # Convert board in a tuple to use it as the key in a dict structure\n",
    "    board_key = tuple(board)\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    for col in available_cols:\n",
    "        q_value = policy.get(board_key,dict()).get(col,0)\n",
    "        if q_value > q_value_max:\n",
    "            best_action = col\n",
    "            q_value_max = q_value\n",
    "    return best_action\n",
    "\n",
    "def get_best_action_combined(board, num_cols, policyA, policyB):\n",
    "    \"\"\"\n",
    "    Returns the best action using the addition of policyA and policyB.\n",
    "    \"\"\"\n",
    "    # greedy policy\n",
    "    q_value_max = float(\"-inf\")\n",
    "    best_action = None\n",
    "    # Convert board in a tuple to use it as the key in a dict structure\n",
    "    board_key = tuple(board)\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    for col in available_cols:\n",
    "        q_value = policyA.get(board_key,dict()).get(col,0) + policyB.get(board_key,dict()).get(col,0)\n",
    "        if q_value > q_value_max:\n",
    "            best_action = col\n",
    "            q_value_max = q_value\n",
    "    return best_action\n",
    "\n",
    "def get_action_combined(board, num_cols, epsilon, policyA, policyB):\n",
    "    \"\"\"\n",
    "    Returns the action using an epsilon-greedy policy. When returning the best action, it combines policyA and policyB\n",
    "    \"\"\"\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # epison-greedy policy\n",
    "    if (np.random.random() > epsilon):\n",
    "        # greedy\n",
    "        return get_best_action_combined(board, num_cols, policyA, policyB)\n",
    "    else:\n",
    "        # random\n",
    "        return int(np.random.choice(available_cols))\n",
    "\n",
    "\n",
    "t = datetime.datetime.now()\n",
    "print(\"Inicio\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "obs = trainer.reset()\n",
    "for i in range(50000):\n",
    "    done = False\n",
    "    t = 0\n",
    "    #env.render()\n",
    "    while not done and t < t_max:\n",
    "        # Convert board in a tuple to use it as the key in a dict structure\n",
    "        board_key = tuple(obs.board)\n",
    "        # Select the action with epsilon-greedy using policyA + policyB\n",
    "        action = get_action_combined(obs.board, num_cols, epsilon, policyMapDQL_1, policyMapDQL_2)\n",
    "        obs, reward, done, info = trainer.step(action)\n",
    "        # Select randomly policy to get next action and policy to update\n",
    "        # With policy_X = policy_Y we are just copying the reference. Both dicts will point to the same place in memory.\n",
    "        # So every change in policy_X will be also in policy_Y and vice versa. If we want to clone the nested dict, we need deepcopy\n",
    "        if (np.random.random() > 0.5):\n",
    "            policyA = policyMapDQL_1\n",
    "            policyB = policyMapDQL_2\n",
    "        else:\n",
    "            policyA = policyMapDQL_2\n",
    "            policyB = policyMapDQL_1\n",
    "        \n",
    "        # Each step usually includes two steps (my movement and opponent's movement), so I need to update the policy for both of them (I can play as player 1 or 2)\n",
    "        # If my movement is a winning one, there is only one step\n",
    "        # Update policy. Q(s,a) = (1 - alpha) * Q(s,a) + alpha * (r + gamma * max(Q(s',a')))\n",
    "        \n",
    "        # If my movement IS NOT a wining movement, we will have two steps\n",
    "        if not(done and reward==1):\n",
    "            # Update policy with my movement: My board - board_key, My action - action, My board after action - env.steps[-2][0][\"observation\"][\"board\"]\n",
    "            board_step1_key = board_key\n",
    "            action_step1 = action\n",
    "            reward_step1 = env.steps[-2][0][\"reward\"]\n",
    "            new_board_step1 = env.steps[-2][0][\"observation\"][\"board\"]\n",
    "            new_board_step1_key = tuple(new_board_step1)\n",
    "            new_action_step1 = get_best_action(new_board_step1, num_cols, policyA)\n",
    "            # Select action using policyA, and update using q-values of policyB\n",
    "            policyA[board_step1_key][action_step1] = ((1 - alpha) * policyA[board_step1_key][action_step1]\n",
    "                                                      + alpha * (reward_step1 + discount * policyB[new_board_step1_key][new_action_step1]))\n",
    "            \n",
    "            # Update policy with opponent's movement: board - env.steps[-2][0][\"observation\"][\"board\"], action - env.steps[-1][1][\"action\"], board after action - obs.board\n",
    "            board_step2_key = new_board_step1_key\n",
    "            action_step2 = env.steps[-1][1][\"action\"]\n",
    "            reward_step2 = reward\n",
    "            new_board_step2 = obs.board\n",
    "            new_board_step2_key = tuple(new_board_step2)\n",
    "            new_action_step2 = get_best_action(new_board_step2, num_cols, policyA)\n",
    "            # Select action using policyA, and update using q-values of policyB\n",
    "            policyA[board_step2_key][action_step2] = ((1 - alpha) * policyA[board_step2_key][action_step2]\n",
    "                                                      + alpha * (reward + discount * policyB[new_board_step2_key][new_action_step2]))\n",
    "        # If my movement IS a wining movement, we will have only one step\n",
    "        else:\n",
    "            # Q-value of (winning_board, state) makes no sense because the game is over, so we simplify the expression\n",
    "            policyA[board_key][action] = ((1 - alpha) * policyA[board_key][action]) + alpha * reward\n",
    "            \n",
    "        total_reward += reward\n",
    "            \n",
    "        t += 1\n",
    "        \n",
    "    rewards_list_double_ql.append(total_reward)\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Decrease epsilon in every step so that exploration (random) is getting smaller against exploitation (greedy)\n",
    "    #epsilon *= 0.99999\n",
    "    obs = trainer.reset()\n",
    "            \n",
    "    if (i % 10000) == 0:\n",
    "        print(i, datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        \n",
    "t = datetime.datetime.now()\n",
    "print(\"Fin\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# 50000\n",
    "# 15:19:37\n",
    "# 16:01:19\n",
    "# 50000\n",
    "# 16:55:27\n",
    "# 18:37:47\n",
    "# 50000\n",
    "# 22:54:12\n",
    "# 00:51:53\n",
    "# 50000\n",
    "# 15:39:45\n",
    "# 16:26:16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.unique(rewards_list_double_ql, return_counts=True)\n",
    "# (array([-1,  1]), array([37645, 12355], dtype=int64))\n",
    "# (array([-1,  1]), array([60768, 39232], dtype=int64))\n",
    "# (array([-1,  1]), array([83792, 66208], dtype=int64))\n",
    "# (array([-1,  1]), array([106606,  93394], dtype=int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Q-Learning mean reward = 0.03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "epsilon = 0.20\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html\n",
    "# ewm --> Weighted average of the values from x0 to xt. The weight of each element is decreasing from xt (weight=1) to xt-n (weight=(1-alpha)^n). alpha=2/(span+1)=2/101\n",
    "# This function gives us the trend of the rewards, instead of a sharp graph with a lot of -1's and 1's\n",
    "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "# Displaying the average of n elemtens instead of each element of the list \n",
    "n = 1000\n",
    "moving_average2 = [sum(rewards_list_double_ql[i:i+n])/n for i in range(0,len(rewards_list_double_ql)-n+1)]\n",
    "\n",
    "clear_output(True)\n",
    "print('Double Q-Learning mean reward =', np.mean(rewards_list_double_ql[-1000:]))\n",
    "plt.title(\"epsilon = %s\" % epsilon)\n",
    "#plt.plot(rewards_list_double_ql, label='ev_sarsa')\n",
    "plt.plot(moving_average(rewards_list_double_ql), label='ev_dql')\n",
    "#plt.plot(moving_average2, label='ev_sarsa2')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(-2, 2)\n",
    "plt.show()\n",
    "\n",
    "# Double Q-Learning mean reward = 0.012\n",
    "# Double Q-Learning mean reward = 0.09\n",
    "# Double Q-Learning mean reward = 0.094\n",
    "# Double Q-Learning mean reward = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rewards_list_double_ql[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save policyMap to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyMapDQL_dict = dict(policyMapDQL_1)\n",
    "policyMapDQL_dict = {k: dict(v) for k, v in policyMapDQL_dict.items()}\n",
    "\n",
    "with open(r'policyMapDQL_1.dict','w+') as f:\n",
    "     f.write(str(policyMapDQL_dict))\n",
    "        \n",
    "policyMapDQL_dict = dict(policyMapDQL_2)\n",
    "policyMapDQL_dict = {k: dict(v) for k, v in policyMapDQL_dict.items()}\n",
    "\n",
    "with open(r'policyMapDQL_2.dict','w+') as f:\n",
    "     f.write(str(policyMapDQL_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'rewards_list_double_ql.txt','w+') as f:\n",
    "    for element in rewards_list_double_ql:\n",
    "        f.write(str(element) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the agent as a function, so that we can use it with Kaggle environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "def agent_DQL(observation, configuration):\n",
    "    \n",
    "    ################################## START NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    # Given a board (as a list) and a column, returns the index of the first row available\n",
    "    def first_row_avail(board, col, num_rows, num_cols):\n",
    "        \"\"\"\n",
    "        Given a board (as a list) and a column, returns the index of the first row available\n",
    "        \"\"\"\n",
    "        index = -1\n",
    "        for i in range(num_rows): # from top to bottom\n",
    "            board_index = col + (i * num_cols)\n",
    "            if board[board_index] == 0:\n",
    "                index = board_index\n",
    "            else:\n",
    "                return index\n",
    "        return index\n",
    "            \n",
    "    ################################## END NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    print(configuration) # {rows: 5, columns: 5, inarow: 3}\n",
    "    # Number of rows on the board\n",
    "    num_rows = configuration.rows\n",
    "    # Number of columns on the board\n",
    "    num_cols = configuration.columns\n",
    "    # Number of checkers \"in a row\" needed to win\n",
    "    in_a_row = configuration.inarow\n",
    "    print(observation) # {board: [...], mark: 1}\n",
    "    # The current serialized board (rows x columns) - array [rows x columns] with top row first\n",
    "    board = observation.board\n",
    "    # Which player the agent is playing as (1 or 2).\n",
    "    mark = observation.mark\n",
    "    # If the board is empty, put the first checker in the middle column\n",
    "    if max(board)==0:\n",
    "        return round(num_cols/2)\n",
    "    \n",
    "    # Load the policiMaps from files\n",
    "    dic = ''\n",
    "    with open(r'policyMapDQL_1.dict','r') as f:\n",
    "        for i in f.readlines():\n",
    "            dic=i #string\n",
    "    policyMapDQL_dict = eval(dic)\n",
    "    policyMapDQL_1 = defaultdict(lambda: defaultdict(lambda: 0), policyMapDQL_dict)\n",
    "    for k, v in policyMapDQL_1.items():\n",
    "        policyMapDQL_1[k] = defaultdict(lambda: 0, v)\n",
    "\n",
    "    dic = ''\n",
    "    with open(r'policyMapDQL_2.dict','r') as f:\n",
    "        for i in f.readlines():\n",
    "            dic=i #string\n",
    "    policyMapDQL_dict = eval(dic)\n",
    "    policyMapDQL_2 = defaultdict(lambda: defaultdict(lambda: 0), policyMapDQL_dict)\n",
    "    for k, v in policyMapDQL_2.items():\n",
    "        policyMapDQL_2[k] = defaultdict(lambda: 0, v)\n",
    "\n",
    "    # List of available columns\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # Convert the list board into a two-dimensional array\n",
    "    #board_array=np.array(board)\n",
    "    #board_array=board_array.reshape((num_rows,num_cols))\n",
    "    # Evaluate the target board with a checker in each available column, and get the best one\n",
    "    reward_list=[] # List with rewards of each movement\n",
    "    for target_col in available_cols:\n",
    "        target_board = board.copy()\n",
    "        target_index = first_row_avail(board, target_col, num_rows, num_cols)\n",
    "        # Convert target_board into a tuple so that we can use it as a key for the policyMap\n",
    "        board_key = tuple(target_board)\n",
    "        reward = policyMapDQL_1[board_key][target_col] + policyMapDQL_2[board_key][target_col]\n",
    "        reward_list.append(reward)\n",
    "        print(\"Column:\"+str(target_col)+\" Reward:\"+str(reward)+\" Reward_1:\"+str(policyMapDQL_1[board_key][target_col])+\" Reward_2:\"+str(policyMapDQL_2[board_key][target_col]))\n",
    "    # Get the highest reward if the agent is player 1 and the lowest if is player 2. The policy has been learned with player 1\n",
    "    if mark == 1:\n",
    "        max_reward = max(reward_list)\n",
    "    else:\n",
    "        # Choose only columns with non-zero reward, because zero reward means unknown states, and the agent plays randomly in these cases\n",
    "        # If every non-zero value is close to 1 (means losing the game), it's better to take a chance with zero values\n",
    "        max_reward = min([value for value in reward_list if value!=0 and value<1.98], default=0)\n",
    "    # Return the column with the highest reward. If several actions have the same reward, select randomly one of them\n",
    "    max_rewards_index = [i for i, x in enumerate(reward_list) if x == max_reward]\n",
    "    # Check if max_rewards_index is empty (every value is zero or almost 1). In this case take a column randomly\n",
    "    if max_rewards_index == []:\n",
    "        max_rewards_index = available_cols\n",
    "    return available_cols[int(np.random.choice(max_rewards_index))]\n",
    "        \n",
    "# Run an episode using the agent above vs the negamax agent.\n",
    "#env.run([agent_DQL, \"negamax\"])\n",
    "#env.run([\"negamax\", agent_DQL])\n",
    "#env.run([\"random\", agent_DQL])\n",
    "#env.render(mode=\"ipython\", width=500, height=450)\n",
    "#env.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fichas = [1, 2, 3, 5, 7]\n",
    "\n",
    "for i in num_fichas:\n",
    "    positives = 0\n",
    "    negatives = 0\n",
    "    zeros = 0\n",
    "    num_tableros = 0\n",
    "    for k, v in policyMapDQL_1.items():\n",
    "        if (sum([i != 0 for i in k])) == i:\n",
    "            num_tableros += 1\n",
    "            if (i <= 2):\n",
    "                print(\"Num. fichas:\", i, \" Tablero:\", k)\n",
    "            for v1 in v.values():\n",
    "                if (v1 > 0):\n",
    "                    positives += 1\n",
    "                else:\n",
    "                    if (v1 < 0):\n",
    "                        negatives += 1\n",
    "                    else:\n",
    "                        zeros += 1\n",
    "\n",
    "    print(\"Num. fichas:\", i, \" Num. tableros:\", num_tableros, \" Positives:\", positives, \", Negatives:\", negatives, \"Zeros:\", zeros)\n",
    "    \n",
    "# Con eps fijo (0,2)\n",
    "# Num. fichas: 1  Num. tableros: 5  Positives: 7 , Negatives: 0 Zeros: 5\n",
    "# Num. fichas: 2  Num. tableros: 8  Positives: 15 , Negatives: 0 Zeros: 25\n",
    "# Num. fichas: 3  Num. tableros: 37  Positives: 16 , Negatives: 0 Zeros: 76\n",
    "# Num. fichas: 5  Num. tableros: 252  Positives: 17 , Negatives: 82 Zeros: 464\n",
    "# Num. fichas: 7  Num. tableros: 828  Positives: 112 , Negatives: 380 Zeros: 1020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Both policies seems to have the same values. Let's check it\n",
    "i = 0\n",
    "for k1, v1 in policyMapDQL_1.items():\n",
    "    for k2, v2 in v1.items():\n",
    "        if abs(policyMapDQL_2[k1][k2] - v2) > 0.1:\n",
    "            print(\"Key1:\"+str(k1)+\" Key2:\"+str(k2)+\" Value_pol1:\"+str(v2)+\" Value_pol2:\"+str(policyMapDQL_2[k1][k2]))\n",
    "            i += 1\n",
    "\n",
    "print(\"Total: \"+str(i))\n",
    "\n",
    "# 2239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Both policies seems to have the same values. Let's check it\n",
    "i = 0\n",
    "for k1, v1 in policyMapDQL_2.items():\n",
    "    for k2, v2 in v1.items():\n",
    "        if abs(policyMapDQL_1[k1][k2] - v2) > 0.1:\n",
    "            print(\"Key1:\"+str(k1)+\" Key2:\"+str(k2)+\" Value_pol1:\"+str(v2)+\" Value_pol2:\"+str(policyMapDQL_1[k1][k2]))\n",
    "            i += 1\n",
    "\n",
    "print(\"Total: \"+str(i))\n",
    "\n",
    "# 2850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par=0\n",
    "impar=0\n",
    "for k in policyMapDQL.keys():\n",
    "    if (sum([i != 0 for i in k]) % 2) == 0:\n",
    "        par += 1\n",
    "    else:\n",
    "        impar += 1\n",
    "par, impar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'evaluate' returns a list of lists (one list per episode).  \n",
    "These lists per episode contains two elements (one per player), and these elements have these possible values:\n",
    "- 1: This player wins\n",
    "- -1: This player loses\n",
    "- 0: Draw (I guess there will be a value for draw)  \n",
    "Example:  \n",
    "[[1, -1], # First episode: Player 1 won  \n",
    " [1, -1], # Second episode: Player 1 won  \n",
    " [1, -1], # Third episode: Player 1 won  \n",
    " [1, -1], # ...  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1],  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "help(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "#evaluate(\"connectx\", [agent_MC, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "evaluate(\"connectx\", [\"random\", agent_DQL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10, debug=True)\n",
    "#evaluate(\"connectx\", [agent, \"negamax\"], num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our agent against random and negamax agent.  \n",
    "- The closer the value is to 1 the better is player 1 agent (wins more times than loses)\n",
    "- The closer the value is to -1 the worse is player 1 agent (loses more times than wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "\n",
    "# Run multiple episodes to estimate its performance.\n",
    "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [agent_DQL, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [agent_DQL, \"negamax\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Random Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", agent_DQL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Negamax Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", agent_DQL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing random vs negamax\n",
      "-0.8\n",
      "Playing random vs agent_MC\n",
      "-1.0\n",
      "Playing random vs agent_QL\n",
      "-0.4\n",
      "Playing random vs agent_SA\n",
      "-0.6\n",
      "Playing random vs agent_ESA\n",
      "-0.6\n",
      "Playing random vs agent_MQL\n",
      "-0.8\n",
      "Playing random vs agent_DQL\n",
      "0.4\n",
      "Playing negamax vs random\n",
      "1.0\n",
      "Playing negamax vs agent_MC\n",
      "1.0\n",
      "Playing negamax vs agent_QL\n",
      "1.0\n",
      "Playing negamax vs agent_SA\n",
      "1.0\n",
      "Playing negamax vs agent_ESA\n",
      "1.0\n",
      "Playing negamax vs agent_MQL\n",
      "0.4\n",
      "Playing negamax vs agent_DQL\n",
      "1.0\n",
      "Playing agent_MC vs random\n",
      "1.0\n",
      "Playing agent_MC vs negamax\n",
      "1.0\n",
      "Playing agent_MC vs agent_QL\n",
      "1.0\n",
      "Playing agent_MC vs agent_SA\n",
      "1.0\n",
      "Playing agent_MC vs agent_ESA\n",
      "1.0\n",
      "Playing agent_MC vs agent_MQL\n",
      "1.0\n",
      "Playing agent_MC vs agent_DQL\n",
      "0.6\n",
      "Playing agent_QL vs random\n",
      "0.2\n",
      "Playing agent_QL vs negamax\n",
      "1.0\n",
      "Playing agent_QL vs agent_MC\n",
      "1.0\n",
      "Playing agent_QL vs agent_SA\n",
      "0.8\n",
      "Playing agent_QL vs agent_ESA\n",
      "1.0\n",
      "Playing agent_QL vs agent_MQL\n",
      "1.0\n",
      "Playing agent_QL vs agent_DQL\n",
      "0.4\n",
      "Playing agent_SA vs random\n",
      "0.8\n",
      "Playing agent_SA vs negamax\n",
      "0.2\n",
      "Playing agent_SA vs agent_MC\n",
      "0.8\n",
      "Playing agent_SA vs agent_QL\n",
      "0.8\n",
      "Playing agent_SA vs agent_ESA\n",
      "0.4\n",
      "Playing agent_SA vs agent_MQL\n",
      "0.2\n",
      "Playing agent_SA vs agent_DQL\n",
      "0.2\n",
      "Playing agent_ESA vs random\n",
      "0.8\n",
      "Playing agent_ESA vs negamax\n",
      "0.4\n",
      "Playing agent_ESA vs agent_MC\n",
      "0.6\n",
      "Playing agent_ESA vs agent_QL\n",
      "0.6\n",
      "Playing agent_ESA vs agent_SA\n",
      "0.2\n",
      "Playing agent_ESA vs agent_MQL\n",
      "0.4\n",
      "Playing agent_ESA vs agent_DQL\n",
      "0.4\n",
      "Playing agent_MQL vs random\n",
      "0.6\n",
      "Playing agent_MQL vs negamax\n",
      "1.0\n",
      "Playing agent_MQL vs agent_MC\n",
      "1.0\n",
      "Playing agent_MQL vs agent_QL\n",
      "1.0\n",
      "Playing agent_MQL vs agent_SA\n",
      "0.8\n",
      "Playing agent_MQL vs agent_ESA\n",
      "0.8\n",
      "Playing agent_MQL vs agent_DQL\n",
      "-0.2\n",
      "Playing agent_DQL vs random\n",
      "0.8\n",
      "Playing agent_DQL vs negamax\n",
      "1.0\n",
      "Playing agent_DQL vs agent_MC\n",
      "-0.6\n",
      "Playing agent_DQL vs agent_QL\n",
      "0.2\n",
      "Playing agent_DQL vs agent_SA\n",
      "0.6\n",
      "Playing agent_DQL vs agent_ESA\n",
      "0.8\n",
      "Playing agent_DQL vs agent_MQL\n",
      "1.0\n",
      "Position   Team       Points\n",
      "1          agent_MC     3.80\n",
      "2          negamax      2.60\n",
      "3          agent_MQL    1.80\n",
      "4          agent_QL     1.20\n",
      "5          agent_DQL    1.00\n",
      "6          agent_SA    -0.40\n",
      "7          agent_ESA   -1.00\n",
      "8          random      -9.00\n"
     ]
    }
   ],
   "source": [
    "# https://docs.python.org/3/library/itertools.html\n",
    "from itertools import permutations\n",
    "from kaggle_environments import evaluate\n",
    "\n",
    "\n",
    "def name(obj):\n",
    "    \"\"\"\n",
    "    If obj is a function, gets the name instead of something like <function agent_MC at 0x0000023B01C3C950>\n",
    "    \"\"\"\n",
    "    if callable(obj):\n",
    "        return getattr(obj, \"__name__\")\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "    \n",
    "conf={\"rows\": 5, \"columns\": 5, \"inarow\": 3}\n",
    "\n",
    "list_agents = [\"random\", \"negamax\", agent_MC, agent_QL, agent_SA, agent_ESA, agent_MQL, agent_DQL]\n",
    "list_matches = list(permutations(list_agents, 2))\n",
    "results = {}\n",
    "\n",
    "for players in list_matches:\n",
    "    print(\"Playing\", name(players[0]), \"vs\", name(players[1]))\n",
    "    list_players = list(players)\n",
    "    result = mean_reward(evaluate(\"connectx\", agents=list_players, configuration=conf, num_episodes=10))\n",
    "    print(result)\n",
    "    results[name(players[0])] = results.get(name(players[0]), 0) + result\n",
    "    results[name(players[1])] = results.get(name(players[1]), 0) - result\n",
    "\n",
    "#print(results)\n",
    "\n",
    "# Display results ordered by the points obtained\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"{:<10} {:<10} {:<6}\".format('Position','Team','Points')) # https://docs.python.org/3/library/string.html#formatspec\n",
    "for i, result in enumerate(sorted_results):\n",
    "    print(\"{:<10} {:<10} {:>6.2f}\".format(i+1, result[0], result[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Temporal Differece Deep Q-learning  \n",
    "Unlike regular Q-learning, in this algorithm we use a neural network to estimate the Q-values of a state.  \n",
    "Off Policy. Learn policy $ \\pi $ (greedy) using policy $ \\mu $ (epsilon-greedy: greedy or random depending on a random value)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 657\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "rewards_list_deep_ql = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RED v2\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=3, activation='relu', input_shape=(num_rows, num_cols, 1))) # padding=valid # Input: (5, 5, 1) Output (3, 3, 8)\n",
    "model.add(Flatten()) # Input: (3, 3, 8) Output: (72)\n",
    "model.add(Dense(8, activation='relu')) # Input: (72) Output: (8)\n",
    "model.add(Dense(action_size, activation=\"linear\")) # Input: (8) Output: (5)\n",
    "model.compile(Adam(lr=0.01), loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"387pt\" viewBox=\"0.00 0.00 308.00 387.00\" width=\"308pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 383)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-383 304,-383 304,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1900476581816 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1900476581816</title>\n",
       "<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 300,-378.5 300,-332.5 0,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-351.8\">conv2d_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"161,-332.5 161,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"161,-355.5 217,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"217,-332.5 217,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258.5\" y=\"-363.3\">[(?, 5, 5, 1)]</text>\n",
       "<polyline fill=\"none\" points=\"217,-355.5 300,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258.5\" y=\"-340.3\">[(?, 5, 5, 1)]</text>\n",
       "</g>\n",
       "<!-- 1900476678272 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1900476678272</title>\n",
       "<polygon fill=\"none\" points=\"28.5,-249.5 28.5,-295.5 271.5,-295.5 271.5,-249.5 28.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-268.8\">conv2d: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"141.5,-249.5 141.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"141.5,-272.5 197.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"197.5,-249.5 197.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"234.5\" y=\"-280.3\">(?, 5, 5, 1)</text>\n",
       "<polyline fill=\"none\" points=\"197.5,-272.5 271.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"234.5\" y=\"-257.3\">(?, 3, 3, 8)</text>\n",
       "</g>\n",
       "<!-- 1900476581816&#45;&gt;1900476678272 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1900476581816-&gt;1900476678272</title>\n",
       "<path d=\"M150,-332.366C150,-324.152 150,-314.658 150,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"153.5,-305.607 150,-295.607 146.5,-305.607 153.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1900285913408 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1900285913408</title>\n",
       "<polygon fill=\"none\" points=\"37.5,-166.5 37.5,-212.5 262.5,-212.5 262.5,-166.5 37.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-185.8\">flatten: Flatten</text>\n",
       "<polyline fill=\"none\" points=\"132.5,-166.5 132.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"132.5,-189.5 188.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"188.5,-166.5 188.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-197.3\">(?, 3, 3, 8)</text>\n",
       "<polyline fill=\"none\" points=\"188.5,-189.5 262.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-174.3\">(?, 72)</text>\n",
       "</g>\n",
       "<!-- 1900476678272&#45;&gt;1900285913408 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1900476678272-&gt;1900285913408</title>\n",
       "<path d=\"M150,-249.366C150,-241.152 150,-231.658 150,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"153.5,-222.607 150,-212.607 146.5,-222.607 153.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1900171479920 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1900171479920</title>\n",
       "<polygon fill=\"none\" points=\"50.5,-83.5 50.5,-129.5 249.5,-129.5 249.5,-83.5 50.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.5\" y=\"-102.8\">dense: Dense</text>\n",
       "<polyline fill=\"none\" points=\"140.5,-83.5 140.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"140.5,-106.5 196.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"196.5,-83.5 196.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223\" y=\"-114.3\">(?, 72)</text>\n",
       "<polyline fill=\"none\" points=\"196.5,-106.5 249.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223\" y=\"-91.3\">(?, 8)</text>\n",
       "</g>\n",
       "<!-- 1900285913408&#45;&gt;1900171479920 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1900285913408-&gt;1900171479920</title>\n",
       "<path d=\"M150,-166.366C150,-158.152 150,-148.658 150,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"153.5,-139.607 150,-129.607 146.5,-139.607 153.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1900171553368 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1900171553368</title>\n",
       "<polygon fill=\"none\" points=\"47,-0.5 47,-46.5 253,-46.5 253,-0.5 47,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"99\" y=\"-19.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"151,-0.5 151,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"151,-23.5 207,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"207,-0.5 207,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"230\" y=\"-31.3\">(?, 8)</text>\n",
       "<polyline fill=\"none\" points=\"207,-23.5 253,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"230\" y=\"-8.3\">(?, 5)</text>\n",
       "</g>\n",
       "<!-- 1900171479920&#45;&gt;1900171553368 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1900171479920-&gt;1900171553368</title>\n",
       "<path d=\"M150,-83.3664C150,-75.1516 150,-65.6579 150,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"153.5,-56.6068 150,-46.6068 146.5,-56.6069 153.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import SVG, display, clear_output\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "display(SVG(model_to_dot(model, show_shapes=True, dpi=72).create(prog='dot', format='svg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 8)           80        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 584       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 45        \n",
      "=================================================================\n",
      "Total params: 709\n",
      "Trainable params: 709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model_DQN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/reading-and-writing-lists-to-a-file-in-python/\n",
    "rewards_list_deep_ql = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('rewards_list_deep_ql.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        rewards_list_deep_ql.append(int(currentPlace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewards_list_deep_ql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load list of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/reading-and-writing-lists-to-a-file-in-python/\n",
    "accum_train_loss = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('loss_list_deep_ql.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        #accum_train_loss.append(int(currentPlace))\n",
    "        accum_train_loss.append(float(currentPlace))\n",
    "        \n",
    "accum_train_loss_2 = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('loss_2_list_deep_ql.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        accum_train_loss_2.append(float(currentPlace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99872, 6391808)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accum_train_loss), len(accum_train_loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "modelpath=\"model_DQN_callback.h5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(modelpath, monitor='loss', verbose=0,\n",
    "                             save_best_only=True,\n",
    "                             mode='min') # graba slo los que mejoran en la funcin de prdida\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio 17:09:03\n",
      "episode: 0 17:09:03 avg last 10 rewards 0.8 avg last 10 loss 0.02304981091292575 avg last 10 loss 2 0.0\n",
      "episode: 500 17:10:54 avg last 10 rewards 0.8 avg last 10 loss 0.013515603911946528 avg last 10 loss 2 0.04437710046768188\n",
      "episode: 1000 17:12:59 avg last 10 rewards 1.0 avg last 10 loss 0.01513131356332451 avg last 10 loss 2 0.05016701221466065\n",
      "episode: 1500 17:16:22 avg last 10 rewards 1.0 avg last 10 loss 0.015579856107069644 avg last 10 loss 2 0.165813085436821\n",
      "episode: 2000 17:19:42 avg last 10 rewards 1.0 avg last 10 loss 0.0179302885197103 avg last 10 loss 2 0.2783802837133408\n",
      "episode: 2500 17:23:01 avg last 10 rewards 1.0 avg last 10 loss 0.013258622295688837 avg last 10 loss 2 0.03907520771026611\n",
      "episode: 3000 17:26:22 avg last 10 rewards 0.6 avg last 10 loss 0.013301501641399227 avg last 10 loss 2 0.07772772312164307\n",
      "episode: 3500 17:29:01 avg last 10 rewards 0.8 avg last 10 loss 0.01666029290063307 avg last 10 loss 2 0.14108283817768097\n",
      "episode: 4000 17:31:02 avg last 10 rewards 0.4 avg last 10 loss 0.017745541539625265 avg last 10 loss 2 0.07939902544021607\n",
      "episode: 4500 17:34:30 avg last 10 rewards 1.0 avg last 10 loss 0.016729050828143955 avg last 10 loss 2 0.15696207880973817\n",
      "episode: 5000 17:37:52 avg last 10 rewards 0.6 avg last 10 loss 0.01690396215126384 avg last 10 loss 2 0.08397414684295654\n",
      "episode: 5500 17:41:16 avg last 10 rewards 0.6 avg last 10 loss 0.023623067495645956 avg last 10 loss 2 0.09890897274017334\n",
      "episode: 6000 17:44:36 avg last 10 rewards 1.0 avg last 10 loss 0.019406242872355504 avg last 10 loss 2 0.10666677355766296\n",
      "episode: 6500 17:47:56 avg last 10 rewards 0.8 avg last 10 loss 0.023242829242371955 avg last 10 loss 2 0.2139203131198883\n",
      "episode: 7000 17:51:15 avg last 10 rewards 0.8 avg last 10 loss 0.01986443368077744 avg last 10 loss 2 0.03870022892951965\n",
      "episode: 7500 17:54:26 avg last 10 rewards 0.6 avg last 10 loss 0.01695756010012701 avg last 10 loss 2 0.08213141560554504\n",
      "episode: 8000 17:57:50 avg last 10 rewards 1.0 avg last 10 loss 0.02392382179386914 avg last 10 loss 2 0.3537429451942444\n",
      "episode: 8500 18:01:07 avg last 10 rewards 0.8 avg last 10 loss 0.01820083284401335 avg last 10 loss 2 0.21470364183187485\n",
      "episode: 9000 18:04:29 avg last 10 rewards 0.8 avg last 10 loss 0.026449672045418992 avg last 10 loss 2 0.22382574081420897\n",
      "episode: 9500 18:07:45 avg last 10 rewards 0.8 avg last 10 loss 0.025257635588059203 avg last 10 loss 2 0.11186180710792541\n",
      "episode: 10000 18:11:06 avg last 10 rewards 0.6 avg last 10 loss 0.01912443452165462 avg last 10 loss 2 0.11919987797737122\n",
      "10000 18:11:06\n",
      "episode: 10500 18:14:23 avg last 10 rewards 0.8 avg last 10 loss 0.021866803124430588 avg last 10 loss 2 0.027920019626617432\n",
      "episode: 11000 18:17:44 avg last 10 rewards 1.0 avg last 10 loss 0.023694825562415645 avg last 10 loss 2 0.04865076541900635\n",
      "episode: 11500 18:21:02 avg last 10 rewards 0.8 avg last 10 loss 0.02267486060154624 avg last 10 loss 2 0.23852474689483644\n",
      "episode: 12000 18:24:34 avg last 10 rewards 0.6 avg last 10 loss 0.020222260802984237 avg last 10 loss 2 0.22849146127700806\n",
      "episode: 12500 18:27:55 avg last 10 rewards 0.8 avg last 10 loss 0.02261756352381781 avg last 10 loss 2 0.053674960136413576\n",
      "episode: 13000 18:31:15 avg last 10 rewards 1.0 avg last 10 loss 0.021374070750607644 avg last 10 loss 2 0.05790464878082276\n",
      "episode: 13500 18:34:35 avg last 10 rewards 1.0 avg last 10 loss 0.01734401395660825 avg last 10 loss 2 0.06119058132171631\n",
      "episode: 14000 18:37:52 avg last 10 rewards 0.4 avg last 10 loss 0.01932020578533411 avg last 10 loss 2 0.029773527383804323\n",
      "episode: 14500 18:41:11 avg last 10 rewards 0.8 avg last 10 loss 0.02853787703497801 avg last 10 loss 2 0.14878890812397003\n",
      "episode: 15000 18:44:27 avg last 10 rewards 0.6 avg last 10 loss 0.022999464266467838 avg last 10 loss 2 0.12561931014060973\n",
      "episode: 15500 18:47:56 avg last 10 rewards 1.0 avg last 10 loss 0.020680271906894632 avg last 10 loss 2 0.022252166271209718\n",
      "episode: 16000 18:51:16 avg last 10 rewards 0.8 avg last 10 loss 0.026750299095874654 avg last 10 loss 2 0.11263070702552795\n",
      "episode: 16500 18:54:41 avg last 10 rewards 0.4 avg last 10 loss 0.02388972570915939 avg last 10 loss 2 0.010311025381088256\n",
      "episode: 17000 18:57:59 avg last 10 rewards 0.6 avg last 10 loss 0.0226449498324655 avg last 10 loss 2 0.40056796073913575\n",
      "episode: 17500 19:01:25 avg last 10 rewards 0.4 avg last 10 loss 0.029376733739627525 avg last 10 loss 2 0.021258461475372314\n",
      "episode: 18000 19:04:43 avg last 10 rewards 0.6 avg last 10 loss 0.028419436179683545 avg last 10 loss 2 0.29812229573726656\n",
      "episode: 18500 19:08:11 avg last 10 rewards 0.6 avg last 10 loss 0.027106644003652036 avg last 10 loss 2 0.15988746881484986\n",
      "episode: 19000 19:11:36 avg last 10 rewards 1.0 avg last 10 loss 0.026430796837667005 avg last 10 loss 2 0.3114521622657776\n",
      "episode: 19500 19:14:53 avg last 10 rewards 0.8 avg last 10 loss 0.02596374735585414 avg last 10 loss 2 0.1498170554637909\n",
      "episode: 20000 19:18:20 avg last 10 rewards 0.8 avg last 10 loss 0.018937818404810968 avg last 10 loss 2 0.038209176063537596\n",
      "20000 19:18:20\n",
      "episode: 20500 19:21:36 avg last 10 rewards 0.8 avg last 10 loss 0.02942263095173985 avg last 10 loss 2 0.10476011037826538\n",
      "episode: 21000 19:24:56 avg last 10 rewards 0.8 avg last 10 loss 0.02636377209564671 avg last 10 loss 2 0.14837605953216554\n",
      "episode: 21500 19:28:13 avg last 10 rewards 0.8 avg last 10 loss 0.02681429119547829 avg last 10 loss 2 0.10941568613052369\n",
      "episode: 22000 19:31:36 avg last 10 rewards 0.4 avg last 10 loss 0.02731714325491339 avg last 10 loss 2 0.1457536041736603\n",
      "episode: 22500 19:34:54 avg last 10 rewards 0.8 avg last 10 loss 0.03397075524553657 avg last 10 loss 2 0.15350418090820311\n",
      "episode: 23000 19:38:18 avg last 10 rewards 0.8 avg last 10 loss 0.02784043357241899 avg last 10 loss 2 0.24876033663749694\n",
      "episode: 23500 19:41:32 avg last 10 rewards 0.8 avg last 10 loss 0.03179527148604393 avg last 10 loss 2 0.16183732748031615\n",
      "episode: 24000 19:44:53 avg last 10 rewards 0.8 avg last 10 loss 0.030651010829024015 avg last 10 loss 2 0.2962603807449341\n",
      "episode: 24500 19:48:10 avg last 10 rewards 0.8 avg last 10 loss 0.03211965502705425 avg last 10 loss 2 0.43186131715774534\n",
      "episode: 25000 19:51:32 avg last 10 rewards 0.8 avg last 10 loss 0.02832031117286533 avg last 10 loss 2 0.2775457054376602\n",
      "episode: 25500 19:54:50 avg last 10 rewards 1.0 avg last 10 loss 0.028151365858502687 avg last 10 loss 2 0.21906540989875795\n",
      "episode: 26000 19:58:11 avg last 10 rewards 0.8 avg last 10 loss 0.029925543686840684 avg last 10 loss 2 0.16592999398708344\n",
      "episode: 26500 20:00:36 avg last 10 rewards 1.0 avg last 10 loss 0.030187051557004452 avg last 10 loss 2 0.04465591311454773\n",
      "episode: 27000 20:02:55 avg last 10 rewards 0.6 avg last 10 loss 0.030646323109976947 avg last 10 loss 2 0.13794362545013428\n",
      "episode: 27500 20:06:03 avg last 10 rewards 0.8 avg last 10 loss 0.02761611412279308 avg last 10 loss 2 0.12484759092330933\n",
      "episode: 28000 20:08:37 avg last 10 rewards 1.0 avg last 10 loss 0.028115625074133278 avg last 10 loss 2 0.10292711853981018\n",
      "episode: 28500 20:10:58 avg last 10 rewards 0.8 avg last 10 loss 0.026298709353432058 avg last 10 loss 2 0.13144429624080659\n",
      "episode: 29000 20:14:22 avg last 10 rewards 1.0 avg last 10 loss 0.022588045755401253 avg last 10 loss 2 0.15514300465583802\n",
      "episode: 29500 20:17:41 avg last 10 rewards 0.4 avg last 10 loss 0.02604351013433188 avg last 10 loss 2 0.15923407375812532\n",
      "episode: 30000 20:21:03 avg last 10 rewards 0.6 avg last 10 loss 0.020440317725297065 avg last 10 loss 2 0.1850142151117325\n",
      "30000 20:21:03\n",
      "episode: 30500 20:24:17 avg last 10 rewards 0.6 avg last 10 loss 0.023631296423263847 avg last 10 loss 2 0.08581969141960144\n",
      "episode: 31000 20:27:39 avg last 10 rewards 0.8 avg last 10 loss 0.02492035166360438 avg last 10 loss 2 0.13503189086914064\n",
      "episode: 31500 20:30:55 avg last 10 rewards 1.0 avg last 10 loss 0.02328810404869728 avg last 10 loss 2 0.23730575442314147\n",
      "episode: 32000 20:34:15 avg last 10 rewards 0.8 avg last 10 loss 0.020571688399650158 avg last 10 loss 2 0.17663044333457947\n",
      "episode: 32500 20:37:34 avg last 10 rewards 1.0 avg last 10 loss 0.021769668359775098 avg last 10 loss 2 0.16892362833023072\n",
      "episode: 33000 20:40:57 avg last 10 rewards 1.0 avg last 10 loss 0.024034445104189218 avg last 10 loss 2 0.2619546175003052\n",
      "episode: 33500 20:44:16 avg last 10 rewards 0.8 avg last 10 loss 0.020107496273703875 avg last 10 loss 2 0.13880982398986816\n",
      "episode: 34000 20:47:33 avg last 10 rewards 1.0 avg last 10 loss 0.027761063911020754 avg last 10 loss 2 0.503646069765091\n",
      "episode: 34500 20:50:58 avg last 10 rewards 1.0 avg last 10 loss 0.02308101477101445 avg last 10 loss 2 0.18597909808158875\n",
      "episode: 35000 20:54:07 avg last 10 rewards 0.8 avg last 10 loss 0.02602405350189656 avg last 10 loss 2 0.11626875400543213\n",
      "episode: 35500 20:57:19 avg last 10 rewards 0.8 avg last 10 loss 0.02035683117574081 avg last 10 loss 2 0.36570659279823303\n",
      "episode: 36000 21:00:21 avg last 10 rewards 0.8 avg last 10 loss 0.027646588766947387 avg last 10 loss 2 0.1498083770275116\n",
      "episode: 36500 21:03:40 avg last 10 rewards 1.0 avg last 10 loss 0.026697522145695984 avg last 10 loss 2 0.2046653985977173\n",
      "episode: 37000 21:06:45 avg last 10 rewards 1.0 avg last 10 loss 0.026119410688988865 avg last 10 loss 2 0.10556972026824951\n",
      "episode: 37500 21:10:06 avg last 10 rewards 0.6 avg last 10 loss 0.024541491561103614 avg last 10 loss 2 0.1977764219045639\n",
      "episode: 38000 21:13:25 avg last 10 rewards 0.4 avg last 10 loss 0.0330518702045083 avg last 10 loss 2 0.23067671358585357\n",
      "episode: 38500 21:16:53 avg last 10 rewards 0.8 avg last 10 loss 0.026257236930541696 avg last 10 loss 2 0.03339923024177551\n",
      "episode: 39000 21:20:11 avg last 10 rewards 0.8 avg last 10 loss 0.03282670890912413 avg last 10 loss 2 0.16612337827682494\n",
      "episode: 39500 21:23:39 avg last 10 rewards 0.8 avg last 10 loss 0.026952629152219743 avg last 10 loss 2 0.22355958819389343\n",
      "episode: 40000 21:26:56 avg last 10 rewards 0.8 avg last 10 loss 0.02310209898278117 avg last 10 loss 2 0.12400094866752624\n",
      "40000 21:26:56\n",
      "episode: 40500 21:30:20 avg last 10 rewards 0.2 avg last 10 loss 0.029100718337576836 avg last 10 loss 2 0.28567450046539306\n",
      "episode: 41000 21:33:41 avg last 10 rewards 0.8 avg last 10 loss 0.025443905196152628 avg last 10 loss 2 0.3576591074466705\n",
      "episode: 41500 21:36:50 avg last 10 rewards 0.8 avg last 10 loss 0.022845081915147603 avg last 10 loss 2 0.22436122894287108\n",
      "episode: 42000 21:40:22 avg last 10 rewards 0.6 avg last 10 loss 0.027181707427371294 avg last 10 loss 2 0.2196311593055725\n",
      "episode: 42500 21:42:36 avg last 10 rewards 0.4 avg last 10 loss 0.02069737616693601 avg last 10 loss 2 0.28845602571964263\n",
      "episode: 43000 21:44:42 avg last 10 rewards 0.6 avg last 10 loss 0.020434387889690698 avg last 10 loss 2 0.157364559173584\n",
      "episode: 43500 21:47:21 avg last 10 rewards 1.0 avg last 10 loss 0.022081346623599528 avg last 10 loss 2 0.24818792641162873\n",
      "episode: 44000 21:49:44 avg last 10 rewards 0.8 avg last 10 loss 0.020328907947987317 avg last 10 loss 2 0.13433963060379028\n",
      "episode: 44500 21:52:18 avg last 10 rewards 0.8 avg last 10 loss 0.025190099631436168 avg last 10 loss 2 0.17514113783836366\n",
      "episode: 45000 21:54:45 avg last 10 rewards 0.8 avg last 10 loss 0.025664003961719573 avg last 10 loss 2 0.1234338790178299\n",
      "episode: 45500 21:56:51 avg last 10 rewards 0.8 avg last 10 loss 0.018561375967692584 avg last 10 loss 2 0.13720549941062926\n",
      "episode: 46000 21:59:47 avg last 10 rewards 0.8 avg last 10 loss 0.020440029492601753 avg last 10 loss 2 0.14132983088493348\n",
      "episode: 46500 22:02:05 avg last 10 rewards 1.0 avg last 10 loss 0.021277618838939815 avg last 10 loss 2 0.2446659564971924\n",
      "episode: 47000 22:04:43 avg last 10 rewards 1.0 avg last 10 loss 0.02408191708382219 avg last 10 loss 2 0.4277091145515442\n",
      "episode: 47500 22:07:50 avg last 10 rewards 0.8 avg last 10 loss 0.02294509808998555 avg last 10 loss 2 0.1497869074344635\n",
      "episode: 48000 22:09:52 avg last 10 rewards 1.0 avg last 10 loss 0.023952230880968272 avg last 10 loss 2 0.1307559460401535\n",
      "episode: 48500 22:12:16 avg last 10 rewards 0.8 avg last 10 loss 0.025626669311895965 avg last 10 loss 2 0.1411571979522705\n",
      "episode: 49000 22:14:29 avg last 10 rewards 0.8 avg last 10 loss 0.018014476471580564 avg last 10 loss 2 0.125026136636734\n",
      "episode: 49500 22:16:44 avg last 10 rewards 0.8 avg last 10 loss 0.028305363049730657 avg last 10 loss 2 0.22647125720977784\n",
      "Fin 22:19:19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from kaggle_environments import make\n",
    "from collections import deque\n",
    "\n",
    "#import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "action_size = env.configuration.columns\n",
    "num_cols = env.configuration.columns\n",
    "num_rows = env.configuration.rows\n",
    "state_size = env.configuration.columns * env.configuration.rows\n",
    "\n",
    "# Training agent in first position (player 1) against the negamax/random agent.\n",
    "#trainer = env.train([None, \"negamax\"])\n",
    "trainer = env.train([None, \"random\"])\n",
    "\n",
    "discount = 0.9\n",
    "#epsilon = 0.99 # parameter for epsilon-greedy policy\n",
    "#epsilon = 0.01 # parameter for epsilon-greedy policy\n",
    "epsilon = 0.22089720180217606 # parameter for epsilon-greedy policy\n",
    "epsilon_min = 0.05 # epsilon is decreasing but we avoid it to be smaller than epsilon_min\n",
    "t_max = 1000 # used for environments without end\n",
    "total_reward = 0 # used for displaying the evolution of the rewards as the model learns\n",
    "memory = deque(maxlen=10000)\n",
    "batch_size = 128\n",
    "accum_train_loss_2 = []\n",
    "last_n_steps = 2\n",
    "num_episodes = 50000\n",
    "\n",
    "#new_model = True\n",
    "new_model = False\n",
    "# Create/Load model\n",
    "if new_model:\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(8, kernel_size=3, activation='relu', input_shape=(num_rows, num_cols, 1))) # padding=valid # Input: (5, 5, 1) Output: (3, 3, 8)\n",
    "    model.add(Flatten()) # Input: (3, 3, 8) Output: (72)\n",
    "    model.add(Dense(8, activation='relu')) # Input: (72) Output: (8)\n",
    "    model.add(Dense(action_size, activation=\"linear\")) # Input: (8) Output: (5)\n",
    "    model.compile(Adam(lr=0.01), loss='mse')\n",
    "    accum_train_loss = []\n",
    "    rewards_list_deep_ql = []\n",
    "    accum_train_loss_2 = []\n",
    "else:\n",
    "    model = load_model('model_DQN.h5')\n",
    "\n",
    "def get_best_action(board, num_cols, model, function):\n",
    "    \"\"\"\n",
    "    Returns the action with maximum q-value or minimum q-value depending on the needs. This will be done only in available columns\n",
    "    Parameter \"function\" should be \"max\" or \"min\"\n",
    "    \"\"\"\n",
    "    # function parameter validation\n",
    "    if (function != \"max\" and function != \"min\"):\n",
    "        print(\"Parameter function should be \"\"max\"\" or \"\"min\"\"\")\n",
    "        return -1\n",
    "    \n",
    "    # greedy policy\n",
    "    if (function == \"max\"):\n",
    "        best_q_value = float(\"-inf\")\n",
    "    else:\n",
    "        best_q_value = float(\"inf\")\n",
    "    \n",
    "    # Search the best q-value but only in available columns (we cannot use np.argmax)\n",
    "    best_action = None\n",
    "    rewards = model(board).numpy()[0]\n",
    "    available_cols = [col for col in range(num_cols) if board[0][0][col] == 0]\n",
    "    for col in available_cols:\n",
    "        q_value = rewards[col]\n",
    "        if (function == \"max\" and q_value > best_q_value):\n",
    "            best_action = col\n",
    "            best_q_value = q_value\n",
    "        else:\n",
    "            if (function == \"min\" and q_value < best_q_value):\n",
    "                best_action = col\n",
    "                best_q_value = q_value\n",
    "                \n",
    "    return best_action\n",
    "\n",
    "def get_action(board, num_cols, epsilon, model):\n",
    "    available_cols = [col for col in range(num_cols) if board[0][0][col] == 0]\n",
    "    # epison-greedy policy\n",
    "    if (np.random.random() > epsilon):\n",
    "        # greedy\n",
    "        # We need to search best value, but only in available columns, so we cannot use np.argmax\n",
    "        return get_best_action(board, num_cols, model, \"max\")\n",
    "    else:\n",
    "        # random\n",
    "        return int(np.random.choice(available_cols))\n",
    "\n",
    "def convert_for_CNN(board, num_rows, num_cols):\n",
    "    \"\"\"\n",
    "    Converts the board (list) into a matrix of shape (rows, cols, 1), so that a CNN can work with it.\n",
    "    Player 2 checkers are replaced to -1\n",
    "    \"\"\"\n",
    "    board = [np.float32(-1) if x==2 else np.float32(x) for x in board]\n",
    "    return np.reshape(board, [1, num_rows, num_cols, 1])\n",
    "    \n",
    "    \n",
    "t = datetime.datetime.now()\n",
    "print(\"Inicio\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "obs = trainer.reset()\n",
    "for i in range(num_episodes):\n",
    "    done = False\n",
    "    t = 0\n",
    "    while not done and t < t_max:\n",
    "        # Convert board in a matrix to use it as the neural network input\n",
    "        board_matrix = convert_for_CNN(obs.board, num_rows, num_cols) #np.reshape(obs.board, [1, state_size])\n",
    "        action = get_action(board_matrix, num_cols, epsilon, model)\n",
    "        obs, reward, done, info = trainer.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        t += 1\n",
    "\n",
    "    # Train at the end of each episode\n",
    "    \n",
    "    # Add to the experience replay only the last n steps of the episode (all of them as player 1)\n",
    "    num_steps = env.steps[-1][0][\"observation\"][\"step\"]\n",
    "    last_reward = env.steps[-1][0][\"reward\"]\n",
    "    for j in range(1, num_steps + 1):\n",
    "        state = convert_for_CNN(env.steps[j-1][0][\"observation\"][\"board\"], num_rows, num_cols)\n",
    "        next_state = convert_for_CNN(env.steps[j][0][\"observation\"][\"board\"], num_rows, num_cols)\n",
    "        done = (env.steps[j][0][\"status\"] == 'DONE')\n",
    "        # REWARD SHAPING --> As we only have reward at the end of the episode, we generate partial rewards\n",
    "        if done:\n",
    "            reward = last_reward * 2\n",
    "        else:\n",
    "            reward = (0.5 ** (num_steps - j)) * last_reward\n",
    "        if env.steps[j-1][0][\"status\"] == 'ACTIVE':\n",
    "            # Player 1\n",
    "            action = env.steps[j][0][\"action\"]\n",
    "        else:\n",
    "            # Player 2 --> Convert player 2 movements into player one's\n",
    "            action = env.steps[j][1][\"action\"]\n",
    "            # Convert player 2 boards to player 1's\n",
    "            state = (state * (-1)) + 0 # +0 to avoid negative zeros\n",
    "            next_state = (next_state * (-1)) + 0 # +0 to avoid negative zeros\n",
    "            reward = reward *(-1) + 0\n",
    "        player = 1\n",
    "        if (j > num_steps - last_n_steps):\n",
    "            memory.append((state, action, reward, next_state, player, done))\n",
    "\n",
    "    # Extract a minibatch from the experience replay and train the neural network\n",
    "    if len(memory) > batch_size:\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "            \n",
    "        X_states = np.empty(shape=(batch_size, num_rows, num_cols, 1))\n",
    "        y_targets = np.empty(shape=(batch_size, action_size))\n",
    "            \n",
    "        for j, (state_batch, action_batch, reward_batch, next_state_batch, player_batch, done_batch) in enumerate(minibatch):\n",
    "            X_states[j] = state_batch[0]\n",
    "            # If it's an ending state, just update the target with the reward. If not, update target using Bellman optimality equation for Q-values\n",
    "            if not done:\n",
    "                target_updated = (reward_batch + discount*np.amax(model(next_state_batch).numpy()[0]))\n",
    "            else:\n",
    "                target_updated = reward_batch\n",
    "                \n",
    "            targets = model(state_batch).numpy()\n",
    "            #print(\"Target obtenida:\", targets[0][action_batch], \"Target objetivo:\", target_updated)\n",
    "            loss = abs(target_updated - targets[0][action_batch])\n",
    "            accum_train_loss_2.append(loss)\n",
    "            targets[0][action_batch] = target_updated\n",
    "            y_targets[j] = targets[0]\n",
    "            \n",
    "        # Train the model with these new targets\n",
    "        history = model.fit(X_states, y_targets, epochs=1, callbacks=callbacks_list, verbose=0)\n",
    "            \n",
    "        accum_train_loss.append(history.history['loss'][0])\n",
    "            \n",
    "    rewards_list_deep_ql.append(total_reward)\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Print a summary each 500 episodes\n",
    "    if (i % 500) == 0: # and len(accum_train_loss)>=10\n",
    "        print(\"episode:\", i, datetime.datetime.now().strftime(\"%H:%M:%S\"), \"avg last 10 rewards\", sum(rewards_list_deep_ql[-10:])/10, \n",
    "              \"avg last 10 loss\", sum(accum_train_loss[-10:])/10, \n",
    "              \"avg last 10 loss 2\", sum(accum_train_loss_2[-10:])/10)\n",
    "    \n",
    "    # Decrease epsilon in every step so that exploration (random) is getting smaller against exploitation (greedy)\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= 0.99999\n",
    "        \n",
    "    obs = trainer.reset()\n",
    "            \n",
    "    if (i % 10000) == 0 and i > 0:\n",
    "        print(i, datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    if (i % 20000) == 0 and i > 0:\n",
    "        last_n_steps += 1\n",
    "        \n",
    "t = datetime.datetime.now()\n",
    "print(\"Fin\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# 50000 CNN random, fit con todo el minibatch (Local) solo los n pasos finales (todos como jugador 1) # 50.000\n",
    "# 10:35:31\n",
    "# 16:18:54\n",
    "# 50000 CNN random, fit con todo el minibatch (Local) solo los n pasos finales (todos como jugador 1) # 100.000\n",
    "# 18:49:19\n",
    "# 22:28:51\n",
    "# 50000 CNN random, fit con todo el minibatch (Local) solo los n pasos finales (todos como jugador 1) # 150.000 <--- MEJOR RESULTADO\n",
    "# 10:14:44\n",
    "# 16:12:15\n",
    "# 50000 CNN random, fit con todo el minibatch (Local) solo los n pasos finales (todos como jugador 1) # 200.000\n",
    "# 17:09:03\n",
    "# 22:19:19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  0,  1]), array([ 31445,      1, 168554], dtype=int64))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.unique(rewards_list_deep_ql, return_counts=True)\n",
    "# (array([-1,  0,  1]), array([12947,     1, 37052], dtype=int64))\n",
    "# (array([-1,  0,  1]), array([22143,     1, 77856], dtype=int64))\n",
    "# (array([-1,  0,  1]), array([ 27200,      1, 122799], dtype=int64))\n",
    "# (array([-1,  0,  1]), array([ 31445,      1, 168554], dtype=int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Q-Learning mean reward = 0.824\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html\n",
    "# ewm --> Weighted average of the values from x0 to xt. The weight of each element is decreasing from xt (weight=1) to xt-n (weight=(1-alpha)^n). alpha=2/(span+1)=2/101\n",
    "# This function gives us the trend of the rewards, instead of a sharp graph with a lot of -1's and 1's\n",
    "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "# Displaying the average of n elemtens instead of each element of the list \n",
    "n = 1000\n",
    "moving_average2 = [sum(rewards_list_deep_ql[i:i+n])/n for i in range(0,len(rewards_list_deep_ql)-n+1)]\n",
    "\n",
    "clear_output(True)\n",
    "print('Deep Q-Learning mean reward =', np.mean(rewards_list_deep_ql[-1000:]))\n",
    "plt.title(\"epsilon = %s\" % epsilon)\n",
    "plt.plot(moving_average(rewards_list_deep_ql), label='ev_deep_ql')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.show()\n",
    "\n",
    "# Deep Q-Learning mean reward = 0.688\n",
    "# Deep Q-Learning mean reward = 0.87\n",
    "# Deep Q-Learning mean reward = 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13398059058392722"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Q-Learning mean loss 1 = 0.023926833822741172\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html\n",
    "# ewm --> Weighted average of the values from x0 to xt. The weight of each element is decreasing from xt (weight=1) to xt-n (weight=(1-alpha)^n). alpha=2/(span+1)=2/101\n",
    "# This function gives us the trend of the rewards, instead of a sharp graph with a lot of -1's and 1's\n",
    "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "# Displaying the average of n elemtens instead of each element of the list \n",
    "n = 1000\n",
    "moving_average2 = [sum(accum_train_loss[i:i+n])/n for i in range(0,len(accum_train_loss)-n+1)]\n",
    "\n",
    "clear_output(True)\n",
    "print('Deep Q-Learning mean loss 1 =', np.mean(accum_train_loss[-1000:]))\n",
    "plt.title(\"epsilon = %s\" % epsilon)\n",
    "plt.plot(moving_average(accum_train_loss), label='loss_deep_ql')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(0, 0.20)\n",
    "plt.show()\n",
    "\n",
    "# Deep Q-Learning mean loss 1 = 0.032533711088239214\n",
    "# Deep Q-Learning mean loss 1 = 0.02243873758718837\n",
    "# Deep Q-Learning mean loss 1 = 0.023926833822741172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html\n",
    "# ewm --> Weighted average of the values from x0 to xt. The weight of each element is decreasing from xt (weight=1) to xt-n (weight=(1-alpha)^n). alpha=2/(span+1)=2/101\n",
    "# This function gives us the trend of the rewards, instead of a sharp graph with a lot of -1's and 1's\n",
    "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "# Displaying the average of n elemtens instead of each element of the list \n",
    "n = 1000\n",
    "moving_average2 = [sum(accum_train_loss_2[i:i+n])/n for i in range(0,len(accum_train_loss_2)-n+1)]\n",
    "\n",
    "clear_output(True)\n",
    "print('Deep Q-Learning mean loss 2 =', np.mean(accum_train_loss_2[-1000:]))\n",
    "plt.title(\"epsilon = %s\" % epsilon)\n",
    "plt.plot(moving_average(accum_train_loss_2), label='loss_deep_ql')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_DQN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Save list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'rewards_list_deep_ql.txt','w+') as f:\n",
    "    for element in rewards_list_deep_ql:\n",
    "        f.write(str(element) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Save list of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'loss_list_deep_ql.txt','w+') as f:\n",
    "    for element in accum_train_loss:\n",
    "        f.write(str(element) + '\\n')\n",
    "        \n",
    "with open(r'loss_2_list_deep_ql.txt','w+') as f:\n",
    "    for element in accum_train_loss_2:\n",
    "        f.write(str(element) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.plot(accum_train_loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx0QJniRhu4_"
   },
   "source": [
    "Displaying kernels in Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Eil-vNA76R8D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "shape of weights[0]:  (3, 3, 1, 8)\n",
      "shape of weights[1]:  (8,)\n",
      "shape of weights[2]:  (72, 8)\n",
      "shape of weights[3]:  (8,)\n",
      "shape of weights[4]:  (8, 5)\n",
      "shape of weights[5]:  (5,)\n"
     ]
    }
   ],
   "source": [
    "weights = model.get_weights()\n",
    "print(np.shape(weights))\n",
    "for i in range(len(weights)):\n",
    "    print('shape of weights[%d]: ' % i, np.shape(weights[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "QP7WnsQ1hxOo"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x540 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# kernels de la primera capa convolucional\n",
    "ncapa = 0\n",
    "\n",
    "nfilters = weights[ncapa].shape[3]\n",
    "ncols = 4 # nmero de columnas en la figura\n",
    "\n",
    "\n",
    "ma = abs(weights[ncapa]).max()\n",
    "nrows = int(np.ceil(nfilters/ncols)) # nmero de filas en la figura\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15,15*nrows/ncols))\n",
    "axes_r = axes.ravel()\n",
    "for i in range(nfilters):\n",
    "    kernel = weights[ncapa][:,:,0,i]\n",
    "    ax = axes_r[i]\n",
    "    ax.imshow(kernel, vmin=-ma, vmax=ma, cmap='bwr')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.set_title('pesos kernel %d' % i, fontsize=10)\n",
    "for i in range(nfilters,nrows*ncols):\n",
    "    fig.delaxes(axes_r[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 5000 Negative: 5000 Zero: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPZ0lEQVR4nO3cf6jd9X3H8edriRVplSmJNk1i40oGi8K0XrIMYTg6aub+iIUK8Y8ahpBOLFToP7GDtf8E7FhbEKYjnWKEthJoXcKmXVMplILVXiVtjGlmVp3eJpi0HTVlwy3pe3+cb+Dseu695/46t/d+ng84nO95fz/f7/fzyffklZPP+Z5vqgpJUht+Z6k7IEkaHUNfkhpi6EtSQwx9SWqIoS9JDVm91B2YyZo1a2rTpk1L3Q1JWlZefPHFn1fV2sn13/rQ37RpE+Pj40vdDUlaVpL8x6C60zuS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpITOGfpKNSb6b5HiSY0k+3dU/n+RnSY50j9v7tnkgyckkJ5Lc1le/OcnRbt1DSbI4w5IkDTLMdfrngc9U1UtJLgdeTHK4W/flqvq7/sZJtgA7geuBDwDfSfL7VXUBeATYDfwAeBrYDjyzMEORJM1kxk/6VXW6ql7qls8Bx4H102yyA3iyqt6pqteAk8DWJOuAK6rquerdxP8J4I75DkCSNLxZ/SI3ySbgJuB54BbgU0nuBsbp/W/gP+n9g/CDvs0mutr/dsuT64OOs5ve/wi49tprZ9PF/2fTnn+Z87bL1esP/sWSHHcp/6yXasxqw1K9txfrfT30F7lJ3gd8A7i/qt6mN1XzIeBG4DTwxYtNB2xe09TfXazaV1VjVTW2du27bh0hSZqjoUI/ySX0Av+rVfVNgKp6q6ouVNVvgK8AW7vmE8DGvs03AKe6+oYBdUnSiAxz9U6AR4HjVfWlvvq6vmYfA17ulg8BO5NcmuQ6YDPwQlWdBs4l2dbt827g4AKNQ5I0hGHm9G8BPgEcTXKkq30WuCvJjfSmaF4HPglQVceSHABeoXflz33dlTsA9wKPA5fRu2rHK3ckaYRmDP2q+j6D5+OfnmabvcDeAfVx4IbZdFCStHD8Ra4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpITOGfpKNSb6b5HiSY0k+3dWvSnI4yavd85V92zyQ5GSSE0lu66vfnORot+6hJFmcYUmSBhnmk/554DNV9QfANuC+JFuAPcCzVbUZeLZ7TbduJ3A9sB14OMmqbl+PALuBzd1j+wKORZI0gxlDv6pOV9VL3fI54DiwHtgB7O+a7Qfu6JZ3AE9W1TtV9RpwEtiaZB1wRVU9V1UFPNG3jSRpBGY1p59kE3AT8DxwTVWdht4/DMDVXbP1wJt9m010tfXd8uT6oOPsTjKeZPzs2bOz6aIkaRpDh36S9wHfAO6vqrenazqgVtPU312s2ldVY1U1tnbt2mG7KEmawVChn+QSeoH/1ar6Zld+q5uyoXs+09UngI19m28ATnX1DQPqkqQRGebqnQCPAser6kt9qw4Bu7rlXcDBvvrOJJcmuY7eF7YvdFNA55Js6/Z5d982kqQRWD1Em1uATwBHkxzpap8FHgQOJLkHeAO4E6CqjiU5ALxC78qf+6rqQrfdvcDjwGXAM91DkjQiM4Z+VX2fwfPxAB+ZYpu9wN4B9XHghtl0UJK0cPxFriQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0JakhM4Z+kseSnEnycl/t80l+luRI97i9b90DSU4mOZHktr76zUmOduseSpKFH44kaTrDfNJ/HNg+oP7lqrqxezwNkGQLsBO4vtvm4SSruvaPALuBzd1j0D4lSYtoxtCvqu8BvxxyfzuAJ6vqnap6DTgJbE2yDriiqp6rqgKeAO6YY58lSXM0nzn9TyX5cTf9c2VXWw+82ddmoqut75Yn1yVJIzTX0H8E+BBwI3Aa+GJXHzRPX9PUB0qyO8l4kvGzZ8/OsYuSpMnmFPpV9VZVXaiq3wBfAbZ2qyaAjX1NNwCnuvqGAfWp9r+vqsaqamzt2rVz6aIkaYA5hX43R3/Rx4CLV/YcAnYmuTTJdfS+sH2hqk4D55Js667auRs4OI9+S5LmYPVMDZJ8HbgVWJNkAvgccGuSG+lN0bwOfBKgqo4lOQC8ApwH7quqC92u7qV3JdBlwDPdQ5I0QjOGflXdNaD86DTt9wJ7B9THgRtm1TtJ0oLyF7mS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktSQGUM/yWNJziR5ua92VZLDSV7tnq/sW/dAkpNJTiS5ra9+c5Kj3bqHkmThhyNJms4wn/QfB7ZPqu0Bnq2qzcCz3WuSbAF2Atd32zycZFW3zSPAbmBz95i8T0nSIpsx9Kvqe8AvJ5V3APu75f3AHX31J6vqnap6DTgJbE2yDriiqp6rqgKe6NtGkjQic53Tv6aqTgN0z1d39fXAm33tJrra+m55cn2gJLuTjCcZP3v27By7KEmabKG/yB00T1/T1Aeqqn1VNVZVY2vXrl2wzklS6+Ya+m91UzZ0z2e6+gSwsa/dBuBUV98woC5JGqG5hv4hYFe3vAs42FffmeTSJNfR+8L2hW4K6FySbd1VO3f3bSNJGpHVMzVI8nXgVmBNkgngc8CDwIEk9wBvAHcCVNWxJAeAV4DzwH1VdaHb1b30rgS6DHime0iSRmjG0K+qu6ZY9ZEp2u8F9g6ojwM3zKp3kqQF5S9yJakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ2ZV+gneT3J0SRHkox3tauSHE7yavd8ZV/7B5KcTHIiyW3z7bwkaXYW4pP+n1bVjVU11r3eAzxbVZuBZ7vXJNkC7ASuB7YDDydZtQDHlyQNaTGmd3YA+7vl/cAdffUnq+qdqnoNOAlsXYTjS5KmMN/QL+DbSV5MsrurXVNVpwG656u7+nrgzb5tJ7rauyTZnWQ8yfjZs2fn2UVJ0kWr57n9LVV1KsnVwOEkP5mmbQbUalDDqtoH7AMYGxsb2EaSNHvz+qRfVae65zPAU/Sma95Ksg6gez7TNZ8ANvZtvgE4NZ/jS5JmZ86hn+S9SS6/uAx8FHgZOATs6prtAg52y4eAnUkuTXIdsBl4Ya7HlyTN3nymd64BnkpycT9fq6pvJfkhcCDJPcAbwJ0AVXUsyQHgFeA8cF9VXZhX7yVJszLn0K+qnwJ/OKD+C+AjU2yzF9g712NKkubHX+RKUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDRh76SbYnOZHkZJI9oz6+JLVspKGfZBXw98CfA1uAu5JsGWUfJKllo/6kvxU4WVU/rar/AZ4Edoy4D5LUrNUjPt564M2+1xPAH01ulGQ3sLt7+eskJ+Z4vDXAz+e47bKULzjmRrQ25tbGuxDv6w8OKo469DOgVu8qVO0D9s37YMl4VY3Ndz/LiWNuQ2tjbm28sHhjHvX0zgSwse/1BuDUiPsgSc0adej/ENic5Lok7wF2AodG3AdJatZIp3eq6nySTwH/CqwCHquqY4t4yHlPES1DjrkNrY25tfHCIo05Ve+aUpckrVD+IleSGmLoS1JDVlToJ7kqyeEkr3bPV07R7vUkR5McSTI+6n7O10y3skjPQ936Hyf58FL0cyENMeZbk/yqO6dHkvzNUvRzISV5LMmZJC9PsX4lnueZxryiznOSjUm+m+R4kmNJPj2gzcKe56paMQ/gb4E93fIe4AtTtHsdWLPU/Z3jGFcB/w78HvAe4EfAlkltbgeeofe7iG3A80vd7xGM+Vbgn5e6rws87j8BPgy8PMX6FXWehxzzijrPwDrgw93y5cC/Lfbf5xX1SZ/eLR32d8v7gTuWriuLZphbWewAnqieHwC/m2TdqDu6gJq8fUdVfQ/45TRNVtp5HmbMK0pVna6ql7rlc8Bxencu6Leg53mlhf41VXUaen+YwNVTtCvg20le7G75sJwMupXF5DfJMG2Wk2HH88dJfpTkmSTXj6ZrS2qlnedhrcjznGQTcBPw/KRVC3qeR30bhnlL8h3g/QNW/fUsdnNLVZ1KcjVwOMlPuk8Yy8Ewt7IY6nYXy8gw43kJ+GBV/TrJ7cA/AZsXu2NLbKWd52GsyPOc5H3AN4D7q+rtyasHbDLn87zsPulX1Z9V1Q0DHgeBty7+t6d7PjPFPk51z2eAp+hNHywXw9zKYqXd7mLG8VTV21X16275aeCSJGtG18UlsdLO84xW4nlOcgm9wP9qVX1zQJMFPc/LLvRncAjY1S3vAg5ObpDkvUkuv7gMfBQYeKXAb6lhbmVxCLi7+9Z/G/Cri9Ney9SMY07y/iTplrfSe2//YuQ9Ha2Vdp5ntNLOczeWR4HjVfWlKZot6HledtM7M3gQOJDkHuAN4E6AJB8A/rGqbgeuAZ7q3jerga9V1beWqL+zVlPcyiLJX3Xr/wF4mt43/ieB/wL+cqn6uxCGHPPHgXuTnAf+G9hZ3aUPy1WSr9O7WmVNkgngc8AlsDLPMww15pV2nm8BPgEcTXKkq30WuBYW5zx7GwZJashKm96RJE3D0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kN+T8tnGF/1ZYaVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "memory2 = memory.copy()\n",
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "zero = 0\n",
    "a = []\n",
    "for i in memory2:\n",
    "    a.append(i[2])\n",
    "    if i[2] > 0:\n",
    "        positive += 1\n",
    "    else:\n",
    "        if i[2] < 0:\n",
    "            negative += 1\n",
    "        else:\n",
    "            zero += 1\n",
    "\n",
    "print(\"Positive:\", positive, \"Negative:\", negative, \"Zero:\", zero)\n",
    "plt.hist(a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.5  , -0.125,  0.25 ,  2.   ]),\n",
       " array([2500, 2500, 2500, 2500], dtype=int64))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(a, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the agent as a function, so that we can use it with Kaggle environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\n",
      "{'remainingOverageTime': 60, 'step': 0, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\n",
      "{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\n",
      "{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0], 'mark': 1}\n",
      "Column:0 Reward:0.18380886\n",
      "Column:1 Reward:-0.055811584\n",
      "Column:2 Reward:0.19777715\n",
      "Column:3 Reward:-0.118466735\n",
      "Column:4 Reward:0.0851478\n",
      "{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\n",
      "{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0], 'mark': 1}\n",
      "Column:0 Reward:0.20683587\n",
      "Column:1 Reward:0.27731067\n",
      "Column:2 Reward:0.17978859\n",
      "Column:3 Reward:-0.08010566\n",
      "Column:4 Reward:0.060389668\n",
      "{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\n",
      "{'remainingOverageTime': 60, 'step': 6, 'board': [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 2, 1, 0, 0], 'mark': 1}\n",
      "Column:0 Reward:1.1892681\n",
      "Column:1 Reward:1.3185668\n",
      "Column:2 Reward:-0.50783277\n",
      "Column:3 Reward:1.0671141\n",
      "Column:4 Reward:0.6319868\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"<!--\n",
       "  Copyright 2020 Kaggle Inc\n",
       "\n",
       "  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "  you may not use this file except in compliance with the License.\n",
       "  You may obtain a copy of the License at\n",
       "\n",
       "      http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "  Unless required by applicable law or agreed to in writing, software\n",
       "  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "  See the License for the specific language governing permissions and\n",
       "  limitations under the License.\n",
       "-->\n",
       "<!DOCTYPE html>\n",
       "<html lang=&quot;en&quot;>\n",
       "  <head>\n",
       "    <title>Kaggle Simulation Player</title>\n",
       "    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n",
       "    <link\n",
       "      rel=&quot;stylesheet&quot;\n",
       "      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n",
       "      crossorigin=&quot;anonymous&quot;\n",
       "    />\n",
       "    <style type=&quot;text/css&quot;>\n",
       "      html,\n",
       "      body {\n",
       "        height: 100%;\n",
       "        font-family: sans-serif;\n",
       "        margin: 0px;\n",
       "      }\n",
       "      canvas {\n",
       "        /* image-rendering: -moz-crisp-edges;\n",
       "        image-rendering: -webkit-crisp-edges;\n",
       "        image-rendering: pixelated;\n",
       "        image-rendering: crisp-edges; */\n",
       "      }\n",
       "    </style>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n",
       "    <script>\n",
       "      // Polyfill for Styled Components\n",
       "      window.React = {\n",
       "        ...preact,\n",
       "        createElement: preact.h,\n",
       "        PropTypes: { func: {} },\n",
       "      };\n",
       "    </script>\n",
       "    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n",
       "  </head>\n",
       "  <body>\n",
       "    <script>\n",
       "      \n",
       "window.kaggle = {\n",
       "  &quot;debug&quot;: true,\n",
       "  &quot;playing&quot;: true,\n",
       "  &quot;step&quot;: 0,\n",
       "  &quot;controls&quot;: true,\n",
       "  &quot;environment&quot;: {\n",
       "    &quot;id&quot;: &quot;07d58f78-d605-11ec-ac3e-38142801973a&quot;,\n",
       "    &quot;name&quot;: &quot;connectx&quot;,\n",
       "    &quot;title&quot;: &quot;ConnectX&quot;,\n",
       "    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n",
       "    &quot;version&quot;: &quot;1.0.1&quot;,\n",
       "    &quot;configuration&quot;: {\n",
       "      &quot;rows&quot;: 5,\n",
       "      &quot;columns&quot;: 5,\n",
       "      &quot;inarow&quot;: 3,\n",
       "      &quot;episodeSteps&quot;: 1000,\n",
       "      &quot;actTimeout&quot;: 2,\n",
       "      &quot;runTimeout&quot;: 1200,\n",
       "      &quot;agentTimeout&quot;: 60,\n",
       "      &quot;timeout&quot;: 2\n",
       "    },\n",
       "    &quot;specification&quot;: {\n",
       "      &quot;action&quot;: {\n",
       "        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n",
       "        &quot;type&quot;: &quot;integer&quot;,\n",
       "        &quot;minimum&quot;: 0,\n",
       "        &quot;default&quot;: 0\n",
       "      },\n",
       "      &quot;agents&quot;: [\n",
       "        2\n",
       "      ],\n",
       "      &quot;configuration&quot;: {\n",
       "        &quot;episodeSteps&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;minimum&quot;: 1,\n",
       "          &quot;default&quot;: 1000\n",
       "        },\n",
       "        &quot;actTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 2\n",
       "        },\n",
       "        &quot;runTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 1200\n",
       "        },\n",
       "        &quot;columns&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 7,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;rows&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 6,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;inarow&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 4,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;agentTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;timeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 2,\n",
       "          &quot;minimum&quot;: 0\n",
       "        }\n",
       "      },\n",
       "      &quot;info&quot;: {},\n",
       "      &quot;observation&quot;: {\n",
       "        &quot;remainingOverageTime&quot;: {\n",
       "          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n",
       "          &quot;shared&quot;: false,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;step&quot;: {\n",
       "          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 0\n",
       "        },\n",
       "        &quot;board&quot;: {\n",
       "          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n",
       "          &quot;type&quot;: &quot;array&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;default&quot;: []\n",
       "        },\n",
       "        &quot;mark&quot;: {\n",
       "          &quot;defaults&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ],\n",
       "          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n",
       "          &quot;enum&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ]\n",
       "        }\n",
       "      },\n",
       "      &quot;reward&quot;: {\n",
       "        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n",
       "        &quot;enum&quot;: [\n",
       "          -1,\n",
       "          0,\n",
       "          1\n",
       "        ],\n",
       "        &quot;default&quot;: 0,\n",
       "        &quot;type&quot;: [\n",
       "          &quot;number&quot;,\n",
       "          &quot;null&quot;\n",
       "        ]\n",
       "      }\n",
       "    },\n",
       "    &quot;steps&quot;: [\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 0,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 1,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 2,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 3,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 4,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 5,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 6,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 7,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: -1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 8,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        }\n",
       "      ]\n",
       "    ],\n",
       "    &quot;rewards&quot;: [\n",
       "      -1,\n",
       "      1\n",
       "    ],\n",
       "    &quot;statuses&quot;: [\n",
       "      &quot;DONE&quot;,\n",
       "      &quot;DONE&quot;\n",
       "    ],\n",
       "    &quot;schema_version&quot;: 1,\n",
       "    &quot;info&quot;: {}\n",
       "  },\n",
       "  &quot;logs&quot;: [\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 4.9e-05,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 0, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.03746,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.584375,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0], 'mark': 1}\\nColumn:0 Reward:0.17041507\\nColumn:1 Reward:0.26072186\\nColumn:2 Reward:0.27578092\\nColumn:3 Reward:-0.18075359\\nColumn:4 Reward:0.047500253\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.004553,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.302765,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0], 'mark': 1}\\nColumn:0 Reward:-0.33774886\\nColumn:1 Reward:0.02001816\\nColumn:2 Reward:-0.143623\\nColumn:3 Reward:-0.25462592\\nColumn:4 Reward:-0.3382532\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000385,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.285789,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 6, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 2, 0], 'mark': 1}\\nColumn:0 Reward:-0.16815278\\nColumn:1 Reward:0.2981326\\nColumn:2 Reward:0.021047592\\nColumn:3 Reward:-0.19417012\\nColumn:4 Reward:-0.22917646\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000145,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.29783,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 8, 'board': [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2, 1, 1, 2, 0], 'mark': 1}\\nColumn:0 Reward:0.5908333\\nColumn:1 Reward:2.3303528\\nColumn:2 Reward:-0.9168894\\nColumn:3 Reward:1.0910288\\nColumn:4 Reward:0.19211501\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 2.5e-05,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 0, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.018938,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.517013,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0], 'mark': 1}\\nColumn:0 Reward:0.18380886\\nColumn:1 Reward:-0.055811584\\nColumn:2 Reward:0.19777715\\nColumn:3 Reward:-0.118466735\\nColumn:4 Reward:0.0851478\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.007758,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.64096,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0], 'mark': 1}\\nColumn:0 Reward:0.20683587\\nColumn:1 Reward:0.27731067\\nColumn:2 Reward:0.17978859\\nColumn:3 Reward:-0.08010566\\nColumn:4 Reward:0.060389668\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.006041,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.524808,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 6, 'board': [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 2, 1, 0, 0], 'mark': 1}\\nColumn:0 Reward:1.1892681\\nColumn:1 Reward:1.3185668\\nColumn:2 Reward:-0.50783277\\nColumn:3 Reward:1.0671141\\nColumn:4 Reward:0.6319868\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 7.6e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ]\n",
       "  ],\n",
       "  &quot;mode&quot;: &quot;ipython&quot;,\n",
       "  &quot;width&quot;: 500,\n",
       "  &quot;height&quot;: 450\n",
       "};\n",
       "\n",
       "\n",
       "window.kaggle.renderer = // Copyright 2020 Kaggle Inc\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "function renderer({\n",
       "  act,\n",
       "  agents,\n",
       "  environment,\n",
       "  frame,\n",
       "  height = 400,\n",
       "  interactive,\n",
       "  isInteractive,\n",
       "  parent,\n",
       "  step,\n",
       "  update,\n",
       "  width = 400,\n",
       "}) {\n",
       "  // Configuration.\n",
       "  const { rows, columns, inarow } = environment.configuration;\n",
       "\n",
       "  // Common Dimensions.\n",
       "  const unit = 8;\n",
       "  const minCanvasSize = Math.min(height, width);\n",
       "  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n",
       "  const cellSize = Math.min(\n",
       "    (width - minOffset * 2) / columns,\n",
       "    (height - minOffset * 2) / rows\n",
       "  );\n",
       "  const cellInset = 0.8;\n",
       "  const pieceScale = cellSize / 100;\n",
       "  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n",
       "  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n",
       "\n",
       "  // Canvas Setup.\n",
       "  let canvas = parent.querySelector(&quot;canvas&quot;);\n",
       "  if (!canvas) {\n",
       "    canvas = document.createElement(&quot;canvas&quot;);\n",
       "    parent.appendChild(canvas);\n",
       "\n",
       "    if (interactive) {\n",
       "      canvas.addEventListener(&quot;click&quot;, evt => {\n",
       "        if (!isInteractive()) return;\n",
       "        const rect = evt.target.getBoundingClientRect();\n",
       "        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n",
       "        if (col >= 0 && col < columns) act(col);\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n",
       "\n",
       "  // Character Paths (based on 100x100 tiles).\n",
       "  const kPath = new Path2D(\n",
       "    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n",
       "  );\n",
       "  const goose1Path = new Path2D(\n",
       "    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n",
       "  );\n",
       "  const goose2Path = new Path2D(\n",
       "    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n",
       "  );\n",
       "  const goose3Path = new Path2D(\n",
       "    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n",
       "  );\n",
       "\n",
       "  // Canvas setup and reset.\n",
       "  let c = canvas.getContext(&quot;2d&quot;);\n",
       "  canvas.width = width;\n",
       "  canvas.height = height;\n",
       "  c.fillStyle = &quot;#000B2A&quot;;\n",
       "  c.fillRect(0, 0, canvas.width, canvas.height);\n",
       "\n",
       "  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n",
       "\n",
       "  const getColor = (mark, opacity = 1) => {\n",
       "    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n",
       "    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n",
       "    return &quot;#fff&quot;;\n",
       "  };\n",
       "\n",
       "  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n",
       "    const [row, col] = getRowCol(cell);\n",
       "    c.arc(\n",
       "      xOffset + xFrame * (col * cellSize + cellSize / 2),\n",
       "      yOffset + yFrame * (row * cellSize + cellSize / 2),\n",
       "      (cellInset * cellSize) / 2 - radiusOffset,\n",
       "      2 * Math.PI,\n",
       "      false\n",
       "    );\n",
       "  };\n",
       "\n",
       "  // Render the pieces.\n",
       "  const board = environment.steps[step][0].observation.board;\n",
       "\n",
       "  const drawPiece = mark => {\n",
       "    // Base Styles.\n",
       "    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n",
       "    c.fillStyle = getColor(mark, opacity);\n",
       "    c.strokeStyle = getColor(mark);\n",
       "    c.shadowColor = getColor(mark);\n",
       "    c.shadowBlur = 8 / cellInset;\n",
       "    c.lineWidth = 1 / cellInset;\n",
       "\n",
       "    // Outer circle.\n",
       "    c.save();\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 50, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.lineWidth *= 4;\n",
       "    c.stroke();\n",
       "    c.fill();\n",
       "    c.restore();\n",
       "\n",
       "    // Inner circle.\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 40, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.stroke();\n",
       "\n",
       "    // Kaggle &quot;K&quot;.\n",
       "    if (mark === 1) {\n",
       "      const scale = 0.54;\n",
       "      c.save();\n",
       "      c.translate(23, 23);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(kPath);\n",
       "      c.restore();\n",
       "    }\n",
       "\n",
       "    // Kaggle &quot;Goose&quot;.\n",
       "    if (mark === 2) {\n",
       "      const scale = 0.6;\n",
       "      c.save();\n",
       "      c.translate(24, 28);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(goose1Path);\n",
       "      c.stroke(goose2Path);\n",
       "      c.stroke(goose3Path);\n",
       "      c.beginPath();\n",
       "      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n",
       "      c.closePath();\n",
       "      c.fill();\n",
       "      c.restore();\n",
       "    }\n",
       "  };\n",
       "\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    const [row, col] = getRowCol(i);\n",
       "    if (board[i] === 0) continue;\n",
       "    // Easing In.\n",
       "    let yFrame = Math.min(\n",
       "      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n",
       "      1\n",
       "    );\n",
       "\n",
       "    if (\n",
       "      step > 1 &&\n",
       "      environment.steps[step - 1][0].observation.board[i] === board[i]\n",
       "    ) {\n",
       "      yFrame = 1;\n",
       "    }\n",
       "\n",
       "    c.save();\n",
       "    c.translate(\n",
       "      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n",
       "      yOffset +\n",
       "        yFrame * (cellSize * row) +\n",
       "        (cellSize - cellSize * cellInset) / 2\n",
       "    );\n",
       "    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n",
       "    drawPiece(board[i]);\n",
       "    c.restore();\n",
       "  }\n",
       "\n",
       "  // Background Gradient.\n",
       "  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n",
       "  const bgStyle = c.createRadialGradient(\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    0,\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    bgRadius\n",
       "  );\n",
       "  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n",
       "  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n",
       "\n",
       "  // Render the board overlay.\n",
       "  c.beginPath();\n",
       "  c.rect(0, 0, canvas.width, canvas.height);\n",
       "  c.closePath();\n",
       "  c.shadowBlur = 0;\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    drawCellCircle(i);\n",
       "    c.closePath();\n",
       "  }\n",
       "  c.fillStyle = bgStyle;\n",
       "  c.fill(&quot;evenodd&quot;);\n",
       "\n",
       "  // Render the board overlay cell outlines.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    c.beginPath();\n",
       "    drawCellCircle(i);\n",
       "    c.strokeStyle = &quot;#0361B2&quot;;\n",
       "    c.lineWidth = 1;\n",
       "    c.stroke();\n",
       "    c.closePath();\n",
       "  }\n",
       "\n",
       "  const drawLine = (fromCell, toCell) => {\n",
       "    if (frame < 0.5) return;\n",
       "    const lineFrame = (frame - 0.5) / 0.5;\n",
       "    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n",
       "    const x2 =\n",
       "      x1 +\n",
       "      lineFrame *\n",
       "        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n",
       "    const y1 =\n",
       "      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n",
       "    const y2 =\n",
       "      y1 +\n",
       "      lineFrame *\n",
       "        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n",
       "    c.beginPath();\n",
       "    c.lineCap = &quot;round&quot;;\n",
       "    c.lineWidth = 4;\n",
       "    c.strokeStyle = getColor(board[fromCell]);\n",
       "    c.shadowBlur = 8;\n",
       "    c.shadowColor = getColor(board[fromCell]);\n",
       "    c.moveTo(x1, y1);\n",
       "    c.lineTo(x2, y2);\n",
       "    c.stroke();\n",
       "  };\n",
       "\n",
       "  // Generate a graph of the board.\n",
       "  const getCell = (cell, rowOffset, columnOffset) => {\n",
       "    const row = Math.floor(cell / columns) + rowOffset;\n",
       "    const col = (cell % columns) + columnOffset;\n",
       "    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n",
       "    return col + row * columns;\n",
       "  };\n",
       "  const makeNode = cell => {\n",
       "    const node = { cell, directions: [], value: board[cell] };\n",
       "    for (let r = -1; r <= 1; r++) {\n",
       "      for (let c = -1; c <= 1; c++) {\n",
       "        if (r === 0 && c === 0) continue;\n",
       "        node.directions.push(getCell(cell, r, c));\n",
       "      }\n",
       "    }\n",
       "    return node;\n",
       "  };\n",
       "  const graph = board.map((_, i) => makeNode(i));\n",
       "\n",
       "  // Check for any wins!\n",
       "  const getSequence = (node, direction) => {\n",
       "    const sequence = [node.cell];\n",
       "    while (sequence.length < inarow) {\n",
       "      const next = graph[node.directions[direction]];\n",
       "      if (!next || node.value !== next.value || next.value === 0) return;\n",
       "      node = next;\n",
       "      sequence.push(node.cell);\n",
       "    }\n",
       "    return sequence;\n",
       "  };\n",
       "\n",
       "  // Check all nodes.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    // Check all directions (not the most efficient).\n",
       "    for (let d = 0; d < 8; d++) {\n",
       "      const seq = getSequence(graph[i], d);\n",
       "      if (seq) {\n",
       "        drawLine(seq[0], seq[inarow - 1]);\n",
       "        i = board.length;\n",
       "        break;\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Upgrade the legend.\n",
       "  if (agents.length && (!agents[0].color || !agents[0].image)) {\n",
       "    const getPieceImage = mark => {\n",
       "      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n",
       "      parent.appendChild(pieceCanvas);\n",
       "      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n",
       "      pieceCanvas.width = 100;\n",
       "      pieceCanvas.height = 100;\n",
       "      c = pieceCanvas.getContext(&quot;2d&quot;);\n",
       "      c.translate(10, 10);\n",
       "      c.scale(0.8, 0.8);\n",
       "      drawPiece(mark);\n",
       "      const dataUrl = pieceCanvas.toDataURL();\n",
       "      parent.removeChild(pieceCanvas);\n",
       "      return dataUrl;\n",
       "    };\n",
       "\n",
       "    agents.forEach(agent => {\n",
       "      agent.color = getColor(agent.index + 1);\n",
       "      agent.image = getPieceImage(agent.index + 1);\n",
       "    });\n",
       "    update({ agents });\n",
       "  }\n",
       "};\n",
       "\n",
       "\n",
       "    \n",
       "    </script>\n",
       "    <script>\n",
       "      const h = htm.bind(preact.h);\n",
       "      const { useContext, useEffect, useRef, useState } = preactHooks;\n",
       "      const styled = window.styled.default;\n",
       "\n",
       "      const Context = preact.createContext({});\n",
       "\n",
       "      const Loading = styled.div`\n",
       "        animation: rotate360 1.1s infinite linear;\n",
       "        border: 8px solid rgba(255, 255, 255, 0.2);\n",
       "        border-left-color: #0cb1ed;\n",
       "        border-radius: 50%;\n",
       "        height: 40px;\n",
       "        position: relative;\n",
       "        transform: translateZ(0);\n",
       "        width: 40px;\n",
       "\n",
       "        @keyframes rotate360 {\n",
       "          0% {\n",
       "            transform: rotate(0deg);\n",
       "          }\n",
       "          100% {\n",
       "            transform: rotate(360deg);\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Logo = styled(\n",
       "        (props) => h`\n",
       "        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n",
       "          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n",
       "            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n",
       "              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n",
       "              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n",
       "              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n",
       "              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n",
       "              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n",
       "              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n",
       "            </g>\n",
       "          </svg>\n",
       "        </a>\n",
       "      `\n",
       "      )`\n",
       "        display: inline-flex;\n",
       "      `;\n",
       "\n",
       "      const Header = styled((props) => {\n",
       "        const { environment } = useContext(Context);\n",
       "\n",
       "        return h`<div className=${props.className} >\n",
       "          <${Logo} />\n",
       "          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n",
       "          ${environment.title}\n",
       "        </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-bottom: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        color: #fff;\n",
       "        display: flex;\n",
       "        flex: 0 0 36px;\n",
       "        font-size: 14px;\n",
       "        justify-content: space-between;\n",
       "        padding: 0 8px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Renderer = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { animate, debug, playing, renderer, speed } = context;\n",
       "        const ref = preact.createRef();\n",
       "\n",
       "        useEffect(async () => {\n",
       "          if (!ref.current) return;\n",
       "\n",
       "          const renderFrame = async (start, step, lastFrame) => {\n",
       "            if (step !== context.step) return;\n",
       "            if (lastFrame === 1) {\n",
       "              if (!animate) return;\n",
       "              start = Date.now();\n",
       "            }\n",
       "            const frame =\n",
       "              playing || animate\n",
       "                ? Math.min((Date.now() - start) / speed, 1)\n",
       "                : 1;\n",
       "            try {\n",
       "              if (debug) console.time(&quot;render&quot;);\n",
       "              await renderer({\n",
       "                ...context,\n",
       "                frame,\n",
       "                height: ref.current.clientHeight,\n",
       "                hooks: preactHooks,\n",
       "                parent: ref.current,\n",
       "                preact,\n",
       "                styled,\n",
       "                width: ref.current.clientWidth,\n",
       "              });\n",
       "            } catch (error) {\n",
       "              if (debug) console.error(error);\n",
       "              console.log({ ...context, frame, error });\n",
       "            } finally {\n",
       "              if (debug) console.timeEnd(&quot;render&quot;);\n",
       "            }\n",
       "            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n",
       "          };\n",
       "\n",
       "          await renderFrame(Date.now(), context.step);\n",
       "        }, [ref.current, context.step, context.renderer]);\n",
       "\n",
       "        return h`<div className=${props.className} ref=${ref} />`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        height: 100%;\n",
       "        left: 0;\n",
       "        justify-content: center;\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Processing = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        const text = processing === true ? &quot;Processing...&quot; : processing;\n",
       "        return h`<div className=${props.className}>${text}</div>`;\n",
       "      })`\n",
       "        bottom: 0;\n",
       "        color: #fff;\n",
       "        font-size: 12px;\n",
       "        left: 0;\n",
       "        line-height: 24px;\n",
       "        position: absolute;\n",
       "        text-align: center;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Viewer = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        return h`<div className=${props.className}>\n",
       "          <${Renderer} />\n",
       "          ${processing && h`<${Processing} />`}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        background-image: radial-gradient(\n",
       "          circle closest-side,\n",
       "          #000b49,\n",
       "          #000b2a\n",
       "        );\n",
       "        display: flex;\n",
       "        flex: 1;\n",
       "        overflow: hidden;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      // Partitions the elements of arr into subarrays of max length num.\n",
       "      const groupIntoSets = (arr, num) => {\n",
       "        const sets = [];\n",
       "        arr.forEach(a => {\n",
       "          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n",
       "            sets.push([]);\n",
       "          }\n",
       "          sets[sets.length - 1].push(a);\n",
       "        });\n",
       "        return sets;\n",
       "      }\n",
       "\n",
       "      // Expects `width` input prop to set proper max-width for agent name span.\n",
       "      const Legend = styled((props) => {\n",
       "        const { agents, legend } = useContext(Context);\n",
       "\n",
       "        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n",
       "\n",
       "        return h`<div className=${props.className}>\n",
       "          ${agentPairs.map(agentList =>\n",
       "            h`<ul>\n",
       "                ${agentList.map(a =>\n",
       "                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n",
       "                      ${a.image && h`<img src=${a.image} />`}\n",
       "                      <span>${a.name}</span>\n",
       "                    </li>`\n",
       "                )}\n",
       "              </ul>`)}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        font-family: sans-serif;\n",
       "        font-size: 14px;\n",
       "        height: 48px;\n",
       "        width: 100%;\n",
       "\n",
       "        ul {\n",
       "          align-items: center;\n",
       "          display: flex;\n",
       "          flex-direction: row;\n",
       "          justify-content: center;\n",
       "        }\n",
       "\n",
       "        li {\n",
       "          align-items: center;\n",
       "          display: inline-flex;\n",
       "          transition: color 1s;\n",
       "        }\n",
       "\n",
       "        span {\n",
       "          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n",
       "          overflow: hidden;\n",
       "          text-overflow: ellipsis;\n",
       "          white-space: nowrap;\n",
       "        }\n",
       "\n",
       "        img {\n",
       "          height: 24px;\n",
       "          margin-left: 4px;\n",
       "          margin-right: 4px;\n",
       "          width: 24px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepInput = styled.input.attrs({\n",
       "        type: &quot;range&quot;,\n",
       "      })`\n",
       "        appearance: none;\n",
       "        background: rgba(255, 255, 255, 0.15);\n",
       "        border-radius: 2px;\n",
       "        display: block;\n",
       "        flex: 1;\n",
       "        height: 4px;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "        width: 100%;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "\n",
       "        &::-webkit-slider-thumb {\n",
       "          appearance: none;\n",
       "          background: #1ebeff;\n",
       "          border-radius: 100%;\n",
       "          cursor: pointer;\n",
       "          height: 12px;\n",
       "          margin: 0;\n",
       "          position: relative;\n",
       "          width: 12px;\n",
       "\n",
       "          &::after {\n",
       "            content: &quot;&quot;;\n",
       "            position: absolute;\n",
       "            top: 0px;\n",
       "            left: 0px;\n",
       "            width: 200px;\n",
       "            height: 8px;\n",
       "            background: green;\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const PlayButton = styled.button`\n",
       "        align-items: center;\n",
       "        background: none;\n",
       "        border: none;\n",
       "        color: white;\n",
       "        cursor: pointer;\n",
       "        display: flex;\n",
       "        flex: 0 0 56px;\n",
       "        font-size: 20px;\n",
       "        height: 40px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepCount = styled.span`\n",
       "        align-items: center;\n",
       "        color: white;\n",
       "        display: flex;\n",
       "        font-size: 14px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        padding: 0 16px;\n",
       "        pointer-events: none;\n",
       "      `;\n",
       "\n",
       "      const Controls = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "        const value = step + 1;\n",
       "        const onClick = () => (playing ? pause() : play());\n",
       "        const onInput = (e) => {\n",
       "          pause();\n",
       "          setStep(parseInt(e.target.value) - 1);\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n",
       "          playing\n",
       "            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "        }</svg><//>\n",
       "            <${StepInput} min=&quot;1&quot; max=${\n",
       "          environment.steps.length\n",
       "        } value=&quot;${value}&quot; onInput=${onInput} />\n",
       "            <${StepCount}>${value} / ${environment.steps.length}<//>\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-top: 4px solid #212121;\n",
       "        display: flex;\n",
       "        flex: 0 0 44px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Info = styled((props) => {\n",
       "        const {\n",
       "          environment,\n",
       "          playing,\n",
       "          step,\n",
       "          speed,\n",
       "          animate,\n",
       "          header,\n",
       "          controls,\n",
       "          settings,\n",
       "        } = useContext(Context);\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            info:\n",
       "            step(${step}),\n",
       "            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n",
       "            speed(${speed}),\n",
       "            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n",
       "          </div>`;\n",
       "      })`\n",
       "        color: #888;\n",
       "        font-family: monospace;\n",
       "        font-size: 12px;\n",
       "      `;\n",
       "\n",
       "      const Settings = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${Info} />\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        background: #fff;\n",
       "        border-top: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        padding: 20px;\n",
       "        width: 100%;\n",
       "\n",
       "        h1 {\n",
       "          font-size: 20px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Player = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { agents, controls, header, legend, loading, settings, width } = context;\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            ${loading && h`<${Loading} />`}\n",
       "            ${!loading && header && h`<${Header} />`}\n",
       "            ${!loading && h`<${Viewer} />`}\n",
       "            ${!loading && legend && h`<${Legend} width=${width}/>`}\n",
       "            ${!loading && controls && h`<${Controls} />`}\n",
       "            ${!loading && settings && h`<${Settings} />`}\n",
       "          </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        background: #212121;\n",
       "        border: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "        height: 100%;\n",
       "        justify-content: center;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const App = () => {\n",
       "        const renderCountRef = useRef(0);\n",
       "        const [_, setRenderCount] = useState(0);\n",
       "\n",
       "        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n",
       "        const speeds = [\n",
       "          0,\n",
       "          3000,\n",
       "          1000,\n",
       "          500,\n",
       "          333, // Default\n",
       "          200,\n",
       "          100,\n",
       "          50,\n",
       "          25,\n",
       "          10,\n",
       "        ];\n",
       "\n",
       "        const contextRef = useRef({\n",
       "          animate: false,\n",
       "          agents: [],\n",
       "          controls: false,\n",
       "          debug: false,\n",
       "          environment: { steps: [], info: {} },\n",
       "          header: window.innerHeight >= 600,\n",
       "          height: window.innerHeight,\n",
       "          interactive: false,\n",
       "          legend: true,\n",
       "          loading: false,\n",
       "          playing: false,\n",
       "          processing: false,\n",
       "          renderer: () => &quot;DNE&quot;,\n",
       "          settings: false,\n",
       "          speed: speeds[4],\n",
       "          step: 0,\n",
       "          width: window.innerWidth,\n",
       "        });\n",
       "\n",
       "        // Context helpers.\n",
       "        const rerender = (contextRef.current.rerender = () =>\n",
       "          setRenderCount((renderCountRef.current += 1)));\n",
       "        const setStep = (contextRef.current.setStep = (newStep) => {\n",
       "          contextRef.current.step = newStep;\n",
       "          rerender();\n",
       "        });\n",
       "        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n",
       "          contextRef.current.playing = playing;\n",
       "          rerender();\n",
       "        });\n",
       "        const pause = (contextRef.current.pause = () => setPlaying(false));\n",
       "\n",
       "        const playNext = () => {\n",
       "          const context = contextRef.current;\n",
       "\n",
       "          if (\n",
       "            context.playing &&\n",
       "            context.step < context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(context.step + 1);\n",
       "            play(true);\n",
       "          } else {\n",
       "            pause();\n",
       "          }\n",
       "        };\n",
       "\n",
       "        const play = (contextRef.current.play = (continuing) => {\n",
       "          const context = contextRef.current;\n",
       "          if (context.playing && !continuing) return;\n",
       "          if (!context.playing) setPlaying(true);\n",
       "          if (\n",
       "            !continuing &&\n",
       "            context.step === context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(0);\n",
       "          }\n",
       "          setTimeout(playNext, context.speed);\n",
       "        });\n",
       "\n",
       "        const updateContext = (o) => {\n",
       "          const context = contextRef.current;\n",
       "          Object.assign(context, o, {\n",
       "            environment: { ...context.environment, ...(o.environment || {}) },\n",
       "          });\n",
       "          rerender();\n",
       "        };\n",
       "\n",
       "        // First time setup.\n",
       "        useEffect(() => {\n",
       "          // Timeout is used to ensure useEffect renders once.\n",
       "          setTimeout(() => {\n",
       "            // Initialize context with window.kaggle.\n",
       "            updateContext(window.kaggle || {});\n",
       "\n",
       "            if (window.kaggle.playing) {\n",
       "                play(true);\n",
       "            }\n",
       "\n",
       "            // Listen for messages received to update the context.\n",
       "            window.addEventListener(\n",
       "              &quot;message&quot;,\n",
       "              (event) => {\n",
       "                // Ensure the environment names match before updating.\n",
       "                try {\n",
       "                  if (\n",
       "                    event.data.environment.name ==\n",
       "                    contextRef.current.environment.name\n",
       "                  ) {\n",
       "                    updateContext(event.data);\n",
       "                  }\n",
       "                } catch {}\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "            // Listen for keyboard commands.\n",
       "            window.addEventListener(\n",
       "              &quot;keydown&quot;,\n",
       "              (event) => {\n",
       "                const {\n",
       "                  interactive,\n",
       "                  isInteractive,\n",
       "                  playing,\n",
       "                  step,\n",
       "                  environment,\n",
       "                } = contextRef.current;\n",
       "                const key = event.keyCode;\n",
       "                const zero_key = 48\n",
       "                const nine_key = 57\n",
       "                if (\n",
       "                  interactive ||\n",
       "                  isInteractive() ||\n",
       "                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n",
       "                )\n",
       "                  return;\n",
       "\n",
       "                if (key === 32) {\n",
       "                  playing ? pause() : play();\n",
       "                } else if (key === 39) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step < environment.steps.length - 1) setStep(step + 1);\n",
       "                  rerender();\n",
       "                } else if (key === 37) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step > 0) setStep(step - 1);\n",
       "                  rerender();\n",
       "                } else if (key >= zero_key && key <= nine_key) {\n",
       "                  contextRef.current.speed = speeds[key - zero_key];\n",
       "                }\n",
       "                event.preventDefault();\n",
       "                return false;\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "          }, 1);\n",
       "        }, []);\n",
       "\n",
       "        if (contextRef.current.debug) {\n",
       "          console.log(&quot;context&quot;, contextRef.current);\n",
       "        }\n",
       "\n",
       "        // Ability to update context.\n",
       "        contextRef.current.update = updateContext;\n",
       "\n",
       "        // Ability to communicate with ipython.\n",
       "        const execute = (contextRef.current.execute = (source) =>\n",
       "          new Promise((resolve, reject) => {\n",
       "            try {\n",
       "              window.parent.IPython.notebook.kernel.execute(source, {\n",
       "                iopub: {\n",
       "                  output: (resp) => {\n",
       "                    const type = resp.msg_type;\n",
       "                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n",
       "                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n",
       "                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n",
       "                  },\n",
       "                },\n",
       "              });\n",
       "            } catch (e) {\n",
       "              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n",
       "            }\n",
       "          }));\n",
       "\n",
       "        // Ability to return an action from an interactive session.\n",
       "        contextRef.current.act = (action) => {\n",
       "          const id = contextRef.current.environment.id;\n",
       "          updateContext({ processing: true });\n",
       "          execute(`\n",
       "            import json\n",
       "            from kaggle_environments import interactives\n",
       "            if &quot;${id}&quot; in interactives:\n",
       "                action = json.loads('${JSON.stringify(action)}')\n",
       "                env, trainer = interactives[&quot;${id}&quot;]\n",
       "                trainer.step(action)\n",
       "                print(json.dumps(env.steps))`)\n",
       "            .then((resp) => {\n",
       "              try {\n",
       "                updateContext({\n",
       "                  processing: false,\n",
       "                  environment: { steps: JSON.parse(resp) },\n",
       "                });\n",
       "                play();\n",
       "              } catch (e) {\n",
       "                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n",
       "                console.error(resp, e);\n",
       "              }\n",
       "            })\n",
       "            .catch((e) => console.error(e));\n",
       "        };\n",
       "\n",
       "        // Check if currently interactive.\n",
       "        contextRef.current.isInteractive = () => {\n",
       "          const context = contextRef.current;\n",
       "          const steps = context.environment.steps;\n",
       "          return (\n",
       "            context.interactive &&\n",
       "            !context.processing &&\n",
       "            context.step === steps.length - 1 &&\n",
       "            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n",
       "          );\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <${Context.Provider} value=${contextRef.current}>\n",
       "            <${Player} />\n",
       "          <//>`;\n",
       "      };\n",
       "\n",
       "      preact.render(h`<${App} />`, document.body);\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n",
       "\" width=\"500\" height=\"450\" frameborder=\"0\"></iframe> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "def agent_DeepQL(observation, configuration):\n",
    "    \n",
    "    ################################## START NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    # Given a board (as a list) and a column, returns the index of the first row available\n",
    "    def first_row_avail(board, col, num_rows, num_cols):\n",
    "        \"\"\"\n",
    "        Given a board (as a list) and a column, returns the index of the first row available\n",
    "        \"\"\"\n",
    "        index = -1\n",
    "        for i in range(num_rows): # from top to bottom\n",
    "            board_index = col + (i * num_cols)\n",
    "            if board[board_index] == 0:\n",
    "                index = board_index\n",
    "            else:\n",
    "                return index\n",
    "        return index\n",
    "            \n",
    "    def convert_for_CNN(board, num_rows, num_cols):\n",
    "        \"\"\"\n",
    "        Converts the board (list) into a matrix of shape (rows, cols, 1), so that a CNN can work with it.\n",
    "        Player 2 checkers are replaced to -1\n",
    "        \"\"\"\n",
    "        board = [np.float32(-1) if x==2 else np.float32(x) for x in board]\n",
    "        return np.reshape(board, [1, num_rows, num_cols, 1])\n",
    "    \n",
    "    ################################## END NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    print(configuration) # {rows: 5, columns: 5, inarow: 3}\n",
    "    # Number of rows on the board\n",
    "    num_rows = configuration.rows\n",
    "    # Number of columns on the board\n",
    "    num_cols = configuration.columns\n",
    "    # Number of checkers \"in a row\" needed to win\n",
    "    in_a_row = configuration.inarow\n",
    "    print(observation) # {board: [...], mark: 1}\n",
    "    # The current serialized board (rows x columns) - array [rows x columns] with top row first\n",
    "    board = observation.board\n",
    "    # Which player the agent is playing as (1 or 2).\n",
    "    mark = observation.mark\n",
    "    # If the board is empty, put the first checker in the middle column\n",
    "    if max(board)==0:\n",
    "        return round(num_cols/2)\n",
    "    \n",
    "    # Load the model from file\n",
    "    model = tf.keras.models.load_model('model_DQN.h5')\n",
    "    # List of available columns\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # Convert the list board into a two-dimensional array\n",
    "    board_matrix = convert_for_CNN(board, num_rows, num_cols) #np.reshape(board, [1, num_rows*num_cols])\n",
    "    # Evaluate the target board with a checker in each available column, and get the best one\n",
    "    reward_list = [] # List of rewards of each movement\n",
    "    reward_indexes_list = [] # List of indexes of each reward\n",
    "    # If the agent is player 2, get the reverse board, and act as player 1\n",
    "    if mark == 2:\n",
    "        board_matrix = board_matrix * (-1) + 0\n",
    "    q_values_list = model(board_matrix).numpy()[0]\n",
    "    for target_col in available_cols:\n",
    "        reward = q_values_list[target_col]\n",
    "        reward_list.append(reward)\n",
    "        reward_indexes_list.append(target_col)\n",
    "        print(\"Column:\"+str(target_col)+\" Reward:\"+str(reward))\n",
    "    # Always get the highest reward. If the agent is player 2 we are working with the reverse board.\n",
    "    max_reward = max(reward_list)\n",
    "    # Return the column with the highest reward. If several actions have the same reward, select randomly one of them\n",
    "    max_rewards_index = [reward_indexes_list[i] for i, x in enumerate(reward_list) if x == max_reward]\n",
    "    # Check if max_rewards_index is empty (every value is zero or almost 1). In this case take a column randomly\n",
    "    if max_rewards_index == []:\n",
    "        max_rewards_index = available_cols\n",
    "    return int(np.random.choice(max_rewards_index))\n",
    "\n",
    "# Run an episode using the agent above vs the negamax agent.\n",
    "#env.run([agent_DeepQL, \"random\"])\n",
    "env.run([agent_DeepQL, \"negamax\"])\n",
    "#env.run([\"negamax\", agent_DeepQL])\n",
    "#env.run([\"random\", agent_DeepQL])\n",
    "env.render(mode=\"ipython\", width=500, height=450)\n",
    "#env.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'evaluate' returns a list of lists (one list per episode).  \n",
    "These lists per episode contains two elements (one per player), and these elements have these possible values:\n",
    "- 1: This player wins\n",
    "- -1: This player loses\n",
    "- 0: Draw (I guess there will be a value for draw)  \n",
    "Example:  \n",
    "[[1, -1], # First episode: Player 1 won  \n",
    " [1, -1], # Second episode: Player 1 won  \n",
    " [1, -1], # Third episode: Player 1 won  \n",
    " [1, -1], # ...  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1],  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "help(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1],\n",
       " [-1, 1]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "#evaluate(\"connectx\", [agent_DeepQL, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "#evaluate(\"connectx\", [\"random\", agent_MQL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "evaluate(\"connectx\", [agent_DeepQL, \"negamax\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our agent against random and negamax agent.  \n",
    "- The closer the value is to 1 the better is player 1 agent (wins more times than loses)\n",
    "- The closer the value is to -1 the worse is player 1 agent (loses more times than wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Agent vs Random Agent: 0.0\n",
      "My Agent vs Negamax Agent: -1.0\n",
      "Random Agent vs My Agent: 0.2\n",
      "Negamax Agent vs My Agent: 1.0\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "\n",
    "# Run multiple episodes to estimate its performance.\n",
    "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [agent_DeepQL, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [agent_DeepQL, \"negamax\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Random Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", agent_DeepQL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Negamax Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", agent_DeepQL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing random vs negamax\n",
      "-0.8\n",
      "Playing random vs agent_MC\n",
      "-0.6\n",
      "Playing random vs agent_QL\n",
      "-0.6\n",
      "Playing random vs agent_SA\n",
      "-0.6\n",
      "Playing random vs agent_ESA\n",
      "0.0\n",
      "Playing random vs agent_MQL\n",
      "-1.0\n",
      "Playing random vs agent_DQL\n",
      "0.0\n",
      "Playing random vs agent_DeepQL\n",
      "-0.4\n",
      "Playing negamax vs random\n",
      "1.0\n",
      "Playing negamax vs agent_MC\n",
      "0.8\n",
      "Playing negamax vs agent_QL\n",
      "1.0\n",
      "Playing negamax vs agent_SA\n",
      "1.0\n",
      "Playing negamax vs agent_ESA\n",
      "1.0\n",
      "Playing negamax vs agent_MQL\n",
      "0.0\n",
      "Playing negamax vs agent_DQL\n",
      "1.0\n",
      "Playing negamax vs agent_DeepQL\n",
      "1.0\n",
      "Playing agent_MC vs random\n",
      "0.6\n",
      "Playing agent_MC vs negamax\n",
      "1.0\n",
      "Playing agent_MC vs agent_QL\n",
      "1.0\n",
      "Playing agent_MC vs agent_SA\n",
      "0.4\n",
      "Playing agent_MC vs agent_ESA\n",
      "1.0\n",
      "Playing agent_MC vs agent_MQL\n",
      "1.0\n",
      "Playing agent_MC vs agent_DQL\n",
      "0.8\n",
      "Playing agent_MC vs agent_DeepQL\n",
      "-1.0\n",
      "Playing agent_QL vs random\n",
      "0.8\n",
      "Playing agent_QL vs negamax\n",
      "1.0\n",
      "Playing agent_QL vs agent_MC\n",
      "1.0\n",
      "Playing agent_QL vs agent_SA\n",
      "0.8\n",
      "Playing agent_QL vs agent_ESA\n",
      "0.8\n",
      "Playing agent_QL vs agent_MQL\n",
      "1.0\n",
      "Playing agent_QL vs agent_DQL\n",
      "0.6\n",
      "Playing agent_QL vs agent_DeepQL\n",
      "-0.4\n",
      "Playing agent_SA vs random\n",
      "0.8\n",
      "Playing agent_SA vs negamax\n",
      "0.6\n",
      "Playing agent_SA vs agent_MC\n",
      "0.8\n",
      "Playing agent_SA vs agent_QL\n",
      "0.4\n",
      "Playing agent_SA vs agent_ESA\n",
      "0.6\n",
      "Playing agent_SA vs agent_MQL\n",
      "0.6\n",
      "Playing agent_SA vs agent_DQL\n",
      "0.0\n",
      "Playing agent_SA vs agent_DeepQL\n",
      "-0.4\n",
      "Playing agent_ESA vs random\n",
      "0.6\n",
      "Playing agent_ESA vs negamax\n",
      "0.8\n",
      "Playing agent_ESA vs agent_MC\n",
      "0.4\n",
      "Playing agent_ESA vs agent_QL\n",
      "0.4\n",
      "Playing agent_ESA vs agent_SA\n",
      "0.4\n",
      "Playing agent_ESA vs agent_MQL\n",
      "0.8\n",
      "Playing agent_ESA vs agent_DQL\n",
      "0.8\n",
      "Playing agent_ESA vs agent_DeepQL\n",
      "-0.8\n",
      "Playing agent_MQL vs random\n",
      "0.4\n",
      "Playing agent_MQL vs negamax\n",
      "1.0\n",
      "Playing agent_MQL vs agent_MC\n",
      "1.0\n",
      "Playing agent_MQL vs agent_QL\n",
      "1.0\n",
      "Playing agent_MQL vs agent_SA\n",
      "1.0\n",
      "Playing agent_MQL vs agent_ESA\n",
      "1.0\n",
      "Playing agent_MQL vs agent_DQL\n",
      "0.4\n",
      "Playing agent_MQL vs agent_DeepQL\n",
      "-0.4\n",
      "Playing agent_DQL vs random\n",
      "0.4\n",
      "Playing agent_DQL vs negamax\n",
      "0.8\n",
      "Playing agent_DQL vs agent_MC\n",
      "-1.0\n",
      "Playing agent_DQL vs agent_QL\n",
      "0.4\n",
      "Playing agent_DQL vs agent_SA\n",
      "0.6\n",
      "Playing agent_DQL vs agent_ESA\n",
      "1.0\n",
      "Playing agent_DQL vs agent_MQL\n",
      "1.0\n",
      "Playing agent_DQL vs agent_DeepQL\n",
      "-0.4\n",
      "Playing agent_DeepQL vs random\n",
      "0.8\n",
      "Playing agent_DeepQL vs negamax\n",
      "0.2\n",
      "Playing agent_DeepQL vs agent_MC\n",
      "-1.0\n",
      "Playing agent_DeepQL vs agent_QL\n",
      "-0.4\n",
      "Playing agent_DeepQL vs agent_SA\n",
      "1.0\n",
      "Playing agent_DeepQL vs agent_ESA\n",
      "-0.4\n",
      "Playing agent_DeepQL vs agent_MQL\n",
      "-1.0\n",
      "Playing agent_DeepQL vs agent_DQL\n",
      "1.0\n",
      "{'random': -9.4, 'negamax': 2.2, 'agent_MC': 3.4, 'agent_QL': 2.3999999999999995, 'agent_SA': -1.1999999999999997, 'agent_ESA': -1.5999999999999996, 'agent_MQL': 2.9999999999999996, 'agent_DQL': -1.8000000000000003, 'agent_DeepQL': 3.0}\n",
      "Position   Agent        Points\n",
      "1          agent_MC       3.40\n",
      "2          agent_DeepQL   3.00\n",
      "3          agent_MQL      3.00\n",
      "4          agent_QL       2.40\n",
      "5          negamax        2.20\n",
      "6          agent_SA      -1.20\n",
      "7          agent_ESA     -1.60\n",
      "8          agent_DQL     -1.80\n",
      "9          random        -9.40\n"
     ]
    }
   ],
   "source": [
    "# https://docs.python.org/3/library/itertools.html\n",
    "from itertools import permutations\n",
    "from kaggle_environments import evaluate\n",
    "\n",
    "\n",
    "def name(obj):\n",
    "    \"\"\"\n",
    "    If obj is a function, gets the name instead of something like <function agent_MC at 0x0000023B01C3C950>\n",
    "    \"\"\"\n",
    "    if callable(obj):\n",
    "        return getattr(obj, \"__name__\")\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "    \n",
    "conf={\"rows\": 5, \"columns\": 5, \"inarow\": 3}\n",
    "\n",
    "list_agents = [\"random\", \"negamax\", agent_MC, agent_QL, agent_SA, agent_ESA, agent_MQL, agent_DQL, agent_DeepQL]\n",
    "list_matches = list(permutations(list_agents, 2))\n",
    "results = {}\n",
    "\n",
    "for players in list_matches:\n",
    "    print(\"Playing\", name(players[0]), \"vs\", name(players[1]))\n",
    "    list_players = list(players)\n",
    "    result = mean_reward(evaluate(\"connectx\", agents=list_players, configuration=conf, num_episodes=10))\n",
    "    print(result)\n",
    "    results[name(players[0])] = results.get(name(players[0]), 0) + result\n",
    "    results[name(players[1])] = results.get(name(players[1]), 0) - result\n",
    "\n",
    "print(results)\n",
    "\n",
    "# Display results ordered by the points obtained\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"{:<10} {:<12} {:<6}\".format('Position','Agent','Points')) # https://docs.python.org/3/library/string.html#formatspec\n",
    "for i, result in enumerate(sorted_results):\n",
    "    print(\"{:<10} {:<12} {:>6.2f}\".format(i+1, result[0], result[1]))\n",
    "\n",
    "# Position   Agent      Points\n",
    "# 1          agent_MC     5.20\n",
    "# 2          agent_MQL    4.00\n",
    "# 3          negamax      3.60\n",
    "# 4          agent_QL     1.60\n",
    "# 5          agent_DeepQL 0.40\n",
    "# 6          agent_SA    -0.60\n",
    "# 7          agent_ESA   -1.00\n",
    "# 8          agent_DQL   -2.20\n",
    "# 9          random     -11.00\n",
    "\n",
    "#Position   Agent        Points\n",
    "#1          agent_MC       3.40\n",
    "#2          agent_DeepQL   3.00\n",
    "#3          agent_MQL      3.00\n",
    "#4          agent_QL       2.40\n",
    "#5          negamax        2.20\n",
    "#6          agent_SA      -1.20\n",
    "#7          agent_ESA     -1.60\n",
    "#8          agent_DQL     -1.80\n",
    "#9          random        -9.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Temporal Differece Double Deep Q-learning  \n",
    "Unlike regular Deep Q-learning, in this algorithm we use 2 neural networks. One for selecting the next best action (online network), and the other one for evaluating it to update Q-values (target network). With this new Q-value we train the first neural network (the one used for selecting). Target network is periodically updated with the weights of online network. With 2 neural networks we try to avoid overestimation and underestimation.  \n",
    "Off Policy. Learn policy $ \\pi $ (greedy) using policy $ \\mu $ (epsilon-greedy: greedy or random depending on a random value)  \n",
    "Based on Hasselt's paper: https://arxiv.org/abs/1509.06461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 657\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "rewards_list_double_deep_ql = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 870\n"
     ]
    }
   ],
   "source": [
    "# RED v2\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "num_cols = 5\n",
    "num_rows = 5\n",
    "action_size = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=3, activation='relu', input_shape=(num_rows, num_cols, 1))) # padding=valid # Input: (5, 5, 1) Output (3, 3, 8)\n",
    "model.add(Flatten()) # Input: (3, 3, 8) Output: (72)\n",
    "model.add(Dense(8, activation='relu')) # Input: (72) Output: (8)\n",
    "model.add(Dense(action_size, activation=\"linear\")) # Input: (8) Output: (5)\n",
    "model.compile(Adam(lr=0.01), loss='mse')\n",
    "\n",
    "model_target = Sequential()\n",
    "model_target.add(Conv2D(8, kernel_size=3, activation='relu', input_shape=(num_rows, num_cols, 1))) # padding=valid # Input: (5, 5, 1) Output (3, 3, 8)\n",
    "model_target.add(Flatten()) # Input: (3, 3, 8) Output: (72)\n",
    "model_target.add(Dense(8, activation='relu')) # Input: (72) Output: (8)\n",
    "model_target.add(Dense(action_size, activation=\"linear\")) # Input: (8) Output: (5)\n",
    "model_target.compile(Adam(lr=0.01), loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"387pt\" viewBox=\"0.00 0.00 323.00 387.00\" width=\"323pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 383)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-383 319,-383 319,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1943537643928 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1943537643928</title>\n",
       "<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 315,-378.5 315,-332.5 0,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"88\" y=\"-351.8\">conv2d_1_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"176,-332.5 176,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"204\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"176,-355.5 232,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"204\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"232,-332.5 232,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"273.5\" y=\"-363.3\">[(?, 5, 5, 1)]</text>\n",
       "<polyline fill=\"none\" points=\"232,-355.5 315,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"273.5\" y=\"-340.3\">[(?, 5, 5, 1)]</text>\n",
       "</g>\n",
       "<!-- 1943537644992 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1943537644992</title>\n",
       "<polygon fill=\"none\" points=\"29,-249.5 29,-295.5 286,-295.5 286,-249.5 29,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.5\" y=\"-268.8\">conv2d_1: Conv2D</text>\n",
       "<polyline fill=\"none\" points=\"156,-249.5 156,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"184\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"156,-272.5 212,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"184\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"212,-249.5 212,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-280.3\">(?, 5, 5, 1)</text>\n",
       "<polyline fill=\"none\" points=\"212,-272.5 286,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-257.3\">(?, 3, 3, 8)</text>\n",
       "</g>\n",
       "<!-- 1943537643928&#45;&gt;1943537644992 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1943537643928-&gt;1943537644992</title>\n",
       "<path d=\"M157.5,-332.366C157.5,-324.152 157.5,-314.658 157.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"161,-305.607 157.5,-295.607 154,-305.607 161,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1943537644600 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1943537644600</title>\n",
       "<polygon fill=\"none\" points=\"38,-166.5 38,-212.5 277,-212.5 277,-166.5 38,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.5\" y=\"-185.8\">flatten_1: Flatten</text>\n",
       "<polyline fill=\"none\" points=\"147,-166.5 147,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"147,-189.5 203,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"203,-166.5 203,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240\" y=\"-197.3\">(?, 3, 3, 8)</text>\n",
       "<polyline fill=\"none\" points=\"203,-189.5 277,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240\" y=\"-174.3\">(?, 72)</text>\n",
       "</g>\n",
       "<!-- 1943537644992&#45;&gt;1943537644600 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1943537644992-&gt;1943537644600</title>\n",
       "<path d=\"M157.5,-249.366C157.5,-241.152 157.5,-231.658 157.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"161,-222.607 157.5,-212.607 154,-222.607 161,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1943537641456 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1943537641456</title>\n",
       "<polygon fill=\"none\" points=\"51,-83.5 51,-129.5 264,-129.5 264,-83.5 51,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103\" y=\"-102.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"155,-83.5 155,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"155,-106.5 211,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"211,-83.5 211,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237.5\" y=\"-114.3\">(?, 72)</text>\n",
       "<polyline fill=\"none\" points=\"211,-106.5 264,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237.5\" y=\"-91.3\">(?, 8)</text>\n",
       "</g>\n",
       "<!-- 1943537644600&#45;&gt;1943537641456 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1943537644600-&gt;1943537641456</title>\n",
       "<path d=\"M157.5,-166.366C157.5,-158.152 157.5,-148.658 157.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"161,-139.607 157.5,-129.607 154,-139.607 161,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1943537640896 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1943537640896</title>\n",
       "<polygon fill=\"none\" points=\"54.5,-0.5 54.5,-46.5 260.5,-46.5 260.5,-0.5 54.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106.5\" y=\"-19.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"158.5,-0.5 158.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"158.5,-23.5 214.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"214.5,-0.5 214.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237.5\" y=\"-31.3\">(?, 8)</text>\n",
       "<polyline fill=\"none\" points=\"214.5,-23.5 260.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"237.5\" y=\"-8.3\">(?, 5)</text>\n",
       "</g>\n",
       "<!-- 1943537641456&#45;&gt;1943537640896 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1943537641456-&gt;1943537640896</title>\n",
       "<path d=\"M157.5,-83.3664C157.5,-75.1516 157.5,-65.6579 157.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"161,-56.6068 157.5,-46.6068 154,-56.6069 161,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import SVG, display, clear_output\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "display(SVG(model_to_dot(model, show_shapes=True, dpi=72).create(prog='dot', format='svg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 8)           80        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 584       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 45        \n",
      "=================================================================\n",
      "Total params: 709\n",
      "Trainable params: 709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model_DDQN.h5')\n",
    "model_target = tf.keras.models.load_model('model_target_DDQN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/reading-and-writing-lists-to-a-file-in-python/\n",
    "rewards_list_double_deep_ql = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('rewards_list_double_deep_ql.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        rewards_list_double_deep_ql.append(int(currentPlace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewards_list_double_deep_ql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load list of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/reading-and-writing-lists-to-a-file-in-python/\n",
    "accum_train_loss = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('loss_list_double_deep_ql.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        accum_train_loss.append(float(currentPlace))\n",
    "        \n",
    "accum_train_loss_2 = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('loss_2_list_double_deep_ql.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        accum_train_loss_2.append(float(currentPlace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149862, 6395904)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accum_train_loss), len(accum_train_loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "modelpath=\"model_DDQN_callback.h5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(modelpath, monitor='loss', verbose=0,\n",
    "                             save_best_only=True,\n",
    "                             mode='min') # graba slo los que mejoran en la funcin de prdida\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n",
      "Inicio 11:54:01\n",
      "episode: 0 11:54:01 avg last 10 rewards 0.8 avg last 10 loss 0.015508028771728277 avg last 10 loss 2 0.0\n",
      "episode: 500 11:54:42 avg last 10 rewards 1.0 avg last 10 loss 0.014209338673390448 avg last 10 loss 2 0.12167543172836304\n",
      "episode: 1000 11:55:22 avg last 10 rewards 0.8 avg last 10 loss 0.01339363430161029 avg last 10 loss 2 0.14353678822517396\n",
      "episode: 1500 11:56:05 avg last 10 rewards 0.8 avg last 10 loss 0.012571645725984126 avg last 10 loss 2 0.08356679677963257\n",
      "episode: 2000 11:56:52 avg last 10 rewards 0.6 avg last 10 loss 0.0145413130056113 avg last 10 loss 2 0.38711132705211637\n",
      "episode: 2500 11:57:42 avg last 10 rewards 1.0 avg last 10 loss 0.011858548317104579 avg last 10 loss 2 0.08449787497520447\n",
      "episode: 3000 11:58:22 avg last 10 rewards 1.0 avg last 10 loss 0.014915959525387735 avg last 10 loss 2 0.10784176588058472\n",
      "episode: 3500 11:59:07 avg last 10 rewards 0.8 avg last 10 loss 0.01486861112061888 avg last 10 loss 2 0.10660597681999207\n",
      "episode: 4000 11:59:53 avg last 10 rewards 0.6 avg last 10 loss 0.01324654701165855 avg last 10 loss 2 0.15679906606674193\n",
      "episode: 4500 12:00:37 avg last 10 rewards 1.0 avg last 10 loss 0.014099546673242003 avg last 10 loss 2 0.23172613680362703\n",
      "episode: 5000 12:01:20 avg last 10 rewards 0.4 avg last 10 loss 0.010002766922116279 avg last 10 loss 2 0.17275722026824952\n",
      "episode: 5500 12:02:03 avg last 10 rewards 1.0 avg last 10 loss 0.01718770896550268 avg last 10 loss 2 0.1497461497783661\n",
      "episode: 6000 12:02:49 avg last 10 rewards 1.0 avg last 10 loss 0.013369835482444614 avg last 10 loss 2 0.08342476487159729\n",
      "episode: 6500 12:03:44 avg last 10 rewards 1.0 avg last 10 loss 0.009156443562824279 avg last 10 loss 2 0.1109425038099289\n",
      "episode: 7000 12:04:28 avg last 10 rewards 0.6 avg last 10 loss 0.010473599919350818 avg last 10 loss 2 0.13116218745708466\n",
      "episode: 7500 12:05:08 avg last 10 rewards 1.0 avg last 10 loss 0.018147584679536522 avg last 10 loss 2 0.2994060218334198\n",
      "episode: 8000 12:05:48 avg last 10 rewards 0.8 avg last 10 loss 0.01278700337279588 avg last 10 loss 2 0.317502036690712\n",
      "episode: 8500 12:06:28 avg last 10 rewards 1.0 avg last 10 loss 0.018183241516817363 avg last 10 loss 2 0.11946885585784912\n",
      "episode: 9000 12:07:10 avg last 10 rewards 0.8 avg last 10 loss 0.010958846437279134 avg last 10 loss 2 0.15859679579734803\n",
      "episode: 9500 12:07:48 avg last 10 rewards 1.0 avg last 10 loss 0.013567009055987001 avg last 10 loss 2 0.09265775978565216\n",
      "episode: 10000 12:08:31 avg last 10 rewards 1.0 avg last 10 loss 0.012387308111647144 avg last 10 loss 2 0.061229726672172545\n",
      "10000 12:08:31\n",
      "episode: 10500 12:09:20 avg last 10 rewards 1.0 avg last 10 loss 0.014039298496209085 avg last 10 loss 2 0.1295409768819809\n",
      "episode: 11000 12:10:15 avg last 10 rewards 0.8 avg last 10 loss 0.013696490554139018 avg last 10 loss 2 0.23709944188594817\n",
      "episode: 11500 12:10:58 avg last 10 rewards 1.0 avg last 10 loss 0.014176932466216385 avg last 10 loss 2 0.18074495643377303\n",
      "episode: 12000 12:11:41 avg last 10 rewards 0.8 avg last 10 loss 0.013109167449874804 avg last 10 loss 2 0.21231609731912612\n",
      "episode: 12500 12:12:26 avg last 10 rewards 0.6 avg last 10 loss 0.014424408000195399 avg last 10 loss 2 0.20701785683631896\n",
      "episode: 13000 12:13:06 avg last 10 rewards 1.0 avg last 10 loss 0.012573472794611008 avg last 10 loss 2 0.2092345952987671\n",
      "episode: 13500 12:13:49 avg last 10 rewards 1.0 avg last 10 loss 0.014389924774877726 avg last 10 loss 2 0.20586952269077302\n",
      "episode: 14000 12:14:37 avg last 10 rewards 1.0 avg last 10 loss 0.011996819719206542 avg last 10 loss 2 0.1484345704317093\n",
      "episode: 14500 12:15:25 avg last 10 rewards 1.0 avg last 10 loss 0.011436897097155452 avg last 10 loss 2 0.16332787573337554\n",
      "episode: 15000 12:16:19 avg last 10 rewards 1.0 avg last 10 loss 0.01656845280667767 avg last 10 loss 2 0.3713610053062439\n",
      "episode: 15500 12:17:03 avg last 10 rewards 0.8 avg last 10 loss 0.013452420302201062 avg last 10 loss 2 0.06985951662063598\n",
      "episode: 16000 12:17:48 avg last 10 rewards 0.8 avg last 10 loss 0.015861514623975382 avg last 10 loss 2 0.09705045223236083\n",
      "episode: 16500 12:18:31 avg last 10 rewards 1.0 avg last 10 loss 0.015906856092624367 avg last 10 loss 2 0.17823920845985414\n",
      "episode: 17000 12:19:15 avg last 10 rewards 1.0 avg last 10 loss 0.015064845618326217 avg last 10 loss 2 0.1379339098930359\n",
      "episode: 17500 12:19:58 avg last 10 rewards 0.8 avg last 10 loss 0.014495811494998633 avg last 10 loss 2 0.09366583228111267\n",
      "episode: 18000 12:20:41 avg last 10 rewards 1.0 avg last 10 loss 0.015506962093058973 avg last 10 loss 2 0.23028387427330016\n",
      "episode: 18500 12:21:27 avg last 10 rewards 0.8 avg last 10 loss 0.01584907219512388 avg last 10 loss 2 0.1580570936203003\n",
      "episode: 19000 12:22:21 avg last 10 rewards 1.0 avg last 10 loss 0.01465156018966809 avg last 10 loss 2 0.1807926118373871\n",
      "episode: 19500 12:23:03 avg last 10 rewards 0.8 avg last 10 loss 0.013449007296003401 avg last 10 loss 2 0.11354382634162903\n",
      "episode: 20000 12:23:46 avg last 10 rewards 1.0 avg last 10 loss 0.015336954616941512 avg last 10 loss 2 0.07559199333190918\n",
      "20000 12:23:46\n",
      "episode: 20500 12:24:30 avg last 10 rewards 0.6 avg last 10 loss 0.015330325567629189 avg last 10 loss 2 0.34775506854057314\n",
      "episode: 21000 12:25:12 avg last 10 rewards 1.0 avg last 10 loss 0.01726000038906932 avg last 10 loss 2 0.19417848587036132\n",
      "episode: 21500 12:25:52 avg last 10 rewards 0.8 avg last 10 loss 0.016465702524874358 avg last 10 loss 2 0.16067572832107543\n",
      "episode: 22000 12:26:36 avg last 10 rewards 1.0 avg last 10 loss 0.013709855906199664 avg last 10 loss 2 0.11066738367080689\n",
      "episode: 22500 12:27:19 avg last 10 rewards 1.0 avg last 10 loss 0.014683134807273745 avg last 10 loss 2 0.0859142541885376\n",
      "episode: 23000 12:28:10 avg last 10 rewards 0.8 avg last 10 loss 0.012559801567113027 avg last 10 loss 2 0.09645432829856873\n",
      "episode: 23500 12:28:54 avg last 10 rewards 1.0 avg last 10 loss 0.00964337969198823 avg last 10 loss 2 0.11111971735954285\n",
      "episode: 24000 12:29:36 avg last 10 rewards 0.6 avg last 10 loss 0.011812469345750287 avg last 10 loss 2 0.08324075341224671\n",
      "episode: 24500 12:30:23 avg last 10 rewards 1.0 avg last 10 loss 0.01598131328355521 avg last 10 loss 2 0.1559859037399292\n",
      "episode: 25000 12:31:05 avg last 10 rewards 1.0 avg last 10 loss 0.013043569901492446 avg last 10 loss 2 0.1608705371618271\n",
      "episode: 25500 12:31:47 avg last 10 rewards 1.0 avg last 10 loss 0.014212973997928202 avg last 10 loss 2 0.15673683881759642\n",
      "episode: 26000 12:32:30 avg last 10 rewards 0.6 avg last 10 loss 0.014432846562704072 avg last 10 loss 2 0.08713979721069336\n",
      "episode: 26500 12:33:11 avg last 10 rewards 0.8 avg last 10 loss 0.01350617923308164 avg last 10 loss 2 0.09982152581214905\n",
      "episode: 27000 12:34:08 avg last 10 rewards 1.0 avg last 10 loss 0.010619567648973316 avg last 10 loss 2 0.10585320293903351\n",
      "episode: 27500 12:34:55 avg last 10 rewards 1.0 avg last 10 loss 0.011033955187303945 avg last 10 loss 2 0.24143415689468384\n",
      "episode: 28000 12:35:36 avg last 10 rewards 1.0 avg last 10 loss 0.009245509066386149 avg last 10 loss 2 0.1359726995229721\n",
      "episode: 28500 12:36:18 avg last 10 rewards 1.0 avg last 10 loss 0.014281218365067617 avg last 10 loss 2 0.15824518501758575\n",
      "episode: 29000 12:37:00 avg last 10 rewards 0.8 avg last 10 loss 0.011151058017276227 avg last 10 loss 2 0.09142650663852692\n",
      "episode: 29500 12:37:44 avg last 10 rewards 1.0 avg last 10 loss 0.012875985424034297 avg last 10 loss 2 0.17316948771476745\n",
      "episode: 30000 12:38:26 avg last 10 rewards 1.0 avg last 10 loss 0.010651172598591075 avg last 10 loss 2 0.09369520246982574\n",
      "30000 12:38:26\n",
      "episode: 30500 12:39:08 avg last 10 rewards 1.0 avg last 10 loss 0.017139661184046417 avg last 10 loss 2 0.1889558970928192\n",
      "episode: 31000 12:39:55 avg last 10 rewards 0.8 avg last 10 loss 0.011602412845240906 avg last 10 loss 2 0.2162727177143097\n",
      "episode: 31500 12:40:51 avg last 10 rewards 1.0 avg last 10 loss 0.014040972775546834 avg last 10 loss 2 0.29081052243709565\n",
      "episode: 32000 12:41:33 avg last 10 rewards 1.0 avg last 10 loss 0.012017563963308931 avg last 10 loss 2 0.08769003450870513\n",
      "episode: 32500 12:42:19 avg last 10 rewards 0.8 avg last 10 loss 0.010067814961075783 avg last 10 loss 2 0.07778472304344178\n",
      "episode: 33000 12:43:05 avg last 10 rewards 1.0 avg last 10 loss 0.010669511754531413 avg last 10 loss 2 0.18910889625549315\n",
      "episode: 33500 12:43:54 avg last 10 rewards 0.6 avg last 10 loss 0.009980596217792482 avg last 10 loss 2 0.14542405009269715\n",
      "episode: 34000 12:44:38 avg last 10 rewards 0.6 avg last 10 loss 0.014664496493060142 avg last 10 loss 2 0.06135548651218414\n",
      "episode: 34500 12:45:20 avg last 10 rewards 1.0 avg last 10 loss 0.012114223634125666 avg last 10 loss 2 0.1456068277359009\n",
      "episode: 35000 12:46:06 avg last 10 rewards 0.8 avg last 10 loss 0.0112605708418414 avg last 10 loss 2 0.12023713290691376\n",
      "episode: 35500 12:47:06 avg last 10 rewards 1.0 avg last 10 loss 0.014255228766705841 avg last 10 loss 2 0.13066981732845306\n",
      "episode: 36000 12:47:47 avg last 10 rewards 0.6 avg last 10 loss 0.011905561562161892 avg last 10 loss 2 0.18635677099227904\n",
      "episode: 36500 12:48:31 avg last 10 rewards 0.8 avg last 10 loss 0.01635710975388065 avg last 10 loss 2 0.15951175689697267\n",
      "episode: 37000 12:49:18 avg last 10 rewards 1.0 avg last 10 loss 0.01138515952625312 avg last 10 loss 2 0.14377717077732086\n",
      "episode: 37500 12:50:00 avg last 10 rewards 0.6 avg last 10 loss 0.013648455607471988 avg last 10 loss 2 0.09012738764286041\n",
      "episode: 38000 12:50:42 avg last 10 rewards 1.0 avg last 10 loss 0.014575353177497164 avg last 10 loss 2 0.06790346503257752\n",
      "episode: 38500 12:51:25 avg last 10 rewards 0.6 avg last 10 loss 0.011362698418088258 avg last 10 loss 2 0.12344111800193787\n",
      "episode: 39000 12:52:16 avg last 10 rewards 0.8 avg last 10 loss 0.010423791233915836 avg last 10 loss 2 0.1511390507221222\n",
      "episode: 39500 12:53:07 avg last 10 rewards 1.0 avg last 10 loss 0.011106498137814924 avg last 10 loss 2 0.12239923477172851\n",
      "episode: 40000 12:53:51 avg last 10 rewards 1.0 avg last 10 loss 0.010528266901383176 avg last 10 loss 2 0.0951683521270752\n",
      "40000 12:53:51\n",
      "episode: 40500 12:54:38 avg last 10 rewards 0.8 avg last 10 loss 0.011784658359829336 avg last 10 loss 2 0.1630016654729843\n",
      "episode: 41000 12:55:21 avg last 10 rewards 1.0 avg last 10 loss 0.009677692141849548 avg last 10 loss 2 0.15134689807891846\n",
      "episode: 41500 12:56:02 avg last 10 rewards 0.8 avg last 10 loss 0.01296900698216632 avg last 10 loss 2 0.1156051516532898\n",
      "episode: 42000 12:56:46 avg last 10 rewards 1.0 avg last 10 loss 0.008623452379833908 avg last 10 loss 2 0.12221828699111939\n",
      "episode: 42500 12:57:27 avg last 10 rewards 0.8 avg last 10 loss 0.011045055912109091 avg last 10 loss 2 0.2189303994178772\n",
      "episode: 43000 12:58:09 avg last 10 rewards 1.0 avg last 10 loss 0.012237915210425854 avg last 10 loss 2 0.10442019701004028\n",
      "episode: 43500 12:59:03 avg last 10 rewards 1.0 avg last 10 loss 0.010702312592184172 avg last 10 loss 2 0.05637587308883667\n",
      "episode: 44000 12:59:51 avg last 10 rewards 0.8 avg last 10 loss 0.009091914974851534 avg last 10 loss 2 0.11169942021369934\n",
      "episode: 44500 13:00:33 avg last 10 rewards 1.0 avg last 10 loss 0.01353423276450485 avg last 10 loss 2 0.1281306564807892\n",
      "episode: 45000 13:01:17 avg last 10 rewards 1.0 avg last 10 loss 0.010734706663060933 avg last 10 loss 2 0.1307375967502594\n",
      "episode: 45500 13:02:02 avg last 10 rewards 1.0 avg last 10 loss 0.012091507471632212 avg last 10 loss 2 0.11416635513305665\n",
      "episode: 46000 13:02:47 avg last 10 rewards 0.6 avg last 10 loss 0.01233630120404996 avg last 10 loss 2 0.06864556074142455\n",
      "episode: 46500 13:03:28 avg last 10 rewards 0.6 avg last 10 loss 0.012507637002272531 avg last 10 loss 2 0.24445246458053588\n",
      "episode: 47000 13:04:08 avg last 10 rewards 1.0 avg last 10 loss 0.015265665418701247 avg last 10 loss 2 0.07280088663101196\n",
      "episode: 47500 13:04:55 avg last 10 rewards 1.0 avg last 10 loss 0.012258594320155681 avg last 10 loss 2 0.120774245262146\n",
      "episode: 48000 13:05:41 avg last 10 rewards 1.0 avg last 10 loss 0.01390815495979041 avg last 10 loss 2 0.2399916261434555\n",
      "episode: 48500 13:06:22 avg last 10 rewards 1.0 avg last 10 loss 0.011883835383923725 avg last 10 loss 2 0.07406730353832244\n",
      "episode: 49000 13:07:04 avg last 10 rewards 1.0 avg last 10 loss 0.011159182980190963 avg last 10 loss 2 0.05876731276512146\n",
      "episode: 49500 13:07:47 avg last 10 rewards 1.0 avg last 10 loss 0.01122802619356662 avg last 10 loss 2 0.14723530411720276\n",
      "Fin 13:08:28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from kaggle_environments import make\n",
    "from collections import deque\n",
    "\n",
    "#import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "action_size = env.configuration.columns\n",
    "num_cols = env.configuration.columns\n",
    "num_rows = env.configuration.rows\n",
    "state_size = env.configuration.columns * env.configuration.rows\n",
    "\n",
    "# Training agent in first position (player 1) against the negamax/random agent.\n",
    "#trainer = env.train([None, \"negamax\"])\n",
    "trainer = env.train([None, \"random\"])\n",
    "\n",
    "discount = 0.9\n",
    "#epsilon = 0.99 # parameter for epsilon-greedy policy\n",
    "#epsilon = 0.01 # parameter for epsilon-greedy policy\n",
    "epsilon = 0.22089720180217606 # parameter for epsilon-greedy policy\n",
    "epsilon_min = 0.05 # epsilon is decreasing but we avoid it to be smaller than epsilon_min\n",
    "t_max = 1000 # used for environments without end\n",
    "total_reward = 0 # used for displaying the evolution of the rewards as the model learns\n",
    "memory = deque(maxlen=10000)\n",
    "batch_size = 128\n",
    "accum_train_loss_2 = []\n",
    "#last_n_steps = 2\n",
    "last_n_steps = 5\n",
    "update_weights_freq = 50 # Each update_weights_freq steps we update the weights of model_target with model weights\n",
    "num_episodes = 50000\n",
    "\n",
    "#new_model = True\n",
    "new_model = False\n",
    "# Create/Load model\n",
    "if new_model:\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(8, kernel_size=3, activation='relu', input_shape=(num_rows, num_cols, 1))) # padding=valid # Input: (5, 5, 1) Output: (3, 3, 8)\n",
    "    model.add(Flatten()) # Input: (3, 3, 8) Output: (72)\n",
    "    model.add(Dense(8, activation='relu')) # Input: (72) Output: (8)\n",
    "    model.add(Dense(action_size, activation=\"linear\")) # Input: (8) Output: (5)\n",
    "    model.compile(Adam(lr=0.01), loss='mse')\n",
    "    \n",
    "    model_target = Sequential()\n",
    "    model_target.add(Conv2D(8, kernel_size=3, activation='relu', input_shape=(num_rows, num_cols, 1))) # padding=valid # Input: (5, 5, 1) Output (3, 3, 8)\n",
    "    model_target.add(Flatten()) # Input: (3, 3, 8) Output: (72)\n",
    "    model_target.add(Dense(8, activation='relu')) # Input: (72) Output: (8)\n",
    "    model_target.add(Dense(action_size, activation=\"linear\")) # Input: (8) Output: (5)\n",
    "    model_target.compile(Adam(lr=0.01), loss='mse')\n",
    "    accum_train_loss = []\n",
    "    rewards_list_double_deep_ql = []\n",
    "    accum_train_loss_2 = []\n",
    "else:\n",
    "    model = load_model('model_DDQN.h5')\n",
    "    model_target = load_model('model_target_DDQN.h5')\n",
    "\n",
    "def get_best_action(board, num_cols, model, function):\n",
    "    \"\"\"\n",
    "    Returns the action with maximum q-value or minimum q-value depending on the needs. This will be done only in available columns\n",
    "    Parameter \"function\" should be \"max\" or \"min\"\n",
    "    \"\"\"\n",
    "    # function parameter validation\n",
    "    if (function != \"max\" and function != \"min\"):\n",
    "        print(\"Parameter function should be \"\"max\"\" or \"\"min\"\"\")\n",
    "        return -1\n",
    "    \n",
    "    # greedy policy\n",
    "    if (function == \"max\"):\n",
    "        best_q_value = float(\"-inf\")\n",
    "    else:\n",
    "        best_q_value = float(\"inf\")\n",
    "    \n",
    "    # Search the best q-value but only in available columns (we cannot use np.argmax)\n",
    "    best_action = None\n",
    "    rewards = model(board).numpy()[0]\n",
    "    available_cols = [col for col in range(num_cols) if board[0][0][col] == 0]\n",
    "    for col in available_cols:\n",
    "        q_value = rewards[col]\n",
    "        if (function == \"max\" and q_value > best_q_value):\n",
    "            best_action = col\n",
    "            best_q_value = q_value\n",
    "        else:\n",
    "            if (function == \"min\" and q_value < best_q_value):\n",
    "                best_action = col\n",
    "                best_q_value = q_value\n",
    "                \n",
    "    return best_action\n",
    "\n",
    "def get_action(board, num_cols, epsilon, model):\n",
    "    available_cols = [col for col in range(num_cols) if board[0][0][col] == 0]\n",
    "    # epison-greedy policy\n",
    "    if (np.random.random() > epsilon):\n",
    "        # greedy\n",
    "        # We need to search best value, but only in available columns, so we cannot use np.argmax\n",
    "        return get_best_action(board, num_cols, model, \"max\")\n",
    "    else:\n",
    "        # random\n",
    "        return int(np.random.choice(available_cols))\n",
    "\n",
    "def convert_for_CNN(board, num_rows, num_cols):\n",
    "    \"\"\"\n",
    "    Converts the board (list) into a matrix of shape (rows, cols, 1), so that a CNN can work with it.\n",
    "    Player 2 checkers are replaced to -1\n",
    "    \"\"\"\n",
    "    board = [np.float32(-1) if x==2 else np.float32(x) for x in board]\n",
    "    return np.reshape(board, [1, num_rows, num_cols, 1])\n",
    "    \n",
    "    \n",
    "t = datetime.datetime.now()\n",
    "print(\"Inicio\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "obs = trainer.reset()\n",
    "for i in range(num_episodes):\n",
    "    done = False\n",
    "    t = 0\n",
    "    while not done and t < t_max:\n",
    "        # Convert board in a matrix to use it as the neural network input\n",
    "        board_matrix = convert_for_CNN(obs.board, num_rows, num_cols) #np.reshape(obs.board, [1, state_size])\n",
    "        action = get_action(board_matrix, num_cols, epsilon, model)\n",
    "        obs, reward, done, info = trainer.step(action)\n",
    "        total_reward += reward\n",
    "        new_board_matrix = convert_for_CNN(obs.board, num_rows, num_cols)\n",
    "        \n",
    "        t += 1\n",
    "        \n",
    "    # Train at the end of each episode\n",
    "\n",
    "    # Add to the experience replay only the last n steps of the episode (all of them as player 1)\n",
    "    num_steps = env.steps[-1][0][\"observation\"][\"step\"]\n",
    "    last_reward = env.steps[-1][0][\"reward\"]\n",
    "    for j in range(1, num_steps + 1):\n",
    "        state = convert_for_CNN(env.steps[j-1][0][\"observation\"][\"board\"], num_rows, num_cols)\n",
    "        next_state = convert_for_CNN(env.steps[j][0][\"observation\"][\"board\"], num_rows, num_cols)\n",
    "        done = (env.steps[j][0][\"status\"] == 'DONE')\n",
    "        # REWARD SHAPING --> As we only have reward at the end of the episode, we generate partial rewards\n",
    "        if done:\n",
    "            reward = last_reward * 2\n",
    "        else:\n",
    "            reward = (0.5 ** (num_steps - j)) * last_reward\n",
    "        if env.steps[j-1][0][\"status\"] == 'ACTIVE':\n",
    "            # Player 1\n",
    "            action = env.steps[j][0][\"action\"]\n",
    "        else:\n",
    "            # Player 2 --> Convert player 2 movements into player one's\n",
    "            action = env.steps[j][1][\"action\"]\n",
    "            # Convert player 2 boards to player 1's\n",
    "            state = (state * (-1)) + 0 # +0 to avoid negative zeros\n",
    "            next_state = (next_state * (-1)) + 0 # +0 to avoid negative zeros\n",
    "            reward = reward *(-1) + 0\n",
    "        player = 1\n",
    "        if (j > num_steps - last_n_steps):\n",
    "            memory.append((state, action, reward, next_state, player, done))\n",
    "\n",
    "    # Extract a minibatch from the experience replay and train the neural network\n",
    "    if len(memory) > batch_size:\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        \n",
    "        # Create a list with the states\n",
    "        state_list = [item[0] for item in minibatch]\n",
    "        next_state_list = [item[3] for item in minibatch]\n",
    "        \n",
    "        # Convert these lists of arrays into numpy arrays\n",
    "        X_states = np.concatenate(state_list, axis=0) # shape=(batch_size, num_rows, num_cols, 1)\n",
    "        y_targets = np.empty(shape=(batch_size, action_size)) # shape=(batch_size, action_size)\n",
    "        next_state_array = np.concatenate(next_state_list, axis=0) # shape=(batch_size, num_rows, num_cols, 1)\n",
    "        \n",
    "        # Predict in each network\n",
    "        state_predict = model(X_states).numpy() # shape=(batch_size, action_size)\n",
    "        next_state_predict = model(next_state_array).numpy() # shape=(batch_size, action_size)\n",
    "        next_state_predict_target = model_target(next_state_array).numpy() # shape=(batch_size, action_size)\n",
    "            \n",
    "        for j, (state_batch, action_batch, reward_batch, next_state_batch, player_batch, done_batch) in enumerate(minibatch):\n",
    "            X_states[j] = state_batch[0]\n",
    "            model_next_qvalue = next_state_predict[j] # shape=(action_size)\n",
    "            model_target_next_qvalue = next_state_predict_target[j] # shape=(action_size)\n",
    "            # If it's an ending state, just update the target with the reward. If not, update target using Bellman optimality equation for Q-values\n",
    "            if not done:\n",
    "                target_updated = reward_batch + discount * model_target_next_qvalue[np.argmax(model_next_qvalue)]\n",
    "            else:\n",
    "                target_updated = reward_batch\n",
    "                \n",
    "            targets = state_predict[j] # shape=(action_size)\n",
    "            #print(\"Target obtenida:\", targets[0][action_batch], \"Target objetivo:\", target_updated)\n",
    "            loss = abs(target_updated - targets[action_batch])\n",
    "            accum_train_loss_2.append(loss)\n",
    "            targets[action_batch] = target_updated\n",
    "            y_targets[j] = targets\n",
    "            \n",
    "        # Train the model with these new targets\n",
    "        history = model.fit(X_states, y_targets, epochs=1, callbacks=callbacks_list, verbose=0)\n",
    "            \n",
    "        accum_train_loss.append(history.history['loss'][0])\n",
    "            \n",
    "    rewards_list_double_deep_ql.append(total_reward)\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Print a summary each 500 episodes\n",
    "    if (i % 500) == 0: # and len(accum_train_loss)>=10\n",
    "        print(\"episode:\", i, datetime.datetime.now().strftime(\"%H:%M:%S\"), \"avg last 10 rewards\", sum(rewards_list_double_deep_ql[-10:])/10, \n",
    "              \"avg last 10 loss\", sum(accum_train_loss[-10:])/10, \n",
    "              \"avg last 10 loss 2\", sum(accum_train_loss_2[-10:])/10)\n",
    "    \n",
    "    # Decrease epsilon in every step so that exploration (random) is getting smaller against exploitation (greedy)\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= 0.99999\n",
    "        \n",
    "    obs = trainer.reset()\n",
    "\n",
    "    # Update target network weights each update_weights_freq steps\n",
    "    if (i % update_weights_freq) == 0 and len(memory) > batch_size and i > 0:\n",
    "        model_weights = model.get_weights()\n",
    "        model_target.set_weights(model_weights)\n",
    "\n",
    "    if (i % 10000) == 0 and i > 0:\n",
    "        print(i, datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    if (i % 20000) == 0 and i > 0:\n",
    "        last_n_steps += 1\n",
    "        \n",
    "\n",
    "t = datetime.datetime.now()\n",
    "print(\"Fin\", t.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "\n",
    "\n",
    "# 50000 CNN random, fit con todo el minibatch (Local) solo los n pasos finales (todos como jugador 1) # 50.000\n",
    "# 00:34:20\n",
    "# 01:49:19\n",
    "# 50000 CNN random, fit con todo el minibatch (Local) solo los n pasos finales (todos como jugador 1) # 100.000\n",
    "# 12:25:53\n",
    "# 14:18:40\n",
    "# 50000 CNN random, fit con todo el minibatch (Local) solo los n pasos finales (todos como jugador 1) # 150.000 <--- MEJOR RESULTADO\n",
    "# 14:51:29\n",
    "# 16:40:06\n",
    "# 50000 CNN random, fit con todo el minibatch (Local) solo los n pasos finales (todos como jugador 1) # 200.000\n",
    "# 11:54:01\n",
    "# 13:08:28\n",
    "\n",
    "# 50000 CNN negamax, fit con todo el minibatch (Local) solo los n pasos finales (todos como jugador 1) # 50.000\n",
    "# 15:11:49\n",
    "# 17:45:53\n",
    "# 50000 CNN negamax, fit con todo el minibatch (Local) solo los n pasos finales (todos como jugador 1) # 100.000\n",
    "# 19:41:24\n",
    "# 22:14:12\n",
    "# 50000 CNN negamax, fit con todo el minibatch (Local) solo los n pasos finales (todos como jugador 1) # 150.000\n",
    "# 22:39:31\n",
    "# 01:18:11\n",
    "# 50000 CNN negamax, fit con todo el minibatch (Local) solo los n pasos finales (todos como jugador 1) # 200.000\n",
    "# 20:38:57\n",
    "# 23:44:46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([ 25977, 174023], dtype=int64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.unique(rewards_list_double_deep_ql, return_counts=True)\n",
    "# (array([-1,  1]), array([12595, 37405], dtype=int64))\n",
    "# (array([-1,  1]), array([19389, 80611], dtype=int64))\n",
    "# (array([-1,  1]), array([ 23248, 126752], dtype=int64))\n",
    "# (array([-1,  1]), array([ 25977, 174023], dtype=int64))\n",
    "\n",
    "# (array([-1,  1]), array([47765,  2235], dtype=int64))\n",
    "# (array([-1,  1]), array([95603,  4397], dtype=int64))\n",
    "# (array([-1,  1]), array([142004,   7996], dtype=int64))\n",
    "# (array([-1,  1]), array([190031,   9969], dtype=int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Deep Q-Learning mean reward = 0.904\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html\n",
    "# ewm --> Weighted average of the values from x0 to xt. The weight of each element is decreasing from xt (weight=1) to xt-n (weight=(1-alpha)^n). alpha=2/(span+1)=2/101\n",
    "# This function gives us the trend of the rewards, instead of a sharp graph with a lot of -1's and 1's\n",
    "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "# Displaying the average of n elemtens instead of each element of the list \n",
    "n = 1000\n",
    "moving_average2 = [sum(rewards_list_double_deep_ql[i:i+n])/n for i in range(0,len(rewards_list_double_deep_ql)-n+1)]\n",
    "\n",
    "clear_output(True)\n",
    "print('Double Deep Q-Learning mean reward =', np.mean(rewards_list_double_deep_ql[-1000:]))\n",
    "plt.title(\"epsilon = %s\" % epsilon)\n",
    "plt.plot(moving_average(rewards_list_double_deep_ql), label='ev_double_deep_ql')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.show()\n",
    "\n",
    "# Double Deep Q-Learning mean reward = 0.62\n",
    "# Double Deep Q-Learning mean reward = 0.792\n",
    "# Double Deep Q-Learning mean reward = 0.896\n",
    "# Double Deep Q-Learning mean reward = 0.904\n",
    "\n",
    "# Double Deep Q-Learning mean reward = -0.916\n",
    "# Double Deep Q-Learning mean reward = -0.848\n",
    "# Double Deep Q-Learning mean reward = -0.926\n",
    "# Double Deep Q-Learning mean reward = -0.956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13398059058392722"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49936"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accum_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Deep Q-Learning mean loss 1 = 0.01345655702817021\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html\n",
    "# ewm --> Weighted average of the values from x0 to xt. The weight of each element is decreasing from xt (weight=1) to xt-n (weight=(1-alpha)^n). alpha=2/(span+1)=2/101\n",
    "# This function gives us the trend of the rewards, instead of a sharp graph with a lot of -1's and 1's\n",
    "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "# Displaying the average of n elemtens instead of each element of the list \n",
    "n = 1000\n",
    "moving_average2 = [sum(accum_train_loss[i:i+n])/n for i in range(0,len(accum_train_loss)-n+1)]\n",
    "\n",
    "clear_output(True)\n",
    "print('Double Deep Q-Learning mean loss 1 =', np.mean(accum_train_loss[-1000:]))\n",
    "plt.title(\"epsilon = %s\" % epsilon)\n",
    "plt.plot(moving_average(accum_train_loss), label='loss_double_deep_ql')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(0, 0.1)\n",
    "plt.show()\n",
    "\n",
    "# Double Deep Q-Learning mean loss 1 = 0.029851529055216815\n",
    "# Double Deep Q-Learning mean loss 1 = 0.01879366305511212\n",
    "# Double Deep Q-Learning mean loss 1 = 0.013116003537375945\n",
    "# Double Deep Q-Learning mean loss 1 = 0.01345655702817021\n",
    "\n",
    "# Double Deep Q-Learning mean loss 1 = 0.02545835962938145\n",
    "# Double Deep Q-Learning mean loss 1 = 0.015763490679324605\n",
    "# Double Deep Q-Learning mean loss 1 = 0.01234157059469726\n",
    "# Double Deep Q-Learning mean loss 1 = 0.00909327546262648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html\n",
    "# ewm --> Weighted average of the values from x0 to xt. The weight of each element is decreasing from xt (weight=1) to xt-n (weight=(1-alpha)^n). alpha=2/(span+1)=2/101\n",
    "# This function gives us the trend of the rewards, instead of a sharp graph with a lot of -1's and 1's\n",
    "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "# Displaying the average of n elemtens instead of each element of the list \n",
    "n = 1000\n",
    "moving_average2 = [sum(accum_train_loss_2[i:i+n])/n for i in range(0,len(accum_train_loss_2)-n+1)]\n",
    "\n",
    "clear_output(True)\n",
    "print('Deep Q-Learning mean loss 2 =', np.mean(accum_train_loss_2[-1000:]))\n",
    "plt.title(\"epsilon = %s\" % epsilon)\n",
    "plt.plot(moving_average(accum_train_loss_2), label='loss_deep_ql')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_DDQN.h5')\n",
    "model_target.save('model_target_DDQN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Save list of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'rewards_list_double_deep_ql.txt','w+') as f:\n",
    "    for element in rewards_list_double_deep_ql:\n",
    "        f.write(str(element) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Save list of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'loss_list_double_deep_ql.txt','w+') as f:\n",
    "    for element in accum_train_loss:\n",
    "        f.write(str(element) + '\\n')\n",
    "        \n",
    "with open(r'loss_2_list_double_deep_ql.txt','w+') as f:\n",
    "    for element in accum_train_loss_2:\n",
    "        f.write(str(element) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.plot(accum_train_loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx0QJniRhu4_"
   },
   "source": [
    "Displaying kernels in Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Eil-vNA76R8D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "shape of weights[0]:  (3, 3, 1, 8)\n",
      "shape of weights[1]:  (8,)\n",
      "shape of weights[2]:  (72, 8)\n",
      "shape of weights[3]:  (8,)\n",
      "shape of weights[4]:  (8, 5)\n",
      "shape of weights[5]:  (5,)\n"
     ]
    }
   ],
   "source": [
    "weights = model.get_weights()\n",
    "print(np.shape(weights))\n",
    "for i in range(len(weights)):\n",
    "    print('shape of weights[%d]: ' % i, np.shape(weights[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QP7WnsQ1hxOo"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x540 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# kernels de la primera capa convolucional\n",
    "ncapa = 0\n",
    "\n",
    "nfilters = weights[ncapa].shape[3]\n",
    "ncols = 4 # nmero de columnas en la figura\n",
    "\n",
    "\n",
    "ma = abs(weights[ncapa]).max()\n",
    "nrows = int(np.ceil(nfilters/ncols)) # nmero de filas en la figura\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15,15*nrows/ncols))\n",
    "axes_r = axes.ravel()\n",
    "for i in range(nfilters):\n",
    "    kernel = weights[ncapa][:,:,0,i]\n",
    "    ax = axes_r[i]\n",
    "    ax.imshow(kernel, vmin=-ma, vmax=ma, cmap='bwr')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.set_title('pesos kernel %d' % i, fontsize=10)\n",
    "for i in range(nfilters,nrows*ncols):\n",
    "    fig.delaxes(axes_r[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 4164 Zero: 0 Positive: 5836\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "memory2 = memory.copy()\n",
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "zero = 0\n",
    "a = []\n",
    "for i in memory2:\n",
    "    a.append(i[2])\n",
    "    if i[2] > 0:\n",
    "        positive += 1\n",
    "    else:\n",
    "        if i[2] < 0:\n",
    "            negative += 1\n",
    "        else:\n",
    "            zero += 1\n",
    "\n",
    "print(\"Negative:\", negative, \"Zero:\", zero, \"Positive:\", positive)\n",
    "plt.hist(a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.5     , -0.125   , -0.03125 ,  0.015625,  0.0625  ,  0.25    ,\n",
       "         2.      ]),\n",
       " array([1680, 1680,  804,  796, 1680, 1680, 1680], dtype=int64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(a, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the agent as a function, so that we can use it with Kaggle environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\n",
      "{'remainingOverageTime': 60, 'step': 0, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\n",
      "{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\n",
      "{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0], 'mark': 1}\n",
      "Column:0 Reward:-0.117846906\n",
      "Column:1 Reward:0.06508827\n",
      "Column:2 Reward:0.17722237\n",
      "Column:3 Reward:0.04854679\n",
      "Column:4 Reward:-0.0121792555\n",
      "{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\n",
      "{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0], 'mark': 1}\n",
      "Column:0 Reward:-0.121901214\n",
      "Column:1 Reward:-0.045523286\n",
      "Column:2 Reward:0.12066203\n",
      "Column:3 Reward:0.013103247\n",
      "Column:4 Reward:-0.05243379\n",
      "{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\n",
      "{'remainingOverageTime': 60, 'step': 6, 'board': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0], 'mark': 1}\n",
      "Column:0 Reward:-0.32141322\n",
      "Column:1 Reward:0.35419166\n",
      "Column:2 Reward:-0.2084791\n",
      "Column:3 Reward:0.42818117\n",
      "Column:4 Reward:0.2503333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"<!--\n",
       "  Copyright 2020 Kaggle Inc\n",
       "\n",
       "  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "  you may not use this file except in compliance with the License.\n",
       "  You may obtain a copy of the License at\n",
       "\n",
       "      http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "  Unless required by applicable law or agreed to in writing, software\n",
       "  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "  See the License for the specific language governing permissions and\n",
       "  limitations under the License.\n",
       "-->\n",
       "<!DOCTYPE html>\n",
       "<html lang=&quot;en&quot;>\n",
       "  <head>\n",
       "    <title>Kaggle Simulation Player</title>\n",
       "    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n",
       "    <link\n",
       "      rel=&quot;stylesheet&quot;\n",
       "      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n",
       "      crossorigin=&quot;anonymous&quot;\n",
       "    />\n",
       "    <style type=&quot;text/css&quot;>\n",
       "      html,\n",
       "      body {\n",
       "        height: 100%;\n",
       "        font-family: sans-serif;\n",
       "        margin: 0px;\n",
       "      }\n",
       "      canvas {\n",
       "        /* image-rendering: -moz-crisp-edges;\n",
       "        image-rendering: -webkit-crisp-edges;\n",
       "        image-rendering: pixelated;\n",
       "        image-rendering: crisp-edges; */\n",
       "      }\n",
       "    </style>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n",
       "    <script>\n",
       "      // Polyfill for Styled Components\n",
       "      window.React = {\n",
       "        ...preact,\n",
       "        createElement: preact.h,\n",
       "        PropTypes: { func: {} },\n",
       "      };\n",
       "    </script>\n",
       "    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n",
       "  </head>\n",
       "  <body>\n",
       "    <script>\n",
       "      \n",
       "window.kaggle = {\n",
       "  &quot;debug&quot;: true,\n",
       "  &quot;playing&quot;: true,\n",
       "  &quot;step&quot;: 0,\n",
       "  &quot;controls&quot;: true,\n",
       "  &quot;environment&quot;: {\n",
       "    &quot;id&quot;: &quot;6c28ab90-d0a1-11ec-bebb-38142801973a&quot;,\n",
       "    &quot;name&quot;: &quot;connectx&quot;,\n",
       "    &quot;title&quot;: &quot;ConnectX&quot;,\n",
       "    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n",
       "    &quot;version&quot;: &quot;1.0.1&quot;,\n",
       "    &quot;configuration&quot;: {\n",
       "      &quot;rows&quot;: 5,\n",
       "      &quot;columns&quot;: 5,\n",
       "      &quot;inarow&quot;: 3,\n",
       "      &quot;episodeSteps&quot;: 1000,\n",
       "      &quot;actTimeout&quot;: 2,\n",
       "      &quot;runTimeout&quot;: 1200,\n",
       "      &quot;agentTimeout&quot;: 60,\n",
       "      &quot;timeout&quot;: 2\n",
       "    },\n",
       "    &quot;specification&quot;: {\n",
       "      &quot;action&quot;: {\n",
       "        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n",
       "        &quot;type&quot;: &quot;integer&quot;,\n",
       "        &quot;minimum&quot;: 0,\n",
       "        &quot;default&quot;: 0\n",
       "      },\n",
       "      &quot;agents&quot;: [\n",
       "        2\n",
       "      ],\n",
       "      &quot;configuration&quot;: {\n",
       "        &quot;episodeSteps&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;minimum&quot;: 1,\n",
       "          &quot;default&quot;: 1000\n",
       "        },\n",
       "        &quot;actTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 2\n",
       "        },\n",
       "        &quot;runTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 1200\n",
       "        },\n",
       "        &quot;columns&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 7,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;rows&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 6,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;inarow&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 4,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;agentTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;timeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 2,\n",
       "          &quot;minimum&quot;: 0\n",
       "        }\n",
       "      },\n",
       "      &quot;info&quot;: {},\n",
       "      &quot;observation&quot;: {\n",
       "        &quot;remainingOverageTime&quot;: {\n",
       "          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n",
       "          &quot;shared&quot;: false,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;step&quot;: {\n",
       "          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 0\n",
       "        },\n",
       "        &quot;board&quot;: {\n",
       "          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n",
       "          &quot;type&quot;: &quot;array&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;default&quot;: []\n",
       "        },\n",
       "        &quot;mark&quot;: {\n",
       "          &quot;defaults&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ],\n",
       "          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n",
       "          &quot;enum&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ]\n",
       "        }\n",
       "      },\n",
       "      &quot;reward&quot;: {\n",
       "        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n",
       "        &quot;enum&quot;: [\n",
       "          -1,\n",
       "          0,\n",
       "          1\n",
       "        ],\n",
       "        &quot;default&quot;: 0,\n",
       "        &quot;type&quot;: [\n",
       "          &quot;number&quot;,\n",
       "          &quot;null&quot;\n",
       "        ]\n",
       "      }\n",
       "    },\n",
       "    &quot;steps&quot;: [\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 0,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 1,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 2,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 3,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 4,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 5,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 6,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 7,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: -1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 8,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 4,\n",
       "          &quot;reward&quot;: 1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        }\n",
       "      ]\n",
       "    ],\n",
       "    &quot;rewards&quot;: [\n",
       "      -1,\n",
       "      1\n",
       "    ],\n",
       "    &quot;statuses&quot;: [\n",
       "      &quot;DONE&quot;,\n",
       "      &quot;DONE&quot;\n",
       "    ],\n",
       "    &quot;schema_version&quot;: 1,\n",
       "    &quot;info&quot;: {}\n",
       "  },\n",
       "  &quot;logs&quot;: [\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 2.2e-05,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 0, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.018905,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.727012,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0], 'mark': 1}\\nERROR! Session/line number was not unique in database. History logging moved to new session 962\\nColumn:0 Reward:-0.117846906\\nColumn:1 Reward:0.06508827\\nColumn:2 Reward:0.17722237\\nColumn:3 Reward:0.04854679\\nColumn:4 Reward:-0.0121792555\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.009643,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.605521,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0], 'mark': 1}\\nColumn:0 Reward:-0.121901214\\nColumn:1 Reward:-0.045523286\\nColumn:2 Reward:0.12066203\\nColumn:3 Reward:0.013103247\\nColumn:4 Reward:-0.05243379\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.019629,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.512521,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 6, 'board': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0], 'mark': 1}\\nColumn:0 Reward:-0.32141322\\nColumn:1 Reward:0.35419166\\nColumn:2 Reward:-0.2084791\\nColumn:3 Reward:0.42818117\\nColumn:4 Reward:0.2503333\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000116,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 4.9e-05,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 0, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.019241,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.565608,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0], 'mark': 1}\\nColumn:0 Reward:-0.05105388\\nColumn:1 Reward:0.07585454\\nColumn:2 Reward:0.096669376\\nColumn:3 Reward:0.034206152\\nColumn:4 Reward:0.040368468\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.009572,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.734671,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0], 'mark': 1}\\nColumn:0 Reward:-0.064905584\\nColumn:1 Reward:-0.038745522\\nColumn:2 Reward:0.08212131\\nColumn:3 Reward:0.030976892\\nColumn:4 Reward:-0.002717197\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.011248,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.502567,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 6, 'board': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 2, 1, 0, 0, 0, 2, 1, 0, 0], 'mark': 1}\\nColumn:0 Reward:-0.22244143\\nColumn:1 Reward:-0.007906914\\nColumn:2 Reward:-0.13019747\\nColumn:3 Reward:-0.34577143\\nColumn:4 Reward:-0.014088273\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 2.9e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.036987,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.50375,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'mark': 2, 'step': 1, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\\nColumn:0 Reward:-0.100441515\\nColumn:1 Reward:0.03313577\\nColumn:2 Reward:0.06304675\\nColumn:3 Reward:-0.0074908733\\nColumn:4 Reward:-0.019672036\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.044479,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.587929,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'mark': 2, 'step': 3, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1]}\\nColumn:0 Reward:-0.1907661\\nColumn:1 Reward:-0.02842164\\nColumn:2 Reward:0.005993545\\nColumn:3 Reward:-0.1616764\\nColumn:4 Reward:-0.087816656\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.00019,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.044221,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.672594,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'mark': 2, 'step': 1, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\\nColumn:0 Reward:-0.100441515\\nColumn:1 Reward:0.03313577\\nColumn:2 Reward:0.06304675\\nColumn:3 Reward:-0.0074908733\\nColumn:4 Reward:-0.019672036\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.04777,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.653201,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'mark': 2, 'step': 3, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1]}\\nColumn:0 Reward:-0.1907661\\nColumn:1 Reward:-0.02842164\\nColumn:2 Reward:0.005993545\\nColumn:3 Reward:-0.1616764\\nColumn:4 Reward:-0.087816656\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000108,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.034215,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.402757,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'mark': 2, 'step': 1, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\\nColumn:0 Reward:-0.100441515\\nColumn:1 Reward:0.03313577\\nColumn:2 Reward:0.06304675\\nColumn:3 Reward:-0.0074908733\\nColumn:4 Reward:-0.019672036\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.022,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.491423,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'mark': 2, 'step': 3, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1]}\\nColumn:0 Reward:-0.1907661\\nColumn:1 Reward:-0.02842164\\nColumn:2 Reward:0.005993545\\nColumn:3 Reward:-0.1616764\\nColumn:4 Reward:-0.087816656\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 5e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 3.6e-05,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 0, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 3.8e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.543779,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0], 'mark': 1}\\nColumn:0 Reward:-0.117846906\\nColumn:1 Reward:0.06508827\\nColumn:2 Reward:0.17722237\\nColumn:3 Reward:0.04854679\\nColumn:4 Reward:-0.0121792555\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 3.2e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.647564,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 2], 'mark': 1}\\nColumn:0 Reward:0.26361984\\nColumn:1 Reward:-0.0042598248\\nColumn:2 Reward:2.0055575\\nColumn:3 Reward:0.0109529495\\nColumn:4 Reward:0.10961631\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 2.5e-05,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 0, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.017872,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.527772,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 2, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0], 'mark': 1}\\nColumn:0 Reward:-0.117846906\\nColumn:1 Reward:0.06508827\\nColumn:2 Reward:0.17722237\\nColumn:3 Reward:0.04854679\\nColumn:4 Reward:-0.0121792555\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.01182,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.603248,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 4, 'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0], 'mark': 1}\\nColumn:0 Reward:-0.121901214\\nColumn:1 Reward:-0.045523286\\nColumn:2 Reward:0.12066203\\nColumn:3 Reward:0.013103247\\nColumn:4 Reward:-0.05243379\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.020492,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.555051,\n",
       "        &quot;stdout&quot;: &quot;{'rows': 5, 'columns': 5, 'inarow': 3, 'episodeSteps': 1000, 'actTimeout': 2, 'runTimeout': 1200, 'agentTimeout': 60, 'timeout': 2}\\n{'remainingOverageTime': 60, 'step': 6, 'board': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0], 'mark': 1}\\nColumn:0 Reward:-0.32141322\\nColumn:1 Reward:0.35419166\\nColumn:2 Reward:-0.2084791\\nColumn:3 Reward:0.42818117\\nColumn:4 Reward:0.2503333\\n&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000115,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ]\n",
       "  ],\n",
       "  &quot;mode&quot;: &quot;ipython&quot;,\n",
       "  &quot;width&quot;: 500,\n",
       "  &quot;height&quot;: 450\n",
       "};\n",
       "\n",
       "\n",
       "window.kaggle.renderer = // Copyright 2020 Kaggle Inc\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "function renderer({\n",
       "  act,\n",
       "  agents,\n",
       "  environment,\n",
       "  frame,\n",
       "  height = 400,\n",
       "  interactive,\n",
       "  isInteractive,\n",
       "  parent,\n",
       "  step,\n",
       "  update,\n",
       "  width = 400,\n",
       "}) {\n",
       "  // Configuration.\n",
       "  const { rows, columns, inarow } = environment.configuration;\n",
       "\n",
       "  // Common Dimensions.\n",
       "  const unit = 8;\n",
       "  const minCanvasSize = Math.min(height, width);\n",
       "  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n",
       "  const cellSize = Math.min(\n",
       "    (width - minOffset * 2) / columns,\n",
       "    (height - minOffset * 2) / rows\n",
       "  );\n",
       "  const cellInset = 0.8;\n",
       "  const pieceScale = cellSize / 100;\n",
       "  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n",
       "  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n",
       "\n",
       "  // Canvas Setup.\n",
       "  let canvas = parent.querySelector(&quot;canvas&quot;);\n",
       "  if (!canvas) {\n",
       "    canvas = document.createElement(&quot;canvas&quot;);\n",
       "    parent.appendChild(canvas);\n",
       "\n",
       "    if (interactive) {\n",
       "      canvas.addEventListener(&quot;click&quot;, evt => {\n",
       "        if (!isInteractive()) return;\n",
       "        const rect = evt.target.getBoundingClientRect();\n",
       "        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n",
       "        if (col >= 0 && col < columns) act(col);\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n",
       "\n",
       "  // Character Paths (based on 100x100 tiles).\n",
       "  const kPath = new Path2D(\n",
       "    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n",
       "  );\n",
       "  const goose1Path = new Path2D(\n",
       "    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n",
       "  );\n",
       "  const goose2Path = new Path2D(\n",
       "    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n",
       "  );\n",
       "  const goose3Path = new Path2D(\n",
       "    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n",
       "  );\n",
       "\n",
       "  // Canvas setup and reset.\n",
       "  let c = canvas.getContext(&quot;2d&quot;);\n",
       "  canvas.width = width;\n",
       "  canvas.height = height;\n",
       "  c.fillStyle = &quot;#000B2A&quot;;\n",
       "  c.fillRect(0, 0, canvas.width, canvas.height);\n",
       "\n",
       "  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n",
       "\n",
       "  const getColor = (mark, opacity = 1) => {\n",
       "    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n",
       "    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n",
       "    return &quot;#fff&quot;;\n",
       "  };\n",
       "\n",
       "  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n",
       "    const [row, col] = getRowCol(cell);\n",
       "    c.arc(\n",
       "      xOffset + xFrame * (col * cellSize + cellSize / 2),\n",
       "      yOffset + yFrame * (row * cellSize + cellSize / 2),\n",
       "      (cellInset * cellSize) / 2 - radiusOffset,\n",
       "      2 * Math.PI,\n",
       "      false\n",
       "    );\n",
       "  };\n",
       "\n",
       "  // Render the pieces.\n",
       "  const board = environment.steps[step][0].observation.board;\n",
       "\n",
       "  const drawPiece = mark => {\n",
       "    // Base Styles.\n",
       "    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n",
       "    c.fillStyle = getColor(mark, opacity);\n",
       "    c.strokeStyle = getColor(mark);\n",
       "    c.shadowColor = getColor(mark);\n",
       "    c.shadowBlur = 8 / cellInset;\n",
       "    c.lineWidth = 1 / cellInset;\n",
       "\n",
       "    // Outer circle.\n",
       "    c.save();\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 50, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.lineWidth *= 4;\n",
       "    c.stroke();\n",
       "    c.fill();\n",
       "    c.restore();\n",
       "\n",
       "    // Inner circle.\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 40, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.stroke();\n",
       "\n",
       "    // Kaggle &quot;K&quot;.\n",
       "    if (mark === 1) {\n",
       "      const scale = 0.54;\n",
       "      c.save();\n",
       "      c.translate(23, 23);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(kPath);\n",
       "      c.restore();\n",
       "    }\n",
       "\n",
       "    // Kaggle &quot;Goose&quot;.\n",
       "    if (mark === 2) {\n",
       "      const scale = 0.6;\n",
       "      c.save();\n",
       "      c.translate(24, 28);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(goose1Path);\n",
       "      c.stroke(goose2Path);\n",
       "      c.stroke(goose3Path);\n",
       "      c.beginPath();\n",
       "      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n",
       "      c.closePath();\n",
       "      c.fill();\n",
       "      c.restore();\n",
       "    }\n",
       "  };\n",
       "\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    const [row, col] = getRowCol(i);\n",
       "    if (board[i] === 0) continue;\n",
       "    // Easing In.\n",
       "    let yFrame = Math.min(\n",
       "      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n",
       "      1\n",
       "    );\n",
       "\n",
       "    if (\n",
       "      step > 1 &&\n",
       "      environment.steps[step - 1][0].observation.board[i] === board[i]\n",
       "    ) {\n",
       "      yFrame = 1;\n",
       "    }\n",
       "\n",
       "    c.save();\n",
       "    c.translate(\n",
       "      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n",
       "      yOffset +\n",
       "        yFrame * (cellSize * row) +\n",
       "        (cellSize - cellSize * cellInset) / 2\n",
       "    );\n",
       "    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n",
       "    drawPiece(board[i]);\n",
       "    c.restore();\n",
       "  }\n",
       "\n",
       "  // Background Gradient.\n",
       "  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n",
       "  const bgStyle = c.createRadialGradient(\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    0,\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    bgRadius\n",
       "  );\n",
       "  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n",
       "  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n",
       "\n",
       "  // Render the board overlay.\n",
       "  c.beginPath();\n",
       "  c.rect(0, 0, canvas.width, canvas.height);\n",
       "  c.closePath();\n",
       "  c.shadowBlur = 0;\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    drawCellCircle(i);\n",
       "    c.closePath();\n",
       "  }\n",
       "  c.fillStyle = bgStyle;\n",
       "  c.fill(&quot;evenodd&quot;);\n",
       "\n",
       "  // Render the board overlay cell outlines.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    c.beginPath();\n",
       "    drawCellCircle(i);\n",
       "    c.strokeStyle = &quot;#0361B2&quot;;\n",
       "    c.lineWidth = 1;\n",
       "    c.stroke();\n",
       "    c.closePath();\n",
       "  }\n",
       "\n",
       "  const drawLine = (fromCell, toCell) => {\n",
       "    if (frame < 0.5) return;\n",
       "    const lineFrame = (frame - 0.5) / 0.5;\n",
       "    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n",
       "    const x2 =\n",
       "      x1 +\n",
       "      lineFrame *\n",
       "        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n",
       "    const y1 =\n",
       "      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n",
       "    const y2 =\n",
       "      y1 +\n",
       "      lineFrame *\n",
       "        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n",
       "    c.beginPath();\n",
       "    c.lineCap = &quot;round&quot;;\n",
       "    c.lineWidth = 4;\n",
       "    c.strokeStyle = getColor(board[fromCell]);\n",
       "    c.shadowBlur = 8;\n",
       "    c.shadowColor = getColor(board[fromCell]);\n",
       "    c.moveTo(x1, y1);\n",
       "    c.lineTo(x2, y2);\n",
       "    c.stroke();\n",
       "  };\n",
       "\n",
       "  // Generate a graph of the board.\n",
       "  const getCell = (cell, rowOffset, columnOffset) => {\n",
       "    const row = Math.floor(cell / columns) + rowOffset;\n",
       "    const col = (cell % columns) + columnOffset;\n",
       "    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n",
       "    return col + row * columns;\n",
       "  };\n",
       "  const makeNode = cell => {\n",
       "    const node = { cell, directions: [], value: board[cell] };\n",
       "    for (let r = -1; r <= 1; r++) {\n",
       "      for (let c = -1; c <= 1; c++) {\n",
       "        if (r === 0 && c === 0) continue;\n",
       "        node.directions.push(getCell(cell, r, c));\n",
       "      }\n",
       "    }\n",
       "    return node;\n",
       "  };\n",
       "  const graph = board.map((_, i) => makeNode(i));\n",
       "\n",
       "  // Check for any wins!\n",
       "  const getSequence = (node, direction) => {\n",
       "    const sequence = [node.cell];\n",
       "    while (sequence.length < inarow) {\n",
       "      const next = graph[node.directions[direction]];\n",
       "      if (!next || node.value !== next.value || next.value === 0) return;\n",
       "      node = next;\n",
       "      sequence.push(node.cell);\n",
       "    }\n",
       "    return sequence;\n",
       "  };\n",
       "\n",
       "  // Check all nodes.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    // Check all directions (not the most efficient).\n",
       "    for (let d = 0; d < 8; d++) {\n",
       "      const seq = getSequence(graph[i], d);\n",
       "      if (seq) {\n",
       "        drawLine(seq[0], seq[inarow - 1]);\n",
       "        i = board.length;\n",
       "        break;\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Upgrade the legend.\n",
       "  if (agents.length && (!agents[0].color || !agents[0].image)) {\n",
       "    const getPieceImage = mark => {\n",
       "      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n",
       "      parent.appendChild(pieceCanvas);\n",
       "      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n",
       "      pieceCanvas.width = 100;\n",
       "      pieceCanvas.height = 100;\n",
       "      c = pieceCanvas.getContext(&quot;2d&quot;);\n",
       "      c.translate(10, 10);\n",
       "      c.scale(0.8, 0.8);\n",
       "      drawPiece(mark);\n",
       "      const dataUrl = pieceCanvas.toDataURL();\n",
       "      parent.removeChild(pieceCanvas);\n",
       "      return dataUrl;\n",
       "    };\n",
       "\n",
       "    agents.forEach(agent => {\n",
       "      agent.color = getColor(agent.index + 1);\n",
       "      agent.image = getPieceImage(agent.index + 1);\n",
       "    });\n",
       "    update({ agents });\n",
       "  }\n",
       "};\n",
       "\n",
       "\n",
       "    \n",
       "    </script>\n",
       "    <script>\n",
       "      const h = htm.bind(preact.h);\n",
       "      const { useContext, useEffect, useRef, useState } = preactHooks;\n",
       "      const styled = window.styled.default;\n",
       "\n",
       "      const Context = preact.createContext({});\n",
       "\n",
       "      const Loading = styled.div`\n",
       "        animation: rotate360 1.1s infinite linear;\n",
       "        border: 8px solid rgba(255, 255, 255, 0.2);\n",
       "        border-left-color: #0cb1ed;\n",
       "        border-radius: 50%;\n",
       "        height: 40px;\n",
       "        position: relative;\n",
       "        transform: translateZ(0);\n",
       "        width: 40px;\n",
       "\n",
       "        @keyframes rotate360 {\n",
       "          0% {\n",
       "            transform: rotate(0deg);\n",
       "          }\n",
       "          100% {\n",
       "            transform: rotate(360deg);\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Logo = styled(\n",
       "        (props) => h`\n",
       "        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n",
       "          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n",
       "            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n",
       "              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n",
       "              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n",
       "              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n",
       "              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n",
       "              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n",
       "              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n",
       "            </g>\n",
       "          </svg>\n",
       "        </a>\n",
       "      `\n",
       "      )`\n",
       "        display: inline-flex;\n",
       "      `;\n",
       "\n",
       "      const Header = styled((props) => {\n",
       "        const { environment } = useContext(Context);\n",
       "\n",
       "        return h`<div className=${props.className} >\n",
       "          <${Logo} />\n",
       "          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n",
       "          ${environment.title}\n",
       "        </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-bottom: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        color: #fff;\n",
       "        display: flex;\n",
       "        flex: 0 0 36px;\n",
       "        font-size: 14px;\n",
       "        justify-content: space-between;\n",
       "        padding: 0 8px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Renderer = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { animate, debug, playing, renderer, speed } = context;\n",
       "        const ref = preact.createRef();\n",
       "\n",
       "        useEffect(async () => {\n",
       "          if (!ref.current) return;\n",
       "\n",
       "          const renderFrame = async (start, step, lastFrame) => {\n",
       "            if (step !== context.step) return;\n",
       "            if (lastFrame === 1) {\n",
       "              if (!animate) return;\n",
       "              start = Date.now();\n",
       "            }\n",
       "            const frame =\n",
       "              playing || animate\n",
       "                ? Math.min((Date.now() - start) / speed, 1)\n",
       "                : 1;\n",
       "            try {\n",
       "              if (debug) console.time(&quot;render&quot;);\n",
       "              await renderer({\n",
       "                ...context,\n",
       "                frame,\n",
       "                height: ref.current.clientHeight,\n",
       "                hooks: preactHooks,\n",
       "                parent: ref.current,\n",
       "                preact,\n",
       "                styled,\n",
       "                width: ref.current.clientWidth,\n",
       "              });\n",
       "            } catch (error) {\n",
       "              if (debug) console.error(error);\n",
       "              console.log({ ...context, frame, error });\n",
       "            } finally {\n",
       "              if (debug) console.timeEnd(&quot;render&quot;);\n",
       "            }\n",
       "            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n",
       "          };\n",
       "\n",
       "          await renderFrame(Date.now(), context.step);\n",
       "        }, [ref.current, context.step, context.renderer]);\n",
       "\n",
       "        return h`<div className=${props.className} ref=${ref} />`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        height: 100%;\n",
       "        left: 0;\n",
       "        justify-content: center;\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Processing = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        const text = processing === true ? &quot;Processing...&quot; : processing;\n",
       "        return h`<div className=${props.className}>${text}</div>`;\n",
       "      })`\n",
       "        bottom: 0;\n",
       "        color: #fff;\n",
       "        font-size: 12px;\n",
       "        left: 0;\n",
       "        line-height: 24px;\n",
       "        position: absolute;\n",
       "        text-align: center;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Viewer = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        return h`<div className=${props.className}>\n",
       "          <${Renderer} />\n",
       "          ${processing && h`<${Processing} />`}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        background-image: radial-gradient(\n",
       "          circle closest-side,\n",
       "          #000b49,\n",
       "          #000b2a\n",
       "        );\n",
       "        display: flex;\n",
       "        flex: 1;\n",
       "        overflow: hidden;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      // Partitions the elements of arr into subarrays of max length num.\n",
       "      const groupIntoSets = (arr, num) => {\n",
       "        const sets = [];\n",
       "        arr.forEach(a => {\n",
       "          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n",
       "            sets.push([]);\n",
       "          }\n",
       "          sets[sets.length - 1].push(a);\n",
       "        });\n",
       "        return sets;\n",
       "      }\n",
       "\n",
       "      // Expects `width` input prop to set proper max-width for agent name span.\n",
       "      const Legend = styled((props) => {\n",
       "        const { agents, legend } = useContext(Context);\n",
       "\n",
       "        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n",
       "\n",
       "        return h`<div className=${props.className}>\n",
       "          ${agentPairs.map(agentList =>\n",
       "            h`<ul>\n",
       "                ${agentList.map(a =>\n",
       "                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n",
       "                      ${a.image && h`<img src=${a.image} />`}\n",
       "                      <span>${a.name}</span>\n",
       "                    </li>`\n",
       "                )}\n",
       "              </ul>`)}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        font-family: sans-serif;\n",
       "        font-size: 14px;\n",
       "        height: 48px;\n",
       "        width: 100%;\n",
       "\n",
       "        ul {\n",
       "          align-items: center;\n",
       "          display: flex;\n",
       "          flex-direction: row;\n",
       "          justify-content: center;\n",
       "        }\n",
       "\n",
       "        li {\n",
       "          align-items: center;\n",
       "          display: inline-flex;\n",
       "          transition: color 1s;\n",
       "        }\n",
       "\n",
       "        span {\n",
       "          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n",
       "          overflow: hidden;\n",
       "          text-overflow: ellipsis;\n",
       "          white-space: nowrap;\n",
       "        }\n",
       "\n",
       "        img {\n",
       "          height: 24px;\n",
       "          margin-left: 4px;\n",
       "          margin-right: 4px;\n",
       "          width: 24px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepInput = styled.input.attrs({\n",
       "        type: &quot;range&quot;,\n",
       "      })`\n",
       "        appearance: none;\n",
       "        background: rgba(255, 255, 255, 0.15);\n",
       "        border-radius: 2px;\n",
       "        display: block;\n",
       "        flex: 1;\n",
       "        height: 4px;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "        width: 100%;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "\n",
       "        &::-webkit-slider-thumb {\n",
       "          appearance: none;\n",
       "          background: #1ebeff;\n",
       "          border-radius: 100%;\n",
       "          cursor: pointer;\n",
       "          height: 12px;\n",
       "          margin: 0;\n",
       "          position: relative;\n",
       "          width: 12px;\n",
       "\n",
       "          &::after {\n",
       "            content: &quot;&quot;;\n",
       "            position: absolute;\n",
       "            top: 0px;\n",
       "            left: 0px;\n",
       "            width: 200px;\n",
       "            height: 8px;\n",
       "            background: green;\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const PlayButton = styled.button`\n",
       "        align-items: center;\n",
       "        background: none;\n",
       "        border: none;\n",
       "        color: white;\n",
       "        cursor: pointer;\n",
       "        display: flex;\n",
       "        flex: 0 0 56px;\n",
       "        font-size: 20px;\n",
       "        height: 40px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepCount = styled.span`\n",
       "        align-items: center;\n",
       "        color: white;\n",
       "        display: flex;\n",
       "        font-size: 14px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        padding: 0 16px;\n",
       "        pointer-events: none;\n",
       "      `;\n",
       "\n",
       "      const Controls = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "        const value = step + 1;\n",
       "        const onClick = () => (playing ? pause() : play());\n",
       "        const onInput = (e) => {\n",
       "          pause();\n",
       "          setStep(parseInt(e.target.value) - 1);\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n",
       "          playing\n",
       "            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "        }</svg><//>\n",
       "            <${StepInput} min=&quot;1&quot; max=${\n",
       "          environment.steps.length\n",
       "        } value=&quot;${value}&quot; onInput=${onInput} />\n",
       "            <${StepCount}>${value} / ${environment.steps.length}<//>\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-top: 4px solid #212121;\n",
       "        display: flex;\n",
       "        flex: 0 0 44px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Info = styled((props) => {\n",
       "        const {\n",
       "          environment,\n",
       "          playing,\n",
       "          step,\n",
       "          speed,\n",
       "          animate,\n",
       "          header,\n",
       "          controls,\n",
       "          settings,\n",
       "        } = useContext(Context);\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            info:\n",
       "            step(${step}),\n",
       "            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n",
       "            speed(${speed}),\n",
       "            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n",
       "          </div>`;\n",
       "      })`\n",
       "        color: #888;\n",
       "        font-family: monospace;\n",
       "        font-size: 12px;\n",
       "      `;\n",
       "\n",
       "      const Settings = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${Info} />\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        background: #fff;\n",
       "        border-top: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        padding: 20px;\n",
       "        width: 100%;\n",
       "\n",
       "        h1 {\n",
       "          font-size: 20px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Player = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { agents, controls, header, legend, loading, settings, width } = context;\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            ${loading && h`<${Loading} />`}\n",
       "            ${!loading && header && h`<${Header} />`}\n",
       "            ${!loading && h`<${Viewer} />`}\n",
       "            ${!loading && legend && h`<${Legend} width=${width}/>`}\n",
       "            ${!loading && controls && h`<${Controls} />`}\n",
       "            ${!loading && settings && h`<${Settings} />`}\n",
       "          </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        background: #212121;\n",
       "        border: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "        height: 100%;\n",
       "        justify-content: center;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const App = () => {\n",
       "        const renderCountRef = useRef(0);\n",
       "        const [_, setRenderCount] = useState(0);\n",
       "\n",
       "        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n",
       "        const speeds = [\n",
       "          0,\n",
       "          3000,\n",
       "          1000,\n",
       "          500,\n",
       "          333, // Default\n",
       "          200,\n",
       "          100,\n",
       "          50,\n",
       "          25,\n",
       "          10,\n",
       "        ];\n",
       "\n",
       "        const contextRef = useRef({\n",
       "          animate: false,\n",
       "          agents: [],\n",
       "          controls: false,\n",
       "          debug: false,\n",
       "          environment: { steps: [], info: {} },\n",
       "          header: window.innerHeight >= 600,\n",
       "          height: window.innerHeight,\n",
       "          interactive: false,\n",
       "          legend: true,\n",
       "          loading: false,\n",
       "          playing: false,\n",
       "          processing: false,\n",
       "          renderer: () => &quot;DNE&quot;,\n",
       "          settings: false,\n",
       "          speed: speeds[4],\n",
       "          step: 0,\n",
       "          width: window.innerWidth,\n",
       "        });\n",
       "\n",
       "        // Context helpers.\n",
       "        const rerender = (contextRef.current.rerender = () =>\n",
       "          setRenderCount((renderCountRef.current += 1)));\n",
       "        const setStep = (contextRef.current.setStep = (newStep) => {\n",
       "          contextRef.current.step = newStep;\n",
       "          rerender();\n",
       "        });\n",
       "        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n",
       "          contextRef.current.playing = playing;\n",
       "          rerender();\n",
       "        });\n",
       "        const pause = (contextRef.current.pause = () => setPlaying(false));\n",
       "\n",
       "        const playNext = () => {\n",
       "          const context = contextRef.current;\n",
       "\n",
       "          if (\n",
       "            context.playing &&\n",
       "            context.step < context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(context.step + 1);\n",
       "            play(true);\n",
       "          } else {\n",
       "            pause();\n",
       "          }\n",
       "        };\n",
       "\n",
       "        const play = (contextRef.current.play = (continuing) => {\n",
       "          const context = contextRef.current;\n",
       "          if (context.playing && !continuing) return;\n",
       "          if (!context.playing) setPlaying(true);\n",
       "          if (\n",
       "            !continuing &&\n",
       "            context.step === context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(0);\n",
       "          }\n",
       "          setTimeout(playNext, context.speed);\n",
       "        });\n",
       "\n",
       "        const updateContext = (o) => {\n",
       "          const context = contextRef.current;\n",
       "          Object.assign(context, o, {\n",
       "            environment: { ...context.environment, ...(o.environment || {}) },\n",
       "          });\n",
       "          rerender();\n",
       "        };\n",
       "\n",
       "        // First time setup.\n",
       "        useEffect(() => {\n",
       "          // Timeout is used to ensure useEffect renders once.\n",
       "          setTimeout(() => {\n",
       "            // Initialize context with window.kaggle.\n",
       "            updateContext(window.kaggle || {});\n",
       "\n",
       "            if (window.kaggle.playing) {\n",
       "                play(true);\n",
       "            }\n",
       "\n",
       "            // Listen for messages received to update the context.\n",
       "            window.addEventListener(\n",
       "              &quot;message&quot;,\n",
       "              (event) => {\n",
       "                // Ensure the environment names match before updating.\n",
       "                try {\n",
       "                  if (\n",
       "                    event.data.environment.name ==\n",
       "                    contextRef.current.environment.name\n",
       "                  ) {\n",
       "                    updateContext(event.data);\n",
       "                  }\n",
       "                } catch {}\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "            // Listen for keyboard commands.\n",
       "            window.addEventListener(\n",
       "              &quot;keydown&quot;,\n",
       "              (event) => {\n",
       "                const {\n",
       "                  interactive,\n",
       "                  isInteractive,\n",
       "                  playing,\n",
       "                  step,\n",
       "                  environment,\n",
       "                } = contextRef.current;\n",
       "                const key = event.keyCode;\n",
       "                const zero_key = 48\n",
       "                const nine_key = 57\n",
       "                if (\n",
       "                  interactive ||\n",
       "                  isInteractive() ||\n",
       "                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n",
       "                )\n",
       "                  return;\n",
       "\n",
       "                if (key === 32) {\n",
       "                  playing ? pause() : play();\n",
       "                } else if (key === 39) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step < environment.steps.length - 1) setStep(step + 1);\n",
       "                  rerender();\n",
       "                } else if (key === 37) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step > 0) setStep(step - 1);\n",
       "                  rerender();\n",
       "                } else if (key >= zero_key && key <= nine_key) {\n",
       "                  contextRef.current.speed = speeds[key - zero_key];\n",
       "                }\n",
       "                event.preventDefault();\n",
       "                return false;\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "          }, 1);\n",
       "        }, []);\n",
       "\n",
       "        if (contextRef.current.debug) {\n",
       "          console.log(&quot;context&quot;, contextRef.current);\n",
       "        }\n",
       "\n",
       "        // Ability to update context.\n",
       "        contextRef.current.update = updateContext;\n",
       "\n",
       "        // Ability to communicate with ipython.\n",
       "        const execute = (contextRef.current.execute = (source) =>\n",
       "          new Promise((resolve, reject) => {\n",
       "            try {\n",
       "              window.parent.IPython.notebook.kernel.execute(source, {\n",
       "                iopub: {\n",
       "                  output: (resp) => {\n",
       "                    const type = resp.msg_type;\n",
       "                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n",
       "                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n",
       "                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n",
       "                  },\n",
       "                },\n",
       "              });\n",
       "            } catch (e) {\n",
       "              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n",
       "            }\n",
       "          }));\n",
       "\n",
       "        // Ability to return an action from an interactive session.\n",
       "        contextRef.current.act = (action) => {\n",
       "          const id = contextRef.current.environment.id;\n",
       "          updateContext({ processing: true });\n",
       "          execute(`\n",
       "            import json\n",
       "            from kaggle_environments import interactives\n",
       "            if &quot;${id}&quot; in interactives:\n",
       "                action = json.loads('${JSON.stringify(action)}')\n",
       "                env, trainer = interactives[&quot;${id}&quot;]\n",
       "                trainer.step(action)\n",
       "                print(json.dumps(env.steps))`)\n",
       "            .then((resp) => {\n",
       "              try {\n",
       "                updateContext({\n",
       "                  processing: false,\n",
       "                  environment: { steps: JSON.parse(resp) },\n",
       "                });\n",
       "                play();\n",
       "              } catch (e) {\n",
       "                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n",
       "                console.error(resp, e);\n",
       "              }\n",
       "            })\n",
       "            .catch((e) => console.error(e));\n",
       "        };\n",
       "\n",
       "        // Check if currently interactive.\n",
       "        contextRef.current.isInteractive = () => {\n",
       "          const context = contextRef.current;\n",
       "          const steps = context.environment.steps;\n",
       "          return (\n",
       "            context.interactive &&\n",
       "            !context.processing &&\n",
       "            context.step === steps.length - 1 &&\n",
       "            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n",
       "          );\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <${Context.Provider} value=${contextRef.current}>\n",
       "            <${Player} />\n",
       "          <//>`;\n",
       "      };\n",
       "\n",
       "      preact.render(h`<${App} />`, document.body);\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n",
       "\" width=\"500\" height=\"450\" frameborder=\"0\"></iframe> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": 5, \"columns\": 5, \"inarow\": 3}, debug=True)\n",
    "\n",
    "def agent_Double_DeepQL(observation, configuration):\n",
    "    \n",
    "    ################################## START NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    # Given a board (as a list) and a column, returns the index of the first row available\n",
    "    def first_row_avail(board, col, num_rows, num_cols):\n",
    "        \"\"\"\n",
    "        Given a board (as a list) and a column, returns the index of the first row available\n",
    "        \"\"\"\n",
    "        index = -1\n",
    "        for i in range(num_rows): # from top to bottom\n",
    "            board_index = col + (i * num_cols)\n",
    "            if board[board_index] == 0:\n",
    "                index = board_index\n",
    "            else:\n",
    "                return index\n",
    "        return index\n",
    "            \n",
    "    def convert_for_CNN(board, num_rows, num_cols):\n",
    "        \"\"\"\n",
    "        Converts the board (list) into a matrix of shape (rows, cols, 1), so that a CNN can work with it.\n",
    "        Player 2 checkers are replaced to -1\n",
    "        \"\"\"\n",
    "        board = [np.float32(-1) if x==2 else np.float32(x) for x in board]\n",
    "        return np.reshape(board, [1, num_rows, num_cols, 1])\n",
    "    \n",
    "    ################################## END NESTED FUNCTIONS ###########################################\n",
    "    \n",
    "    print(configuration) # {rows: 5, columns: 5, inarow: 3}\n",
    "    # Number of rows on the board\n",
    "    num_rows = configuration.rows\n",
    "    # Number of columns on the board\n",
    "    num_cols = configuration.columns\n",
    "    # Number of checkers \"in a row\" needed to win\n",
    "    in_a_row = configuration.inarow\n",
    "    print(observation) # {board: [...], mark: 1}\n",
    "    # The current serialized board (rows x columns) - array [rows x columns] with top row first\n",
    "    board = observation.board\n",
    "    # Which player the agent is playing as (1 or 2).\n",
    "    mark = observation.mark\n",
    "    # If the board is empty, put the first checker in the middle column\n",
    "    if max(board)==0:\n",
    "        return round(num_cols/2)\n",
    "    \n",
    "    # Load the model from file\n",
    "    model = tf.keras.models.load_model('model_DDQN.h5')\n",
    "    # List of available columns\n",
    "    available_cols = [col for col in range(num_cols) if board[col] == 0]\n",
    "    # Convert the list board into a two-dimensional array\n",
    "    board_matrix = convert_for_CNN(board, num_rows, num_cols) #np.reshape(board, [1, num_rows*num_cols])\n",
    "    # Evaluate the target board with a checker in each available column, and get the best one\n",
    "    reward_list = [] # List of rewards of each movement\n",
    "    reward_indexes_list = [] # List of indexes of each reward\n",
    "    # If the agent is player 2, get the reverse board, and act as player 1\n",
    "    if mark == 2:\n",
    "        board_matrix = board_matrix * (-1) + 0\n",
    "    q_values_list = model(board_matrix).numpy()[0]\n",
    "    for target_col in available_cols:\n",
    "        reward = q_values_list[target_col]\n",
    "        reward_list.append(reward)\n",
    "        reward_indexes_list.append(target_col)\n",
    "        print(\"Column:\"+str(target_col)+\" Reward:\"+str(reward))\n",
    "    # Always get the highest reward. If the agent is player 2 we are working with the reverse board.\n",
    "    max_reward = max(reward_list)\n",
    "    # Return the column with the highest reward. If several actions have the same reward, select randomly one of them\n",
    "    max_rewards_index = [reward_indexes_list[i] for i, x in enumerate(reward_list) if x == max_reward]\n",
    "    # Check if max_rewards_index is empty (every value is zero or almost 1). In this case take a column randomly\n",
    "    if max_rewards_index == []:\n",
    "        max_rewards_index = available_cols\n",
    "    return int(np.random.choice(max_rewards_index))\n",
    "\n",
    "# Run an episode using the agent above vs the negamax agent.\n",
    "#env.run([agent_Double_DeepQL, \"random\"])\n",
    "env.run([agent_Double_DeepQL, \"negamax\"])\n",
    "#env.run([\"negamax\", agent_Double_DeepQL])\n",
    "#env.run([\"negamax\", agent_DeepQL])\n",
    "#env.run([\"random\", agent_DeepQL])\n",
    "env.render(mode=\"ipython\", width=500, height=450)\n",
    "#env.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'evaluate' returns a list of lists (one list per episode).  \n",
    "These lists per episode contains two elements (one per player), and these elements have these possible values:\n",
    "- 1: This player wins\n",
    "- -1: This player loses\n",
    "- 0: Draw (I guess there will be a value for draw)  \n",
    "Example:  \n",
    "[[1, -1], # First episode: Player 1 won  \n",
    " [1, -1], # Second episode: Player 1 won  \n",
    " [1, -1], # Third episode: Player 1 won  \n",
    " [1, -1], # ...  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1],  \n",
    " [1, -1],  \n",
    " [1, -1],  \n",
    " [-1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate\n",
    "help(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1],\n",
       " [1, -1]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "#evaluate(\"connectx\", [agent_DeepQL, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "#evaluate(\"connectx\", [\"random\", agent_MQL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "#evaluate(\"connectx\", [agent_Double_DeepQL, \"negamax\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "evaluate(\"connectx\", [agent_Double_DeepQL_nega, agent_Double_DeepQL_rand], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)\n",
    "evaluate(\"connectx\", [agent_Double_DeepQL_rand, agent_Double_DeepQL_nega], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our agent against random and negamax agent.  \n",
    "- The closer the value is to 1 the better is player 1 agent (wins more times than loses)\n",
    "- The closer the value is to -1 the worse is player 1 agent (loses more times than wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Agent vs Random Agent: 0.0\n",
      "My Agent vs Negamax Agent: -1.0\n",
      "Random Agent vs My Agent: 0.2\n",
      "Negamax Agent vs My Agent: 1.0\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "\n",
    "# Run multiple episodes to estimate its performance.\n",
    "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [agent_DeepQL, \"random\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [agent_DeepQL, \"negamax\"], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Random Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", agent_DeepQL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))\n",
    "print(\"Negamax Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", agent_DeepQL], configuration={\"rows\": 5, \"columns\": 5, \"inarow\": 3}, num_episodes=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing negamax vs agent_MC\n",
      "1.0\n",
      "Playing negamax vs agent_MQL\n",
      "0.2\n",
      "Playing negamax vs agent_DeepQL\n",
      "1.0\n",
      "Playing negamax vs agent_Double_DeepQL\n",
      "1.0\n",
      "Playing agent_MC vs negamax\n",
      "1.0\n",
      "Playing agent_MC vs agent_MQL\n",
      "1.0\n",
      "Playing agent_MC vs agent_DeepQL\n",
      "-1.0\n",
      "Playing agent_MC vs agent_Double_DeepQL\n",
      "1.0\n",
      "Playing agent_MQL vs negamax\n",
      "1.0\n",
      "Playing agent_MQL vs agent_MC\n",
      "1.0\n",
      "Playing agent_MQL vs agent_DeepQL\n",
      "0.2\n",
      "Playing agent_MQL vs agent_Double_DeepQL\n",
      "0.0\n",
      "Playing agent_DeepQL vs negamax\n",
      "0.4\n",
      "Playing agent_DeepQL vs agent_MC\n",
      "-1.0\n",
      "Playing agent_DeepQL vs agent_MQL\n",
      "-1.0\n",
      "Playing agent_DeepQL vs agent_Double_DeepQL\n",
      "1.0\n",
      "Playing agent_Double_DeepQL vs negamax\n",
      "-1.0\n",
      "Playing agent_Double_DeepQL vs agent_MC\n",
      "-1.0\n",
      "Playing agent_Double_DeepQL vs agent_MQL\n",
      "-1.0\n",
      "Playing agent_Double_DeepQL vs agent_DeepQL\n",
      "1.0\n",
      "{'negamax': 1.8000000000000003, 'agent_MC': 2.0, 'agent_MQL': 3.0, 'agent_DeepQL': -1.8, 'agent_Double_DeepQL': -5.0}\n",
      "Position   Agent                Points\n",
      "1          agent_MQL              3.00\n",
      "2          agent_MC               2.00\n",
      "3          negamax                1.80\n",
      "4          agent_DeepQL          -1.80\n",
      "5          agent_Double_DeepQL   -5.00\n"
     ]
    }
   ],
   "source": [
    "# https://docs.python.org/3/library/itertools.html\n",
    "from itertools import permutations\n",
    "from kaggle_environments import evaluate\n",
    "\n",
    "\n",
    "def name(obj):\n",
    "    \"\"\"\n",
    "    If obj is a function, gets the name instead of something like <function agent_MC at 0x0000023B01C3C950>\n",
    "    \"\"\"\n",
    "    if callable(obj):\n",
    "        return getattr(obj, \"__name__\")\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "    \n",
    "conf={\"rows\": 5, \"columns\": 5, \"inarow\": 3}\n",
    "\n",
    "#list_agents = [\"random\", \"negamax\", agent_MC, agent_QL, agent_SA, agent_ESA, agent_MQL, agent_DQL, agent_DeepQL, agent_Double_DeepQL]\n",
    "list_agents = [\"negamax\", agent_MC, agent_MQL, agent_DeepQL, agent_Double_DeepQL]\n",
    "list_matches = list(permutations(list_agents, 2))\n",
    "results = {}\n",
    "\n",
    "for players in list_matches:\n",
    "    print(\"Playing\", name(players[0]), \"vs\", name(players[1]))\n",
    "    list_players = list(players)\n",
    "    result = mean_reward(evaluate(\"connectx\", agents=list_players, configuration=conf, num_episodes=10))\n",
    "    print(result)\n",
    "    results[name(players[0])] = results.get(name(players[0]), 0) + result\n",
    "    results[name(players[1])] = results.get(name(players[1]), 0) - result\n",
    "\n",
    "print(results)\n",
    "\n",
    "# Display results ordered by the points obtained\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"{:<10} {:<20} {:<6}\".format('Position','Agent','Points')) # https://docs.python.org/3/library/string.html#formatspec\n",
    "for i, result in enumerate(sorted_results):\n",
    "    print(\"{:<10} {:<20} {:>6.2f}\".format(i+1, result[0], result[1]))\n",
    "\n",
    "#Position   Agent                Points\n",
    "#1          agent_MQL              3.00\n",
    "#2          agent_MC               2.00\n",
    "#3          negamax                1.80\n",
    "#4          agent_DeepQL          -1.80\n",
    "#5          agent_Double_DeepQL   -5.00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
